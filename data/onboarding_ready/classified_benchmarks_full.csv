paper_id,paper_title,title,paper_year,year,paper_doi,primary_domain,secondary_domains,confidence,rationale,summary,benchmark_id,benchmark_name,primary_url,tier,tier_name,num_tasks,is_api_runnable,has_huggingface_data,has_requirements_analysis,data_source_type,api_keys,modality
08faaa5de214ee965c20e2bbbd66bcc7cea21b12,Hummus: A Dataset of Humorous Multimodal Metaphor Use,Hummus: A Dataset of Humorous Multimodal Metaphor Use,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper introduces a benchmark specifically for evaluating the interpretation of humor and multimodal metaphors, focusing on the resolution of creative incongruity and cross-domain symbolic mapping.","**Creative Artifacts Evaluated:**
The artifacts consist of humorous image-caption pairs featuring multimodal metaphors. These include visual scenes where specific objects or compositions serve as metaphorical vehicles for abstract concepts, paired with short-form natural language captions that trigger or resolve humorous interpretations.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and figurative reasoning, specifically the ability to resolve ""creative incongruity."" It assesses cross-modal synthesis—the capacity to link specific visual elements to abstract metaphorical schemas—and the linguistic fluency required to articulate the semantic logic behind a joke. It evaluates how well models can decode the intentional ambiguity and symbolic mapping inherent in creative humor.

**Evaluation Context:**
This is a multimodal evaluation framework designed for Large Multimodal Models (LMMs). It utilizes a battery of automatic metrics to quantify interpretive accuracy: F1 scores for classification, LaBSE (language-agnostic BERT sentence embeddings) for semantic similarity in metaphor naming, Intersection over Union (IoU) for spatial grounding of metaphorical objects, and ROUGE for the coherence of generated explanations.

**Task Characteristics:**
The tasks follow a hierarchy from discriminative classification (detecting metaphors) and constrained generation (naming metaphors, bounding box localization, and text highlighting) to interpretive reasoning (explaining the humor). The benchmark focuses on the *deconstruction and understanding* of creative figurative language rather than original generation, requiring models to perform cross-domain transfer between visual symbols and linguistic concepts.",08faaa5de214ee965c20e2bbbd66bcc7cea21b12:HUMMUS,HUMMUS,https://github.com/xiaoyuisrain/humorous-multimodal-metaphor-use,1.0,API-Only,6.0,True,False,True,github,openai,multimodal
0232c9163970edebaa20003238d8b5c66caa4452,CM-Gen: A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling,CM-Gen: A Neural Framework for Chinese Metaphor Generation with Explicit Context Modelling,2022.0,2022.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper specifically addresses the generation and identification of metaphors, which is the central focus of the Figurative Language & Rhetoric domain, while also involving divergent thinking to bridge disparate conceptual domains.","**Creative Artifacts Evaluated:**
The primary artifacts are **Chinese nominal metaphors**, specifically sentences that establish a figurative equivalence between a given subject (tenor) and an imaginative concept (vehicle). These are generated within specific linguistic contexts to ensure the figurative mapping is semantically grounded.

**Creative Capabilities Assessed:**
The framework assesses **metaphoricity** (the ability to bridge disparate conceptual domains) and **conceptual novelty**, specifically the model's capacity to produce ""unseen"" tenor-vehicle pairings that go beyond common idioms. It also measures **contextual consistency**, **linguistic fluency**, and **semantic diversity**, evaluating the model’s **divergent thinking** in mapping abstract subjects to varied concrete imagery.

**Evaluation Context:**
Evaluation is situated in a **text-only modality** using a **mixed-method approach**. It utilizes **automatic metrics**—including Perplexity, Dist-n for diversity, and a specialized **neural metaphoricity classifier**—alongside **human expert ratings** (1-5 Likert scale) for creativity and consistency. The context involves both **LLM-based generation** and **discriminative identification** tasks.

**Task Characteristics:**
The work involves **constrained generation**, where the model must synthesize a complete metaphor from a provided tenor, and **evaluative judgment** through a binary classification task to identify metaphorical vs. literal language. A distinguishing feature is the **explicit context modeling**, which requires the model to align the creative leap of the metaphor with surrounding textual constraints rather than generating isolated analogies.",0232c9163970edebaa20003238d8b5c66caa4452:Chinese NM Corpus (CMC),Chinese NM Corpus (CMC),https://github.com/liyucheng09/Metaphor_Generator,2.0,GPU/Local,2.0,False,False,True,github,none,text
0125e4f8d35544737086549b1dd894855e0cc5e7,Understanding Figurative Meaning through Explainable Visual Entailment,Understanding Figurative Meaning through Explainable Visual Entailment,2024.0,2024.0,,Figurative Language & Rhetoric,Humor & Satire,high,"The paper explicitly focuses on decoding non-literal linguistic devices like metaphors, similes, and idioms, while also incorporating sarcasm and humor into its multimodal evaluation framework.","**Creative Artifacts Evaluated:**
The artifacts consist of image-text pairs where the text contains specific figurative devices: metaphors, similes, idioms, sarcasm, and humor. The model-generated artifacts are natural language explanations that justify the entailment or contradiction between a visual premise and a figurative hypothesis.

**Creative Capabilities Assessed:**
The benchmark measures the ability to decode non-literal, abstract meanings and perform cross-modal conceptual mapping. It specifically assesses interpretative depth, reasoning soundness, and the capacity to avoid ""literalist"" hallucinations. It evaluates whether models can move beyond surface-level visual recognition to grasp the underlying intent and emotional nuance of creative linguistic expressions.

**Evaluation Context:**
This is a multimodal (vision-language) evaluation framework for Vision-Language Models (VLMs). It employs a hybrid scoring approach: automatic metrics (F1, BERTScore, BLEURT) for label accuracy and explanation similarity, alongside human expert ratings for reasoning adequacy, preference, and error analysis regarding unsound or incomplete logic.

**Task Characteristics:**
The core task is ""Explainable Figurative Visual Entailment,"" which combines discriminative judgment (classifying the relationship between image and text) with constrained natural language generation (explaining the reasoning). It emphasizes the challenge of cross-domain transfer, specifically testing if models trained on literal visual entailment can generalize to the nuanced, non-literal interpretations required for figurative language.",0125e4f8d35544737086549b1dd894855e0cc5e7:V-FLUTE,V-FLUTE,https://github.com/asaakyan/V-FLUTE,2.0,GPU/Local,1.0,False,False,True,request_access,openai; anthropic; google; huggingface,multimodal
00900ed6f920fe788fcda25d4d81f5917c0710cd,Mothman at SemEval-2024 Task 9: An Iterative System for Chain-of-Thought Prompt Optimization,Mothman at SemEval-2024 Task 9: An Iterative System for Chain-of-Thought Prompt Optimization,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,The paper specifically evaluates lateral thinking puzzles and riddles that require 'outside-the-box' reasoning and the ability to navigate linguistic traps and ambiguity.,"**Creative Artifacts Evaluated:**
The evaluation focuses on sentence-based lateral thinking puzzles and riddles. These artifacts are short, text-based scenarios designed to mislead the reader through linguistic ambiguity, conventional assumptions, or ""functional fixedness,"" requiring a non-linear approach to reach a solution.

**Creative Capabilities Assessed:**
The benchmark primarily measures lateral thinking and ""outside-the-box"" problem-solving. It assesses the model's ability to bypass standard common-sense heuristics to identify non-obvious solutions. Specific focus is placed on cognitive flexibility and robustness, as the task requires the model to maintain reasoning accuracy despite semantic and contextual reconstructions (SR and CR) and adversarial perturbations that alter the puzzle's surface form without changing its underlying logic.

**Evaluation Context:**
The context is purely text-based, utilizing Large Language Models (LLMs) optimized through an iterative Chain-of-Thought (CoT) prompting system. Evaluation is conducted via automatic scoring (Accuracy) across various data subsets, including base puzzles and adversarial versions, with human performance data used as a comparative baseline for creative reasoning depth.

**Task Characteristics:**
This is a constrained problem-solving task presented in a multiple-choice question-answering format. Unlike open-ended generation, it emphasizes the *understanding* and *resolution* of creative challenges. The task requires the model to navigate linguistic traps and perform convergent thinking to select the correct answer from a set of plausible distractors.",00900ed6f920fe788fcda25d4d81f5917c0710cd:BRAINTEASER,BRAINTEASER,https://github.com/alvin-pc-chen/semeval_brainteaser,3.0,Special,1.0,False,False,True,github,openai,text
05b04bba9cbe127b4175906b57ce51e082fe3601,Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator,Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator,2025.0,2025.0,,Scientific Discovery,Narrative & Story Writing; Functional & Professional Writing; Lateral Thinking & Creative Problem Solving,high,"The paper presents a novel research framework and experimental design (CreataSet and CrEval) for systematically evaluating AI creativity, while specifically focusing on artifacts like long-form prose, technical responses, and divergent thinking capabilities.","This research presents a large-scale framework for assessing textual creativity across 87 distinct domains, utilizing the **CreataSet** and the **CrEval** evaluator.

**Creative Artifacts Evaluated:**
The evaluation focuses on diverse textual outputs, including long-form creative writing, human-authored prose, and over one million synthetic text pairs. These artifacts span a wide breadth of 87 domains, ranging from standard literary compositions to specialized technical and instructional responses.

**Creative Capabilities Assessed:**
The benchmarks measure discriminative creativity judgment, specifically the ability to identify superior originality and stylistic nuance in pairwise comparisons. Key capabilities include the model's capacity for divergent thinking, the recognition of human-level creative markers versus synthetic patterns, and the ability to improve generative ""win rates"" through preference optimization (DPO).

**Evaluation Context:**
The context is strictly text-based, leveraging a massive dataset of 100K human-level and 1M synthetic pairs. Evaluation is conducted through LLM-as-a-judge (CrEval) and validated against human ground truth using F1 scores, Kappa coefficients, and agreement rates. It includes both in-distribution testing and out-of-distribution (O.O.D.) generalization on long-form content.

**Task Characteristics:**
Tasks are characterized by pairwise evaluation and judgment of open-ended responses. The framework also incorporates model refinement through iterative feedback and cross-domain transfer, testing whether a model’s creative judgment can generalize from short-form synthetic data to complex, long-form creative writing.",05b04bba9cbe127b4175906b57ce51e082fe3601:CreataSet,CreataSet,https://creval-creative-evaluation.github.io/,2.0,GPU/Local,1.0,False,True,True,huggingface,none,text
04a8d300d3ff12e94bff022f046d608e7fe66787,Open-ended Scientific Discovery via Bayesian Surprise,Open-ended Scientific Discovery via Bayesian Surprise,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on generating novel scientific hypotheses and experimental designs, while utilizing divergent thinking and surprise-based metrics to identify non-obvious, 'outside-the-box' ideas.","**Creative Artifacts Evaluated:**
The primary artifacts are novel scientific hypotheses formulated as natural language propositions. These are supported by structured experimental designs, expected results, and explanatory rationales derived from real-world datasets.

**Creative Capabilities Assessed:**
The framework measures originality through ""Bayesian surprise,"" quantified as the KL divergence between prior and posterior LLM beliefs. It further assesses utility, interestingness, and technical validity. Key capabilities include divergent thinking—generating a high volume of unique, non-obvious ideas—and the logical faithfulness of the discovery process.

**Evaluation Context:**
Evaluation is situated in a high-complexity, text-based scientific domain. It employs a hybrid approach: automated scoring via LLM-based surprisal metrics and human expert validation using Likert scales. The system utilizes Monte Carlo Tree Search (MCTS) to navigate the hypothesis space, comparing LLM-driven discovery against alternative reward models and human benchmarks.

**Task Characteristics:**
Tasks focus on open-ended generation and autonomous ideation. The process involves a constrained search through a belief space to maximize information gain, followed by internal process validation. This ensures that the generated experimental steps and predictions are scientifically plausible and logically consistent with the proposed hypotheses.",04a8d300d3ff12e94bff022f046d608e7fe66787:Open-ended Data-Driven Discovery Evaluation Framework,Open-ended Data-Driven Discovery Evaluation Framework,https://github.com/allenai/autods,1.0,API-Only,3.0,True,True,True,huggingface,openai,text
0251d301230802e2189e8721745b3a9f90d48386,HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation,HypoBench: Towards Systematic and Principled Benchmarking for Hypothesis Generation,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper introduces a benchmark specifically for generating and evaluating scientific hypotheses and causal rules across social and natural sciences, emphasizing abductive reasoning and pattern discovery.","**Creative Artifacts Evaluated**
The primary artifacts are textual explanatory hypotheses. These include linguistic pattern descriptions (identifying deception or mental stress), social influence predictors (explaining retweet counts or paper citations), and causal rules for synthetic systems (modeling college admissions or marine ecosystem variables).

**Creative Capabilities Assessed**
The benchmark measures abductive reasoning, pattern discovery, and conceptual depth. Specifically, it evaluates the novelty, plausibility, and clarity of generated ideas. It also assesses the functional utility of these creative outputs—their ability to be converted into predictive rules that achieve high accuracy and F1 scores on unseen data.

**Evaluation Context**
This is a text-based framework utilizing LLMs as both generators and judges (e.g., GPT-4o). It employs a ""hypothesis-to-inference"" pipeline where creative propositions are validated against empirical data. Evaluation is multi-faceted, combining traditional performance metrics (Accuracy, MSE) with subjective ratings for the qualitative ""goodness"" of the hypothesis.

**Task Characteristics**
The benchmark consists of 12 tasks spanning real-world social science discovery and synthetic rule-finding. These tasks represent constrained generation and ideation, requiring models to move from raw data observations to structured, generalizable insights. A distinguishing feature is the use of controlled difficulty levels in synthetic environments to systematically test the limits of LLM reasoning across diverse domains like psychology, marketing, and ecology.",0251d301230802e2189e8721745b3a9f90d48386:HypoBench,HypoBench,https://chicagohai.github.io/HypoBench/,1.0,API-Only,12.0,True,True,True,huggingface,openai; anthropic; google; together,text
06424848967aec352e1b5c3fcd8a414934e300a5,Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework,Let Androids Dream of Electric Sheep: A Human-like Image Implication Understanding and Reasoning Framework,2025.0,2025.0,,Figurative Language & Rhetoric,Visual Arts & Stylized Imagery; Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates a model's ability to decode visual rhetoric, symbolism, and metaphorical subtext, which are core components of figurative language and interpretive reasoning.","**Creative Artifacts Evaluated:**
Abstract, metaphorical, and culturally-coded images (50 English, 50 Chinese context) that utilize visual rhetoric to convey non-literal meanings.

**Creative Capabilities Assessed:**
Conceptual depth, emotional intelligence, and the identification of rhetorical skills (e.g., symbolism, irony). The framework measures the ability to transcend surface-level visual descriptions to grasp deep implications and domain-specific cultural contexts, focusing on interpretive reasoning rather than raw generation.

**Evaluation Context:**
A multimodal VLM assessment utilizing a dual-scoring approach: automatic accuracy for Multiple-Choice Questions (MCQ) and a nuanced GPT-4o judge for Open-Style Questions (OSQ). The latter employs a five-point rubric—Surface-level Information, Emotional Expression, Domain and Context, Rhetorical Skills, and Deep Implications—validated by high human correlation.

**Task Characteristics:**
Interpretive reasoning and semantic decoding. The tasks transition from constrained discriminative judgment to open-ended generative explanation, requiring models to synthesize visual stimuli into complex, abstract linguistic interpretations. This emphasizes the model's capacity for ""human-like"" insight into the intentionality and metaphorical subtext behind creative visual works.",06424848967aec352e1b5c3fcd8a414934e300a5:Image Implication Benchmark,Image Implication Benchmark,https://github.com/MING-ZCH/Let-Androids-Dream-of-Electric-Sheep,1.0,API-Only,2.0,True,True,True,huggingface,openai; google,multimodal
07038bad5e5be59656343afdb0dce1cf7209cf43,Pun2Pun: Benchmarking LLMs on Textual-Visual Chinese-English Pun Translation via Pragmatics Model and Linguistic Reasoning,Pun2Pun: Benchmarking LLMs on Textual-Visual Chinese-English Pun Translation via Pragmatics Model and Linguistic Reasoning,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on puns, a specific form of wordplay and humor, while requiring complex linguistic reasoning and lateral thinking to resolve semantic ambiguities across languages and modalities.","**Creative Artifacts Evaluated:**
The benchmark evaluates cross-lingual (Chinese-English) textual puns and multimodal visual puns consisting of images paired with captions. These artifacts are categorized into homophonic (phonetic-based) and homographic (orthographic-based) wordplay, representing complex linguistic and semiotic creative structures.

**Creative Capabilities Assessed:**
Assessment focuses on linguistic reasoning, conceptual depth, and divergent thinking. Specific capabilities include the ability to identify punning mechanisms (decomposition), preserve rhetorical effects across languages (translation), and interpret cultural nuances and usage contexts (appreciation). Iterative refinement tasks further measure innovativeness, content retention, and target language fluency.

**Evaluation Context:**
This is a multimodal, cross-lingual evaluation framework for LLMs and VLMs. It utilizes a hybrid scoring methodology: automatic accuracy for classification, LLM-as-a-judge (GPT-4o-mini) for scoring consistency and translation quality (Hit/Overlap metrics), and human evaluation for assessing the qualitative success of iterative creative refinements.

**Task Characteristics:**
Tasks range from constrained classification and identification to open-ended generation and cross-lingual transfer. The benchmark emphasizes complex problem-solving through pun decomposition and iterative refinement, requiring models to navigate semantic ambiguity, cultural pragmatics, and the interplay between visual and textual modalities.",07038bad5e5be59656343afdb0dce1cf7209cf43:Pun2Pun,Pun2Pun,https://github.com/rexera/Pun2Pun,2.0,GPU/Local,9.0,False,False,True,github,openai; anthropic; google; together,multimodal
06a8a91b3f0d79e5919d7cf3ce2e5502636b9f35,Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation,Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and Interpretation,2024.0,2024.0,,Figurative Language & Rhetoric,,high,"The paper focuses exclusively on the detection and interpretation of metaphors, which are the core linguistic devices defined within the Figurative Language & Rhetoric domain.","**Creative Artifacts Evaluated:**
The artifacts consist of parallel cross-lingual sentences in English and Spanish containing metaphorical tokens. These include creative linguistic expressions across multiple parts of speech, specifically nouns, verbs, adjectives, and adverbs, where meaning deviates from literal usage.

**Creative Capabilities Assessed:**
The benchmarks measure conceptual depth and figurative understanding, specifically the ability to identify non-literal linguistic deviations and interpret their semantic intent. Key capabilities include semantic mapping—determining if a metaphorical premise entails a specific hypothesis—and linguistic flexibility, assessed through the model’s ability to generalize to novel, out-of-vocabulary (Oov) creative expressions not encountered during training.

**Evaluation Context:**
Evaluation is conducted in a text-only modality using Large Language Models (e.g., DeBERTa, XLM-RoBERTa). Performance is quantified through automatic metrics including F1 scores (distinguishing between in-vocabulary and out-of-vocabulary metaphors) and accuracy for inference tasks. The context is defined by cross-lingual and cross-domain robustness, comparing model performance across distinct metaphor corpora (VUAM, CoMeta, and Meta4XNLI).

**Task Characteristics:**
Tasks are centered on evaluation and judgment rather than generation. They include sequence labeling for metaphor detection and Natural Language Inference (NLI) for metaphor interpretation. The framework emphasizes cross-domain transfer and generalization, testing whether models can apply figurative logic learned in one domain or language to another, thereby assessing the stability of metaphorical reasoning.",06a8a91b3f0d79e5919d7cf3ce2e5502636b9f35:Meta4XNLI,Meta4XNLI,https://huggingface.co/datasets/HiTZ/meta4xnli,2.0,GPU/Local,2.0,False,True,True,huggingface,none,text
0e42d4694acc559ac22d1ea280f4a6d7f9878f0f,"ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation","ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative Generation",2024.0,2024.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on text-to-image diffusion models for generating high-fidelity digital images and artistic styles, specifically evaluating aesthetic quality and the ability to create novel visual content without replicating training data.","**Creative Artifacts Evaluated:**
The artifacts consist of high-fidelity digital images across eight distinct categories, including specific artistic styles (e.g., Amedeo Modigliani’s paintings) and general-purpose visual content generated from text prompts.

**Creative Capabilities Assessed:**
The research primarily assesses **originality** and **divergent thinking**, specifically the model's ability to generate novel content while avoiding verbatim replication of training data. It measures **sample diversity** (via Vendi score), **aesthetic quality** (FID/KID), and **semantic alignment** (CLIP prompt fidelity). A core focus is the trade-off between ""inspiration"" (capturing a style) and ""imitation"" (memorizing specific training samples).

**Evaluation Context:**
This is a multimodal (text-to-image) evaluation framework for diffusion models. It utilizes **automatic scoring** through computer vision metrics (SSCD, MSS) to quantify creative novelty and detect copyright-infringing replication. The context spans both low-data regimes (few-shot learning) and large-scale pre-training environments (LAION12M).

**Task Characteristics:**
Tasks involve **open-ended generation** and **novelty-constrained synthesis**. The FSCG-8 benchmark requires few-shot style adaptation from minimal references, while the replication prevention task functions as a ""negative constraint"" problem, where the model must generate high-quality outputs that are demonstrably distinct from its training corpus.",0e42d4694acc559ac22d1ea280f4a6d7f9878f0f:FSCG-8 (Few-Shot Creative Generation 8),FSCG-8 (Few-Shot Creative Generation 8),https://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public,2.0,GPU/Local,1.0,False,True,True,huggingface,huggingface,multimodal
0a3a8a54ba7c7e1dd05539a772b4c798c91fa683,EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation,EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation,2025.0,2025.0,,Narrative & Story Writing,,high,"The paper focuses specifically on the evaluation and generation of open-ended narrative stories, assessing core narrative elements like structural coherence, stylistic consistency, and emotional resonance.","**Creative Artifacts Evaluated:**
The research focuses exclusively on open-ended narrative stories. These artifacts are evaluated across multiple dimensions to capture the nuance of creative writing rather than simple factual correctness.

**Creative Capabilities Assessed:**
The framework assesses multi-dimensional narrative quality, specifically structural coherence, stylistic consistency, and emotional resonance (empathy). It further measures creative impact through reader engagement and specific narrative elements such as surprise, complexity, and thematic relevance. Holistic ""overall quality"" is also measured to test zero-shot generalization.

**Evaluation Context:**
This is a text-based domain where Large Language Models (LLMs) function as both evaluators and generators. The study employs a ""self-evolving"" pairwise reasoning approach to align automatic scoring with human intuition. Validation involves comparing model outputs against human ground truth using automatic correlation metrics (Pearson, Spearman, Kendall) and F1-scores, supplemented by direct human assessment via 1-5 Likert scales and head-to-head win rates.

**Task Characteristics:**
The work centers on the interplay between evaluation/judgment and creative refinement. Tasks include multi-dimensional quality prediction, zero-shot generalization to unseen benchmarks (OpenMEVA), and the use of an evaluator as a reward model to fine-tune generators. This creates a closed-loop system where the ability to judge creative work is directly leveraged to improve the generation of novel narratives.",0a3a8a54ba7c7e1dd05539a772b4c798c91fa683:StoryER,StoryER,https://github.com/xindaaW/EvolvR,2.0,GPU/Local,1.0,False,False,True,github,openai; anthropic; google,text
0a5d466b39d5758e92b0a63fab6ca2c0ca9c74ea,DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models,DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models,2025.0,2025.0,,Mathematical Reasoning,Scientific Discovery,high,"The paper focuses on evaluating formal mathematical proofs and counterexamples, while its emphasis on generating hypotheses for unsolved open problems aligns with scientific discovery.","**Creative Artifacts Evaluated:**
The benchmark evaluates formal mathematical proofs, specific mathematical counterexamples (constructing objects to invalidate propositions), and exploratory research contributions or hypotheses directed toward unsolved open problems.

**Creative Capabilities Assessed:**
Assessment focuses on technical correctness, conceptual depth, and originality. It measures the model's ability to perform novel synthesis and constructive thinking, distinguishing between flawed attempts and complete, valid solutions. For open problems, it specifically evaluates ""referential value""—the potential for LLM outputs to provide meaningful insights or directions for human researchers in cutting-edge discovery.

**Evaluation Context:**
This is a text-based evaluation of LLMs using high-complexity undergraduate and research-level mathematics. Evaluation is strictly expert-driven; mathematical specialists manually score outputs based on a three-point scale (0, 0.5, 1) for proofs and counterexamples, while open problems receive qualitative expert reviews to determine their utility in a research context.

**Task Characteristics:**
Tasks involve both open-ended and constrained generation. Unlike standard math benchmarks that focus on rote calculation or algorithmic reasoning, these tasks require the construction of unique mathematical objects and the navigation of unsolved theoretical spaces. This emphasizes divergent thinking and the ability to handle non-foundational, non-standardized problems that lack existing step-by-step solutions.",0a5d466b39d5758e92b0a63fab6ca2c0ca9c74ea:DeepMath-Creative,DeepMath-Creative,https://github.com/DeepMathLLM/DeepMath,3.0,Special,2.0,False,False,True,github,openai; anthropic; google; together; huggingface,text
0a6e71c671c50dc7e6c8ccd0e0b2f5e63801afed,Collaborative Neural Painting,Collaborative Neural Painting,2023.0,2023.0,,Visual Arts & Stylized Imagery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation and refinement of stroke-based digital paintings and sketches, emphasizing aesthetic quality and creative problem-solving through constrained and open-ended artistic tasks.","**Creative Artifacts Evaluated:**
Stroke-based digital paintings and hierarchical vector sketches representing specific object categories. Unlike static pixel grids, these artifacts are defined by sequential, parameterized brushstrokes that represent the generative process of artistic creation.

**Creative Capabilities Assessed:**
The benchmark evaluates technical fidelity through stroke-level reconstruction and contextual consistency when completing partial artworks. It assesses hierarchical refinement (the ability to add fine details to coarse sketches) and spatial reasoning. Both convergent thinking (filling specific gaps) and divergent thinking (generating novel paintings from class labels) are measured, alongside aesthetic quality and stylistic coherence.

**Evaluation Context:**
This research utilizes a specialized stroke-based modality, moving beyond traditional computer vision tasks. Evaluation is a hybrid of automated geometric metrics (Stroke L1 distance using Hungarian Matching), distributional image metrics (FID), and human-centric subjective preference rankings to capture the nuances of artistic intent and visual appeal.

**Task Characteristics:**
The tasks encompass constrained generation (spatial, block, and random completion), refinement (level-based detail addition), and open-ended generation. The framework is uniquely collaborative, positioning the model as a co-creator that must interpret, respond to, and extend existing artistic context within a sequential, multi-stage workflow.",0a6e71c671c50dc7e6c8ccd0e0b2f5e63801afed:Collaborative Neural Painting (CNP) Benchmark,Collaborative Neural Painting (CNP) Benchmark,https://ldm.tib.eu/dataset/collaborative-neural-painting,2.0,GPU/Local,5.0,False,False,True,url,none,multimodal
1026b46b64d4c49776b82cddd54923a55f87679b,MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation,MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation,2024.0,2024.0,,Functional & Professional Writing,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the automated evaluation of open-ended educational questions, a task explicitly linked to pedagogical design in the Functional & Professional Writing domain, while targeting higher-order thinking and divergent responses characteristic of Lateral Thinking.","**Creative Artifacts Evaluated:**
The artifacts consist of open-ended educational questions derived from context paragraphs (EduPr dataset). These are not simple factual retrieval queries but complex, text-based prompts designed to stimulate higher-order thinking and divergent responses from learners.

**Creative Capabilities Assessed:**
The framework evaluates five core dimensions: novelty, complexity, relevance, clarity, and answerability. These criteria specifically measure the generator's ability to produce original, cognitively demanding, and contextually grounded inquiries. The assessment focuses on the ""originality"" and ""depth"" of the questions, which are hallmarks of creative pedagogical design.

**Evaluation Context:**
This text-based research introduces a multi-LLM iterative review process (MIRROR) using GPT-4, Gemini, and Llama2-70b. The methodology focuses on achieving consensus among diverse models to simulate human expert judgment. Performance is validated against human expert ratings using Pearson’s correlation for alignment and Fleiss’s Kappa for inter-annotator agreement.

**Task Characteristics:**
The work encompasses both open-ended generation and automated evaluation/judgment. It distinguishes itself by employing an iterative ""review and response"" cycle, where models critique and refine their initial ratings. This approach addresses the challenge of evaluating subjective creative qualities in text through multi-agent collaboration and refinement rather than single-pass scoring.",1026b46b64d4c49776b82cddd54923a55f87679b:MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating),MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating),https://github.com/my625/PromptQG,1.0,API-Only,2.0,True,False,True,github,openai; google,text
1600f96cb9d67221b3a55de817182a718d67942f,HW-TSC at SemEval-2024 Task 9: Exploring Prompt Engineering Strategies for Brain Teaser Puzzles Through LLMs,HW-TSC at SemEval-2024 Task 9: Exploring Prompt Engineering Strategies for Brain Teaser Puzzles Through LLMs,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Humor & Satire; Figurative Language & Rhetoric,high,"The paper explicitly evaluates lateral thinking and 'out-of-the-box' reasoning through brain teasers and riddles that leverage linguistic tricks, puns, and semantic ambiguity.","**Creative Artifacts Evaluated:**
The artifacts consist of text-based brain teasers categorized into two distinct types: word puzzles, which utilize linguistic tricks, phonetic puns, and orthographic manipulations, and sentence puzzles, which present nonsensical or counter-intuitive scenarios that subvert standard logical expectations and commonsense reasoning.

**Creative Capabilities Assessed:**
The benchmark measures lateral thinking and creative problem-solving. It specifically assesses the ability to engage in divergent thinking by bypassing conventional logic to identify unconventional solutions. This involves recognizing semantic ambiguity and the ability to navigate ""out-of-the-box"" reasoning patterns required to resolve puzzles that are intentionally designed to be misleading.

**Evaluation Context:**
The evaluation is situated within a text-only modality, focusing on the performance of Large Language Models (LLMs) using various prompt engineering strategies. Task complexity is high due to the adversarial nature of the puzzles, which are designed to trip up standard probabilistic reasoning. Scoring is strictly automatic, based on accuracy within a multiple-choice question-answering (MCQA) framework.

**Task Characteristics:**
The tasks are defined as constrained problem-solving and the understanding/deciphering of creative intent. Unlike open-ended generation, this work focuses on the model's ability to evaluate and select the correct ""creative"" logic from a set of distractors, emphasizing the recognition of lateral logic over the generation of new creative content.",,,,,,,,,,,,
187eefa5167705cf916751c135aa650c545a87a8,Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?,Pron vs Prompt: Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing?,2024.0,2024.0,,Narrative & Story Writing,Performing Arts & Video Synthesis,high,"The paper evaluates the generation of movie synopses and titles by comparing a professional fiction author's work against AI, focusing on narrative coherence, literary craftsmanship, and thematic originality.","**Creative Artifacts Evaluated:**
The benchmark evaluates short-form conceptual movie titles and long-form narrative movie synopses (approximately 600 words each).

**Creative Capabilities Assessed:**
Assessment focuses on stylistic and thematic originality, aesthetic attractiveness, and holistic creativity. Crucially, it measures ""anthology potential""—the suitability of the work for professional publication or curation—and ""own voice,"" which gauges the presence of a distinct, non-generic authorial identity and unique stylistic signature.

**Evaluation Context:**
This text-based study features a direct ""man vs. machine"" competition pitting a world-class professional novelist against GPT-4. Evaluation is performed by human experts using a multi-dimensional Likert-scale rubric grounded in formal creativity theory. It also includes a discrimination task where experts attempt to identify the source (human vs. AI) of each artifact to test for stylistic ""tells.""

**Task Characteristics:**
The tasks encompass open-ended ideation (generating titles from scratch) and constrained narrative generation (developing detailed synopses based on specific titles). The focus is on high-level literary craftsmanship, narrative coherence, and the ability to simulate professional-grade creative prose that challenges elite human standards.",187eefa5167705cf916751c135aa650c545a87a8:Pron vs Prompt Creative Writing Contest,Pron vs Prompt Creative Writing Contest,https://github.com/grmarco/pron-vs-prompt,3.0,Special,2.0,False,False,True,github,openai,text
144a5c14db46e76b7b762d64fe53cf5dfd95ffe4,When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?,When 'YES' Meets 'BUT': Can Large Models Comprehend Contradictory Humor Through Comparative Reasoning?,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Narrative & Story Writing,high,"The paper explicitly focuses on evaluating the comprehension of contradictory humor and irony in comics, utilizing tasks that require decoding visual metaphors and narrative reasoning.","### Domain-Focused Summary: YESBUT (V2)

**Creative Artifacts Evaluated:**
The primary artifacts are two-panel comics characterized by visual juxtaposition and irony. Secondary artifacts include generated textual narratives, explicit explanations of ironic contradictions, symbolic interpretations of visual metaphors, and thematic titles that encapsulate narrative intent.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and narrative reasoning through the lens of humor and contradiction. It specifically assesses the ability to decode visual symbolism, identify cultural subtext, and perform comparative reasoning between juxtaposed panels. Evaluation metrics, including human-rated fidelity and GPT-based scoring, focus on the model's capacity for nuanced interpretation, irony detection, and the synthesis of conflicting visual information.

**Evaluation Context:**
This is a multimodal evaluation framework for Vision-Language Models (VLMs). It utilizes a hybrid scoring approach that combines automated linguistic metrics (BERT Score, ROUGE-2) with LLM-as-a-judge and human qualitative assessments. The context emphasizes the transition from literal image recognition to high-level semantic understanding of abstract creative concepts.

**Task Characteristics:**
Tasks involve a mix of open-ended generation (describing narratives and explaining contradictions) and constrained discriminative judgment (selecting titles and symbolic meanings). This requires models to perform cross-modal interpretation, bridging the gap between literal visual evidence and metaphorical, often humorous, conclusions.",,,,,,,,,,,,
14549416485dcee89d257eb8e37e1ad83b925d12,FHS-adapter: fine-grained hierarchical semantic adapter for Chinese landscape paintings generation,FHS-adapter: fine-grained hierarchical semantic adapter for Chinese landscape paintings generation,2024.0,2024.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on the generation and aesthetic evaluation of traditional Chinese landscape and flower-and-bird paintings, emphasizing artistic style, brushwork, and non-photorealistic rendering.","**Creative Artifacts Evaluated:**
The research evaluates traditional Chinese landscape paintings (Shan Shui) and Chinese flower-and-bird paintings. These artifacts are characterized by specific historical stylistic conventions, non-photorealistic rendering, and traditional ink-and-wash aesthetics.

**Creative Capabilities Assessed:**
The study assesses aesthetic quality and technical proficiency across five domain-specific dimensions: composition and layout, technique and brushwork, color harmony, antiquing effect (historical stylistic fidelity), and emotional expression. It specifically measures the model’s ability to interpret fine-grained, hierarchical textual descriptions and translate them into complex visual structures that convey ""spirit"" or ""mood.""

**Evaluation Context:**
This is a multimodal (text-to-image) generation framework utilizing diffusion models. Evaluation is bifurcated into automatic metrics (FID for distribution similarity and CLIP-T Score for text-image alignment) and a rigorous human expert study. The latter involves 21 scholars who provide qualitative ratings, bridging the gap between statistical image quality and nuanced artistic judgment.

**Task Characteristics:**
The primary task is open-ended generation from fine-grained prompts, requiring the integration of hierarchical semantic information. The work also features cross-domain transfer, testing the adapter's ability to generalize learned artistic styles from landscape data to the distinct subject matter of flower-and-bird paintings, thereby evaluating the robustness of the stylistic capture.",14549416485dcee89d257eb8e37e1ad83b925d12:T2ICLP,T2ICLP,https://github.com/T2ICLP/t2iclp,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
19a9937a1029ee5b1f6da8a945ed9a5c5a029c57,DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling,DeepRapper: Neural Rap Generation with Rhyme and Rhythm Modeling,2021.0,2021.0,,Music & Auditory Arts,Poetry & Verse,high,The paper focuses on generating rap lyrics by modeling the specific relationship between phonetic rhyme patterns and musical rhythmic beats.,"**Creative Artifacts Evaluated:**
The model produces complex rap lyrics that incorporate both linguistic content and explicit structural markers, specifically rhyme patterns and rhythmic beat tokens. These artifacts are not merely text but are structured musical-linguistic sequences where timing and phonetic alignment are encoded directly into the output.

**Creative Capabilities Assessed:**
Evaluation focuses on technical proficiency in phonetic rhyming, including rhyme accuracy, density, and diversity (multi-syllabic and internal rhymes). It also assesses rhythmic synchronization through beat accuracy and the statistical distribution of beats. Beyond structural constraints, the model is evaluated on linguistic fluency and the ability to maintain thematic coherence throughout the generated verse.

**Evaluation Context:**
The research utilizes the D-RAP dataset for text-based generation. The evaluation framework is a sophisticated hybrid: objective metrics quantify structural precision (e.g., Wasserstein Distance for beat distribution and Combo-N for rhyme patterns), while human experts provide subjective Likert-scale ratings (1–5) on aesthetic qualities such as rhyme quality, diversity, and thematic depth.

**Task Characteristics:**
This is a highly constrained generation task requiring the simultaneous optimization of semantic meaning, phonetic constraints, and temporal prosody. It distinguishes itself from standard poetry generation by emphasizing the rhythmic ""flow"" and the complex, multi-syllabic rhyme structures unique to the rap genre, moving beyond simple end-rhymes to model the intricate relationship between lyrics and musical beats.",19a9937a1029ee5b1f6da8a945ed9a5c5a029c57:Rap Generation Evaluation on D-RAP,Rap Generation Evaluation on D-RAP,https://github.com/microsoft/muzic/tree/master/DeepRapper/data,2.0,GPU/Local,1.0,False,False,True,unknown,none,text
1f1093fbd6eb15b2a79e0e39d5e3621f8090fc84,ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies,ParallelPARC: A Scalable Pipeline for Generating Natural-Language Analogies,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric; Scientific Discovery,high,"The paper focuses on analogical reasoning and identifying structural isomorphisms across disparate domains, which are core components of lateral thinking, while using scientific processes as the primary subject matter for these mappings.","**Creative Artifacts Evaluated:**
Paragraph-length natural language descriptions of complex scientific processes (e.g., biological cycles, physical transformations, or chemical reactions). These artifacts function as structured relational representations of domain-specific phenomena.

**Creative Capabilities Assessed:**
The benchmark measures structural alignment and relational mapping, which are fundamental components of analogical reasoning and creative cognition. It specifically assesses conceptual depth and convergent thinking by requiring models to identify deep structural isomorphisms while ignoring superficial semantic overlaps or thematic distractors.

**Evaluation Context:**
This is a text-based evaluation framework where Large Language Models (LLMs) are benchmarked against a human-validated gold set. Scoring is based on accuracy across binary classification and multiple-choice formats, incorporating specific metrics for ""distractor"" resistance to ensure the model is not relying on surface-level keyword matching or simple word-frequency heuristics.

**Task Characteristics:**
The tasks center on evaluation and judgment through cross-domain transfer. Rather than open-ended generation, the focus is on the discriminative recognition of structural similarity. This requires the model to perform high-level abstraction, mapping the underlying logic and causal chains of one scientific process onto a disparate domain to determine if the underlying ""schema"" is preserved.",1f1093fbd6eb15b2a79e0e39d5e3621f8090fc84:ProPara-Logy,ProPara-Logy,https://github.com/orensul/ParallelPARC,1.0,API-Only,2.0,True,False,True,github,openai,text
197f69ecbf5eb0113e6cf817a77bed805985e043,MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents,MineAnyBuild: Benchmarking Spatial Planning for Open-world AI Agents,2025.0,2025.0,,"3D, Spatial & Architectural Design",Lateral Thinking & Creative Problem Solving,high,"The paper focuses on benchmarking the generation and planning of 3D architectural structures and spatial reasoning within Minecraft, while also evaluating open-ended ideation and divergent thinking.","**Creative Artifacts Evaluated:**
The benchmark evaluates executable 3D blueprint matrices, structural layouts, and specific block-based architectural designs within the Minecraft sandbox environment. These artifacts include textual brainstorms for novel structures and voxel-based representations of complex buildings.

**Creative Capabilities Assessed:**
It measures architectural complexity, aesthetic fidelity, and originality in block combinations. Beyond pure generation, it assesses spatial reasoning through mental rotation and the ability to translate abstract textual prompts into coherent, executable 3D structures, testing both divergent thinking and technical correctness.

**Evaluation Context:**
This is a multimodal framework (text, image, and 3D data) where performance is primarily judged by a high-level LLM critic (GPT-4.1) using weighted scores for subjective qualities like ""creativity"" and ""completeness,"" alongside automatic accuracy metrics for spatial VQA tasks.

**Task Characteristics:**
The work spans open-ended ideation (brainstorming creative builds), constrained generation (following spatial instructions), and evaluative judgment (spatial commonsense and reasoning). It distinguishes itself from general creativity research by requiring agents to bridge the gap between abstract creative intent and precise, executable spatial planning in a 3D open-world domain.",197f69ecbf5eb0113e6cf817a77bed805985e043:MineAnyBuild,MineAnyBuild,https://huggingface.co/datasets/SaDil/MineAnyBuild,3.0,Special,5.0,False,True,True,huggingface,openai; anthropic; google; huggingface,multimodal
1a4b3d0814833bd0c011bf25d036e877d479d90c,InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser,InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser,2023.0,2023.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on the generation and evaluation of stylized digital images, specifically addressing style transfer, multi-style blending, and aesthetic quality in diffusion-based models.","**Creative Artifacts Evaluated:**
The research evaluates stylized digital images that integrate specific artistic aesthetics from reference images with textual content prompts. This includes complex hybrid artifacts that blend two distinct, often disparate, artistic styles into a single visual output.

**Creative Capabilities Assessed:**
The benchmarks measure **stylistic fidelity** (adherence to a reference style) and **content preservation** (maintaining the integrity of the text-prompted subject). It assesses **aesthetic quality** through human judgment and **stylistic synthesis**, specifically the capacity for controllable, multi-style blending and the successful navigation of the ""content-style trade-off.""

**Evaluation Context:**
The evaluation is centered on **Diffusion-based multimodal generation** (specifically Stable Diffusion). It employs a hybrid scoring approach: **automated CLIP scores** quantify style and content alignment, while **human pairwise comparisons** evaluate subjective stylization effects and structural integrity against baseline models.

**Task Characteristics:**
Tasks involve **constrained generation** and **cross-domain style transfer**. The work emphasizes **multi-style fusion**, requiring the model to perform complex visual ideation to satisfy dual stylistic constraints simultaneously. This represents an open-ended creative process focused on refinement and the technical control of latent noise to influence artistic output.",1a4b3d0814833bd0c011bf25d036e877d479d90c:Stylized Image Generation Evaluation,Stylized Image Generation Evaluation,https://github.com/cuixing100876/InstaStyle,2.0,GPU/Local,2.0,False,False,True,github,huggingface,image
20f408f62be92cd36bbcbdff4ceda7409baf9262,PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction,PersonaVlog: Personalized Multimodal Vlog Generation with Multi-Agent Collaboration and Iterative Self-Correction,2025.0,2025.0,,Performing Arts & Video Synthesis,Narrative & Story Writing; Visual Arts & Stylized Imagery,high,The paper's core focus is the generation of multimodal Vlogs (video sequences) which requires the integration of narrative storyboards and consistent visual imagery across a temporal sequence.,"**Creative Artifacts Evaluated:**
The benchmark evaluates narrative-driven text storyboards, personalized keyframe images featuring consistent human characters, and short-form video sequences (Vlogs) synthesized from these frames.

**Creative Capabilities Assessed:**
Assessment focuses on narrative quality, specifically story interest, temporal continuity, behavioral diversity, and thematic consistency. Visual creativity is measured through character consistency (maintaining identity across frames), text-image alignment, and technical video aesthetics, including motion smoothness, dynamic degree, and imaging quality.

**Evaluation Context:**
This multimodal framework employs a hybrid evaluation approach. High-level creative elements (storytelling and themes) are judged by MLLMs on a 1–5 scale. Technical and visual fidelity are assessed via automated metrics, including CLIP scores for semantic alignment and skeleton keypoint Euclidean distance to quantify character-level visual consistency.

**Task Characteristics:**
The domain involves multi-stage, constrained generation where a persona and theme must be translated across modalities (text-to-image-to-video). It emphasizes iterative refinement and cross-modal synthesis, requiring the model to maintain a stable identity and logical narrative flow throughout a complex, multi-agent creative pipeline. This distinguishes it from single-prompt generation by focusing on long-range character and thematic stability.",20f408f62be92cd36bbcbdff4ceda7409baf9262:ThemeVlogEval,ThemeVlogEval,https://personavlog-paper.github.io/,2.0,GPU/Local,3.0,False,False,True,unknown,openai; none,multimodal
20be3ba3f361d9630cc0c17442a0d0132873e63d,Benchmarking Language Model Creativity: A Case Study on Code Generation,Benchmarking Language Model Creativity: A Case Study on Code Generation,2024.0,2024.0,,Programming & Algorithmic Creativity,Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates the generation of novel and functional Python code for competitive programming, focusing on both technical correctness and the originality of algorithmic approaches.","**Creative Artifacts Evaluated:**
The primary artifacts are Python source code solutions for complex competitive programming problems sourced from Codeforces. These represent functional, logic-driven scripts designed to solve specific algorithmic challenges while adhering to arbitrary, often restrictive, formatting or structural requirements.

**Creative Capabilities Assessed:**
The benchmark evaluates a dual-axis model of creativity. Convergent thinking is assessed through technical correctness (functional pass rates via unit tests) and strict adherence to task-specific constraints. Divergent thinking is quantified by the novelty and originality of the programming techniques used, specifically measuring the ratio of unique algorithmic approaches employed by the model compared to a reference set of human-authored solutions.

**Evaluation Context:**
This is a text-based code generation context using Large Language Models (LLMs). Evaluation utilizes the NEOGAUGE metric, a hybrid approach combining automated execution for correctness with LLM-based judgment for constraint verification and technique identification. This framework moves beyond simple ""pass@k"" metrics to assess the qualitative uniqueness of the code.

**Task Characteristics:**
Tasks involve constrained code generation and algorithmic problem-solving. They are characterized by open-endedness within a fixed logical framework, requiring models to navigate specific limitations (e.g., avoiding certain libraries or loops) to find unconventional but valid solutions, forcing the model into less common regions of the solution space.",20be3ba3f361d9630cc0c17442a0d0132873e63d:NEOCODER,NEOCODER,https://github.com/JHU-CLSP/NeoCoder,2.0,GPU/Local,1.0,False,False,True,github,openai,text
20ad9e51e946d283f3da93ea1d7a35b01b27a5e1,One Joke to Rule them All? On the (Im)possibility of Generalizing Humor,One Joke to Rule them All? On the (Im)possibility of Generalizing Humor,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper explicitly focuses on the classification and generalization of various humor genres like satire and dad jokes, while also addressing linguistic incongruity and sarcasm.","**Creative Artifacts Evaluated:**
The study focuses on short-form textual humor across four distinct sub-genres: user-submitted product inquiries (Amazon Questions), standalone one-liner sentences, satirical news headlines (e.g., *The Onion*), and structured ""dad jokes"" sourced from social media.

**Creative Capabilities Assessed:**
The primary capability assessed is **humor judgment**, specifically the model's ability to discriminate between humorous and non-humorous intent. This requires recognizing linguistic incongruity, sarcasm, and comedic timing. Furthermore, the framework evaluates **cross-domain generalization**, measuring whether a model’s understanding of one comedic style (e.g., satire) transfers to another (e.g., wordplay), thereby testing the universality of humor recognition.

**Evaluation Context:**
The evaluation is situated within a text-only LLM framework. It utilizes automatic scoring (Accuracy) to benchmark model performance as a discriminative judge. Unlike generative benchmarks, this focuses on the model's capacity to act as a critic of human-generated creative content.

**Task Characteristics:**
Tasks are structured as binary classification (humorous vs. non-humorous). The methodology emphasizes **cross-domain transfer and evaluation**, moving beyond simple pattern matching to investigate whether humor detection is a monolithic creative skill or a fragmented, domain-specific capability.",20ad9e51e946d283f3da93ea1d7a35b01b27a5e1:Humor Transfer Learning Framework,Humor Transfer Learning Framework,https://github.com/morturr/HumorTransferLearning.git,2.0,GPU/Local,4.0,False,False,True,github,none,text
1acc80d5fd0ffd63a04951becda58b4626802e1d,"CFunModel: A ""Funny"" Language Model Capable of Chinese Humor Generation and Processing","CFunModel: A ""Funny"" Language Model Capable of Chinese Humor Generation and Processing",2025.0,2025.0,,Humor & Satire,Dialogue Generation & Social Interaction; Performing Arts & Video Synthesis,high,"The paper focuses on Chinese humor generation and processing, specifically utilizing multi-turn crosstalk scripts that require role-specific dialogue and performance-oriented structures.","**Creative Artifacts Evaluated:**
The model processes and generates short-form Chinese jokes and long-form crosstalk (*Xiangsheng*) scripts. These artifacts include single-sentence punchlines, narrative jokes, and multi-turn comedic dialogues specifically structured for the traditional roles of *Dougen* (the lead comedian) and *Penggen* (the foil).

**Creative Capabilities Assessed:**
Assessment focuses on humor recognition (distinguishing comedic intent from plain text), contextual appropriateness in dialogue, and the linguistic agility required for joke continuation. The benchmark measures the model’s grasp of cultural nuance, comedic timing, and the specific structural requirements of traditional Chinese performance art, such as setup-and-punchline dynamics.

**Evaluation Context:**
The research utilizes the CFunSet benchmark, a new, large-scale Chinese humor dataset. Evaluation is hybrid: discriminative tasks (recognition and response selection) are measured via automatic accuracy metrics, while generative tasks (joke and crosstalk creation) rely on human qualitative assessment through case studies to judge subjective comedic quality and narrative flow.

**Task Characteristics:**
Tasks range from constrained classification and multiple-choice selection to open-ended generation and thematic continuation. A distinguishing feature is the role-specific generation, which requires the model to maintain distinct personas and conversational logic within a collaborative, multi-turn comedic framework.",1acc80d5fd0ffd63a04951becda58b4626802e1d:CFunSet,CFunSet,https://huggingface.co/datasets/ZhenghanYU/CFunSet,1.0,API-Only,4.0,True,True,True,huggingface,openai; huggingface,text
1c3a3c217a1deab466bd1db854e33aed3ff8880d,FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article,FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article,2025.0,2025.0,,Scientific Discovery,Functional & Professional Writing; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating novel research hypotheses and prospective problem statements for scientific articles, which directly aligns with the Scientific Discovery domain's emphasis on generating actionable research insights.","**Creative Artifacts Evaluated**
The primary artifacts are technical ""Future Work"" sections of scientific articles. These consist of structured research hypotheses, methodological extensions, and prospective problem statements that propose specific, grounded advancements based on the preceding content of a scientific paper.

**Creative Capabilities Assessed**
The benchmark measures divergent thinking through novelty and ideation, alongside convergent thinking via feasibility and technical relevance. It specifically assesses the ability to generate ""non-hallucinated"" creative extensions that maintain logical coherence and readability while proposing scientifically plausible and useful research trajectories.

**Evaluation Context**
This is a text-centric scientific domain benchmark utilizing a Retrieval-Augmented Generation (RAG) framework. It employs a tripartite evaluation strategy: traditional NLP metrics (ROUGE, BERTScore, Jaccard/Cosine similarity), a multi-dimensional LLM-as-judge suite (scoring novelty on a 0–10 scale and feasibility via binary classification), and human verification of alignment with ground-truth research gaps.

**Task Characteristics**
Tasks include constrained ideation (generating future directions from specific paper contexts), information extraction (identifying existing research gaps), and critical evaluation/judgment. The work is distinguished by its focus on ""grounded creativity""—the ability to project future scientific progress within the rigid constraints of existing data rather than unconstrained or purely aesthetic generation.",1c3a3c217a1deab466bd1db854e33aed3ff8880d:FutureGen,FutureGen,https://huggingface.co/datasets/iaadlab/FutureGen,1.0,API-Only,3.0,True,True,True,huggingface,openai,text
1b1432e9a68eb0b56de584428b352adb377a53bd,"Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models","Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Scientific Discovery,high,"The paper focuses on evaluating associative creativity and divergent thinking through word association tasks, which aligns with the cognitive benchmarks and 'outside-the-box' reasoning described in the Lateral Thinking domain, while also proposing a novel scientific metric (PACE) for LLM evaluation.","**Creative Artifacts Evaluated:**
The primary artifacts are **parallel word association chains** consisting of discrete textual tokens. These are generated as three independent sequences of three words each, stemming from standardized seed words (e.g., ""bear,"" ""snow,"" ""toaster"") derived from the Intercontinental Dictionary Series.

**Creative Capabilities Assessed:**
The benchmark measures **associative creativity** and **divergent thinking**. Specifically, it evaluates **semantic distance** (the originality and remoteness of associations) using vector space analysis. It also assesses **lexical diversity** (Type-Token Ratio), **conceptual concreteness**, and the structural nature of associations, distinguishing between **syntagmatic** (thematic/contextual) and **paradigmatic** (substitutable/category-based) relationships.

**Evaluation Context:**
This is a **text-based LLM evaluation** framework. It utilizes an **automatic scoring metric** (PACE) based on cosine distance within FastText word embeddings, offering a scalable, objective alternative to subjective human judgment. The metric’s validity is established through correlation with human-preference rankings (Chatbot Arena) and LLM-as-judge creative writing leaderboards (EQ-Bench).

**Task Characteristics:**
The tasks involve **open-ended, constrained generation** focused on **ideation**. Models must perform **parallel association**, requiring them to explore multiple semantic paths simultaneously from a single prompt while avoiding sequential bias. This isolates the foundational cognitive process of ""remote association"" from complex narrative synthesis or stylistic flair.",1b1432e9a68eb0b56de584428b352adb377a53bd:PACE (Parallel Association Chain Evaluation),PACE (Parallel Association Chain Evaluation),https://github.com/ziliang6/PACE,1.0,API-Only,2.0,True,False,True,github,openai; anthropic; google; together; huggingface,text
2108eec365e3e4eee42de37addd6d10b1eeb17ca,Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs,Idea-2-3D: Collaborative LMM Agents Enable 3D Model Generation from Interleaved Multimodal Inputs,2024.0,2024.0,,"3D, Spatial & Architectural Design",Visual Arts & Stylized Imagery; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation of 3D digital models from complex multimodal inputs, requiring spatial reasoning and structural fidelity while synthesizing aesthetic and conceptual elements.","**Creative Artifacts Evaluated:**
The primary artifacts are 3D digital models generated from ""IDEAs""—complex, interleaved multimodal prompts that combine natural language descriptions, 2D reference images, and existing 3D assets. These outputs represent spatial interpretations of abstract or creative concepts.

**Creative Capabilities Assessed:**
Assessment focuses on conceptual alignment (IDEA satisfaction), structural fidelity, and aesthetic appeal. The benchmark measures the system’s ability to synthesize heterogeneous, open-ended inputs into a coherent 3D form, evaluating both the technical accuracy of the cross-modal translation and the subjective quality of the creative execution.

**Evaluation Context:**
The evaluation operates within a high-complexity multimodal framework (text, image, and 3D). It utilizes a hybrid scoring approach: automatic metrics (CLIP and ULIP-2) quantify semantic and structural similarity between inputs and outputs, while human participants provide qualitative judgments on user preference and prompt compliance.

**Task Characteristics:**
The core task is open-ended, multimodal generation characterized by cross-domain synthesis. Unlike standard text-to-3D tasks, this involves interpreting interleaved constraints, requiring the system to perform conceptual ideation and refinement. It emphasizes the integration of diverse creative inputs to produce a unified, high-fidelity 3D representation.",2108eec365e3e4eee42de37addd6d10b1eeb17ca:Eval3DAIGC-198,Eval3DAIGC-198,https://idea23d.github.io/,2.0,GPU/Local,2.0,False,True,True,huggingface,openai,multimodal
2171d913a9845de4da95a90b1ca3bbcb8194b72a,Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation,Large Language Models as Biomedical Hypothesis Generators: A Comprehensive Evaluation,2024.0,2024.0,,Scientific Discovery,Dialogue Generation & Social Interaction,high,"The paper focuses on generating novel biomedical hypotheses and evaluating their technical feasibility and originality, which aligns with Scientific Discovery, while the use of a multi-agent framework with specialized roles (Analyst, Scientist, Critic) incorporates elements of Dialogue Generation & Social Interaction.","**Creative Artifacts Evaluated:**
Structured biomedical hypotheses that propose novel relationships or mechanisms based on existing scientific literature. These artifacts are not general ideas but specific, scientifically grounded propositions intended to predict or drive future research.

**Creative Capabilities Assessed:**
The framework measures scientific originality (novelty), domain-specific utility (significance and relevance), and technical feasibility (verifiability). It also assesses linguistic fluency and the diversity of generated ideas (via SelfBLEU) to ensure the models are not merely reproducing training data or generating repetitive concepts.

**Evaluation Context:**
This is a text-based evaluation utilizing a unique temporal data split (training on historical data to predict ""future"" discoveries) to rigorously test for true novelty. Evaluation is a hybrid of traditional NLP metrics (BLEU, ROUGE), multi-dimensional LLM-as-a-judge scoring (GPT-4), and expert human validation. The context spans both standalone generation and a collaborative multi-agent framework involving specialized roles like Analyst, Scientist, and Critic.

**Task Characteristics:**
The tasks involve constrained ideation and predictive generation. Unlike open-ended creative writing, these tasks require synthesizing complex background knowledge to identify gaps in scientific understanding. The multi-agent approach introduces iterative refinement and critical evaluation as core components of the creative process, emphasizing scientific rigor over pure divergent thinking.",2171d913a9845de4da95a90b1ca3bbcb8194b72a:LLM4BioHypoGen,LLM4BioHypoGen,https://github.com/TsinghuaC3I/LLM4BioHypoGen,1.0,API-Only,2.0,True,False,True,github,openai,text
222715cde1948ebe255099084c83f24c719fc917,IRFL: Image Recognition of Figurative Language,IRFL: Image Recognition of Figurative Language,2023.0,2023.0,,Figurative Language & Rhetoric,Visual Arts & Stylized Imagery; Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on idioms, metaphors, and similes (figurative language) and requires models to perform symbolic mapping and abstract reasoning to connect these linguistic concepts with visual imagery.","**Creative Artifacts Evaluated:**
The benchmark evaluates figurative linguistic constructs—specifically idioms, metaphors, and similes—paired with their visual counterparts. These include both existing images that represent figurative meanings and AI-generated images produced from figurative prompts.

**Creative Capabilities Assessed:**
The primary focus is on conceptual depth and abstract reasoning. The benchmark measures a model’s ability to transcend literal interpretations to achieve symbolic mapping. This involves cross-modal divergent thinking, where the model must associate abstract linguistic concepts with non-literal, symbolic visual imagery rather than surface-level depictions.

**Evaluation Context:**
This multimodal framework utilizes Vision-Language Models (VLMs) and Diffusion models. Evaluation is hybrid, employing automatic metrics (Accuracy, Mean Precision) for discriminative tasks and human classification for generative outputs. Human judges specifically categorize images to determine if the model successfully captured the figurative essence or defaulted to literalism.

**Task Characteristics:**
The benchmark features both discriminative and generative tasks, including multiple-choice selection, retrieval, and constrained generation. These tasks require the model to navigate semantic ambiguity and perform cross-domain transfer, moving from abstract text to symbolic visual representations while filtering out literal ""distractor"" images.",222715cde1948ebe255099084c83f24c719fc917:Image Recognition of Figurative Language (IRFL),Image Recognition of Figurative Language (IRFL),https://huggingface.co/datasets/lampent/IRFL,2.0,GPU/Local,3.0,False,True,True,huggingface,,multimodal
256ac7f8b1b7fe81e41b5b2af28c179f56611576,PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation,PoetryDiffusion: Towards Joint Semantic and Metrical Manipulation in Poetry Generation,2023.0,2023.0,,Poetry & Verse,Figurative Language & Rhetoric,high,"The paper focuses exclusively on the generation of structured poetic forms like English sonnets and Chinese SongCi, emphasizing adherence to rigid metrical, tonal, and rhyme constraints.","**Creative Artifacts Evaluated:**
The research evaluates two distinct forms of structured text: 14-line English sonnets with specific rhyme schemes and classical Chinese SongCi poetry, which follows rigid metrical templates (CiPai) and tonal patterns.

**Creative Capabilities Assessed:**
The study assesses the model's ability to perform joint semantic and metrical manipulation. Key capabilities include structural adherence (precise line counts, rhyme schemes, and tonal patterns), linguistic fluency, and thematic coherence. It also measures ""poeticness,"" a qualitative assessment of aesthetic value and emotional resonance beyond literal meaning.

**Evaluation Context:**
This is a text-based generative task utilizing diffusion-based Large Language Models. Evaluation is a hybrid of standard NLP metrics (BLEU, ROUGE, Perplexity), domain-specific automated checks for formal accuracy (rhyme and tone matching), and human expert judgment using a 1-5 Likert scale to evaluate nuanced creative quality.

**Task Characteristics:**
The work centers on highly constrained generation, where the model must satisfy simultaneous formal and semantic requirements. Unlike open-ended creative writing, these tasks require the model to solve a multi-objective optimization problem: maintaining poetic tradition and rigid structural rules while ensuring the resulting text is meaningful and contextually relevant.",256ac7f8b1b7fe81e41b5b2af28c179f56611576:Sonnet Generation,Sonnet Generation,https://github.com/ChorlingLau/PoetryDiffusion/,2.0,GPU/Local,1.0,False,False,True,github,none,text
269e13fb1fd065ddcf1d79547f6a58ea846b5f8a,FigCLIP: A Generative Multimodal Model with Bidirectional Cross-attention for Understanding Figurative Language via Visual Entailment,FigCLIP: A Generative Multimodal Model with Bidirectional Cross-attention for Understanding Figurative Language via Visual Entailment,2024.0,2024.0,,Figurative Language & Rhetoric,Humor & Satire; Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on interpreting metaphors and irony through multimodal reasoning, which are core components of figurative language, while also addressing humor and cross-domain conceptual mapping.","**Creative Artifacts Evaluated:**
Multimodal pairings consisting of digital images and figurative text, specifically encompassing metaphors, irony, and humor. The model’s primary creative output is a natural language explanation that decodes the non-literal relationship between the visual and textual components.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and semantic reasoning, specifically the ability to interpret abstract intent rather than literal descriptions. It assesses the model’s capacity for cross-modal synthesis—linking visual cues to figurative concepts—and its linguistic fluency in articulating the underlying logic of irony or metaphor.

**Evaluation Context:**
This multimodal VLM evaluation utilizes the FigLang-2024 Shared Task. It employs a unique hybrid automatic scoring mechanism (F1@50 and F1@60) that conditions discriminative classification accuracy (entailment vs. contradiction) on the semantic quality of generated explanations, as measured by BERTscore against reference human interpretations.

**Task Characteristics:**
The work involves a ""Visual Entailment"" framework applied to creative language. It requires a combination of constrained judgment and open-ended generation. The task is characterized by cross-domain transfer between visual perception and high-level linguistic abstraction, focusing on resolving semantic ambiguity and interpreting complex rhetorical devices.",269e13fb1fd065ddcf1d79547f6a58ea846b5f8a:FigLang-2024 Multimodal Figurative Language Shared Task,FigLang-2024 Multimodal Figurative Language Shared Task,https://huggingface.co/datasets/ColumbiaNLP/V-FLUTE,2.0,GPU/Local,1.0,False,True,True,huggingface,huggingface; openai,multimodal
234d5dc1fe156c8d79beb373525d4b1bc177db9d,CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation,CHIMERA: A Knowledge Base of Scientific Idea Recombinations for Research Analysis and Ideation,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on scientific ideation and research analysis through conceptual recombinations, which directly aligns with Scientific Discovery, while utilizing analogical mapping and cross-domain transfer characteristic of Lateral Thinking.","**Creative Artifacts Evaluated:**
Scientific idea recombinations, specifically conceptual blends (merging two ideas into a new one) and inspirations (applying a concept from one domain to another), represented as text spans and relational links within research abstracts.

**Creative Capabilities Assessed:**
Combinatorial creativity and scientific ideation. The benchmark measures the ability to identify existing creative links (extraction) and propose novel, useful concept pairings (prediction) that demonstrate conceptual depth and research utility.

**Evaluation Context:**
A text-based scientific domain using LLMs. Evaluation is multi-layered, employing standard extraction metrics (F1), LLM-based semantic soft-matching (GPT-4o-mini) to judge entity similarity, and human expert ranking of ""helpfulness"" for real-world research inspiration.

**Task Characteristics:**
The benchmark combines discriminative tasks (classifying and extracting creative relationships from existing literature) with generative ideation (predicting novel recombinations). It bridges information extraction with open-ended creative problem-solving and constrained retrieval, focusing on the cross-domain transfer of scientific concepts.",234d5dc1fe156c8d79beb373525d4b1bc177db9d:Recombination Extraction,Recombination Extraction,https://github.com/noy-sternlicht/CHIMERA-KB,2.0,GPU/Local,3.0,False,True,True,huggingface,openai; huggingface,text
236c90554273ca3e07fff78bd9b2d9c50de2cf74,GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation,GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation,2025.0,2025.0,,Scientific Discovery,Functional & Professional Writing,high,"The paper evaluates scientific research abstracts and peer-review decisions, focusing on the novelty and impact of research hypotheses within STEM disciplines.","**Creative Artifacts Evaluated:**
The framework evaluates scientific research abstracts and peer review decisions, specifically focusing on high-level conceptual frameworks within the machine learning domain (e.g., ICLR and NeurIPS papers). It also examines ""atomic facts"" in long-form text and extracted ""viewpoints"" that represent distilled research claims.

**Creative Capabilities Assessed:**
The primary focus is on expert-level judgment of research quality, specifically novelty (the ability to detect plagiarized or derivative work) and the potential impact or utility of scientific hypotheses. It assesses the model's capacity for conceptual depth and technical discernment by predicting peer-review outcomes (e.g., Accept vs. Reject).

**Evaluation Context:**
This is a text-based evaluation using a lightweight graph-based framework. It utilizes automatic scoring (Accuracy, Macro F1, and normed token cost) alongside LLM-as-a-judge (LLaMa-3.1 405b) to validate the factual consistency of intermediate reasoning steps. The system is benchmarked against historical human expert decisions.

**Task Characteristics:**
The tasks are discriminative and evaluative rather than generative. They involve classifying complex research ideas into discrete quality tiers (Reject, Poster, Oral) and performing novelty assessment. The process emphasizes structured ideation through ""viewpoint"" extraction and graph-based aggregation to simulate the multi-faceted nature of expert peer review.",236c90554273ca3e07fff78bd9b2d9c50de2cf74:ICLR Papers,ICLR Papers,https://github.com/ulab-uiuc/GraphEval,2.0,GPU/Local,2.0,False,False,True,github,openai,text
24599e9a55091d6f417e91799e63ff14797cf4be,TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models,TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models,2024.0,2024.0,,Visual Arts & Stylized Imagery,Dialogue Generation & Social Interaction,high,"The paper focuses on the generation and retrieval of novel, imaginative, and counterfactual images, while also situating these tasks within multi-turn conversational dialogue contexts.","**Creative Artifacts Evaluated:**
The primary artifacts are novel, counterfactual, and imaginative images generated from text prompts. These include knowledge-intensive visuals and photorealistic scenes that do not exist in standard datasets, as well as images acquired or generated within the context of multi-turn conversational dialogues.

**Creative Capabilities Assessed:**
The research assesses the model’s ability to handle conceptual novelty and counterfactual reasoning. Key metrics evaluate semantic alignment (CLIP-T) and visual consistency (CLIP-I). A distinguishing capability is ""modality decision-making""—the model’s autonomous judgment in determining whether a prompt necessitates the creation of an original, non-existent image or the retrieval of a relevant existing one.

**Evaluation Context:**
The evaluation is situated in a multimodal (text-to-image) framework utilizing Large Multimodal Models (LMMs). It relies on automatic scoring, specifically CLIP-based similarity metrics for generation and Recall (R@k) for retrieval. The context spans both static, prompt-based benchmarks and dynamic, multi-turn conversational settings.

**Task Characteristics:**
Tasks are characterized by a hybrid ""unified"" approach, combining open-ended generation with constrained retrieval. This requires autonomous selection between generating new content and searching existing databases. The work also features multi-turn dialog-based image acquisition and cross-domain evaluation, specifically contrasting ""creative"" imaginative prompts with ""knowledge-intensive"" factual prompts.",24599e9a55091d6f417e91799e63ff14797cf4be:TIGeR-Bench,TIGeR-Bench,https://tiger-t2i.github.io,2.0,GPU/Local,3.0,False,True,True,huggingface,none,multimodal
2445bf811a2d2249ee55fa1bdcadedca9aada66b,GuessBench: Sensemaking Multimodal Creativity in the Wild,GuessBench: Sensemaking Multimodal Creativity in the Wild,2025.0,2025.0,,"3D, Spatial & Architectural Design",Lateral Thinking & Creative Problem Solving; Visual Arts & Stylized Imagery,high,"The paper focuses on the interpretation and generation of 3D Minecraft structures, requiring models to perform lateral reasoning to map abstract voxel-based geometry to real-world concepts.","**Creative Artifacts Evaluated:**
The primary artifacts are user-generated Minecraft builds, captured as both static 2D images of completed structures and dynamic three-stage image sequences showing progressive construction. The generative component evaluates synthetic Minecraft build images produced by models based on conceptual prompts.

**Creative Capabilities Assessed:**
The benchmark measures ""sensemaking creativity,"" specifically the ability to interpret abstract or low-fidelity creative intent from voxel-based visual cues. It assesses interpretive accuracy, divergent thinking (guessing concepts from minimal hints), and conceptual depth by requiring models to map blocky, non-photorealistic geometry to real-world entities. The generative task evaluates technical correctness and plausibility within the specific constraints of the Minecraft aesthetic.

**Evaluation Context:**
This is a multimodal evaluation of Vision Language Models (VLMs). Scoring for interpretation tasks is automated via an LLM-as-a-judge (GPT-4o) to extract and validate accuracy from open-ended natural language responses. The generative task utilizes human judgment focused on entity presence and structural plausibility.

**Task Characteristics:**
Tasks include open-ended interpretation (guessing concepts from images and text hints), dynamic visual reasoning (interpreting evolving build sequences), and constrained generation. It differentiates itself from standard visual reasoning by focusing on ""in-the-wild"" creative expressions where the visual representation is intentionally abstract or stylized rather than literal.",2445bf811a2d2249ee55fa1bdcadedca9aada66b:GuessBench,GuessBench,https://arxiv.org/abs/2506.00814,2.0,GPU/Local,3.0,False,True,True,huggingface,openai; google,multimodal
2af6b9d54651808f87afc7030ab93c0baaf68989,ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline,ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline,2025.0,2025.0,,Programming & Algorithmic Creativity,Scientific Discovery; Narrative & Story Writing,high,"The paper evaluates the construction of languages as formal, rule-based systems, emphasizing internal logical consistency and technical correctness akin to algorithmic design, while also incorporating linguistic typology and world-building elements.","**Creative Artifacts Evaluated:**
The primary artifacts are constructed languages (conlangs), which include formal grammatical rule sets (morphology and syntax), phonological systems, and translated sentence strings generated according to those self-defined linguistic frameworks.

**Creative Capabilities Assessed:**
The framework measures divergent thinking through typological diversity, assessing the model's ability to generate varied linguistic structures across a population of outputs. It also evaluates technical correctness and internal logical consistency, specifically the model’s ability to adhere to its own established rules during translation tasks, ensuring the generated system is functional rather than merely aesthetic.

**Evaluation Context:**
This text-based evaluation utilizes a multi-hop LLM pipeline. Scoring is a hybrid of automated and expert methods: an LLM judge classifies languages based on 16 WALS (World Atlas of Language Structures) features to calculate mean pairwise Hamming distances for diversity. Internal consistency is measured via LLM-derived ""Consistency Rates"" and validated by human expert ordinal ratings, with agreement measured by Spearman ρ and weighted κ.

**Task Characteristics:**
The domain involves complex, open-ended system design followed by constrained generation. It requires high-level conceptual depth to build a coherent rule-based framework and the systematicity to apply those rules across multi-step reasoning tasks, transitioning from abstract linguistic ideation to practical translation.",2af6b9d54651808f87afc7030ab93c0baaf68989:ConlangCrafter Evaluation Framework,ConlangCrafter Evaluation Framework,https://github.com/m-alper/conlangcrafter,1.0,API-Only,2.0,True,False,True,github,openai; google,text
28a3582ecab72e2a91ec9004075d744b8bac4640,IdeaBench: Benchmarking Large Language Models for Research Idea Generation,IdeaBench: Benchmarking Large Language Models for Research Idea Generation,2024.0,2024.0,,Scientific Discovery,,high,"The paper explicitly focuses on benchmarking the generation of scientific research hypotheses and ideas, emphasizing novelty, technical feasibility, and the synthesis of existing literature into actionable insights.","**Creative Artifacts Evaluated:**
The primary artifacts are structured, domain-specific scientific research hypotheses and detailed research ideas. These are text-based outputs where the LLM assumes the persona of a specialized researcher to propose novel directions for scientific inquiry across various academic disciplines.

**Creative Capabilities Assessed:**
The benchmark primarily evaluates ""Insight,"" a composite metric encompassing novelty (originality) and feasibility (technical and practical viability). It also assesses the model's ability to minimize idea overlap (redundancy) and its semantic similarity to existing literature, measuring the balance between incremental progress and radical innovation. These metrics focus on conceptual depth and the synthesis of existing knowledge into new, actionable propositions.

**Evaluation Context:**
This is a text-based evaluation framework utilizing an LLM-as-a-judge approach. GPT-4o is employed to rank and rate the quality of generated ideas, supplemented by BERTScore for automated semantic analysis. The context is highly specialized, requiring the evaluation system to distinguish between plausible scientific advancement and hallucinated or trivial suggestions within a research-heavy environment.

**Task Characteristics:**
The task involves constrained, domain-specific ideation and scientific problem-solving. It requires the model to synthesize complex background knowledge into actionable research propositions. This distinguishes the work from open-ended creative writing by enforcing rigorous standards of scientific logic, technical relevance, and scholarly innovation.",28a3582ecab72e2a91ec9004075d744b8bac4640:IdeaBench,IdeaBench,https://anonymous.4open.science/r/IdeaBench-2747/,1.0,API-Only,1.0,True,False,True,github,openai,text
2a9db8d5445cf3f078ec608966cac6e09597ce74,Creative Painting with Latent Diffusion Models,Creative Painting with Latent Diffusion Models,2022.0,2022.0,,Visual Arts & Stylized Imagery,Lateral Thinking & Creative Problem Solving,high,"The paper centers on the generation of artistic digital paintings and surrealist visual compositions, while explicitly evaluating the model's capacity for divergent thinking and the translation of abstract themes into creative imagery.","**Creative Artifacts Evaluated**
The model generates artistic digital paintings and thematic visual compositions. These include surrealist object designs (e.g., furniture mimicking natural forms like avocados) and complex scenes representing high-level socio-political concepts, such as the ""urbanization of China,"" rendered in the styles of specific historical artists.

**Creative Capabilities Assessed**
Assessment focuses on aesthetic quality, stylistic fidelity, and conceptual depth. The evaluation measures the model’s expressiveness and its ability to incorporate ""environmental and humane information"" into visual outputs. It specifically tests the capacity for divergent thinking by translating abstract, high-level themes into detailed, evocative visual narratives.

**Evaluation Context**
This research operates in a multimodal (text-to-image) framework using Latent Diffusion Models. The evaluation is primarily qualitative and author-led, relying on visual inspection to compare outputs against baseline models. It emphasizes the subjective impact of the imagery rather than automated metrics like FID or CLIP scores.

**Task Characteristics**
Tasks involve both constrained generation from specific prompts and open-ended generation from abstract themes. A key characteristic is the use of prompt extension, where high-level ideas are refined into descriptive instructions. This represents a process of ideation and refinement, moving from abstract conceptualization to concrete artistic execution.",2a9db8d5445cf3f078ec608966cac6e09597ce74:Comparative Artistic Generation from LDM Prompts,Comparative Artistic Generation from LDM Prompts,,2.0,GPU/Local,1.0,False,False,True,unknown,none,image
29203f0b8b9be7fd70d99bf7390c6a78b68a9289,Conceptual Design Generation Using Large Language Models,Conceptual Design Generation Using Large Language Models,2023.0,2023.0,,Engineering & Technical Design,Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates the generation of conceptual engineering solutions and mechanism proposals, focusing on functional creativity, technical feasibility, and divergent ideation within a technical design context.","**Creative Artifacts Evaluated:**
Textual conceptual design solutions for 12 open-ended engineering problems. These artifacts consist of functional descriptions and mechanism proposals intended to solve specific technical challenges, such as assistive devices for the disabled or novel consumer product architectures.

**Creative Capabilities Assessed:**
The study assesses functional creativity through the lenses of novelty, feasibility, and usefulness (utility). It further evaluates divergent thinking by measuring the diversity and semantic spread of generated solution sets, as well as the model's ability to replicate or exceed the conceptual range and ""design space"" coverage of human-generated baselines.

**Evaluation Context:**
The context is purely textual, utilizing GPT-3 variants across zero-shot and few-shot prompting strategies. Evaluation is a rigorous hybrid of human expert judgment—employing the Consensual Assessment Technique (CAT) on a 3-point scale—and automated computational metrics. The latter utilizes SentenceBERT embeddings to calculate convex hull hypervolumes (for diversity) and nearest-neighbor cosine similarities (for human-alignment).

**Task Characteristics:**
The tasks focus on open-ended ideation and technical problem-solving. This involves unconstrained generation where the model must synthesize functional requirements into plausible engineering concepts. The research is distinguished by its focus on ""useful"" creativity within a technical domain, specifically comparing LLM-generated solution distributions against crowdsourced human design data to determine if AI can match human-level engineering intuition.",29203f0b8b9be7fd70d99bf7390c6a78b68a9289:Conceptual Design Generation Problems,Conceptual Design Generation Problems,https://github.com/kevinma1515/gpt_idetc,3.0,Special,4.0,False,False,True,github,openai,text
28085f480ce456a376ebace9b899e3bc93dbc048,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,TinyStories: How Small Can Language Models Be and Still Speak Coherent English?,2023.0,2023.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation of short synthetic stories and evaluates them based on narrative coherence and consistency, while also incorporating diagnostic tasks for logical and common-sense reasoning.","**Creative Artifacts Evaluated:**
The primary artifacts are short, synthetic stories characterized by a preschool-level vocabulary. These include open-ended story completions and constrained narratives generated from specific prompts or instruction sets (e.g., requiring the inclusion of specific words, themes, or plot endings).

**Creative Capabilities Assessed:**
Assessment targets linguistic fluency (grammar), narrative consistency (plot coherence and context-tracking), and creative originality. It also measures ""common-sense"" reasoning and factual accuracy within a narrative framework, alongside the model's ability to adhere to stylistic or content-based constraints. Diversity is quantified by measuring the novelty of generated text against training data to distinguish between creative synthesis and rote memorization.

**Evaluation Context:**
The research focuses on the text modality, specifically targeting Small Language Models (SLMs) ranging from 1M to 35M parameters. Evaluation employs a hybrid approach: automated qualitative scoring via GPT-4 (GPT-Eval) for subjective traits, human qualitative assessment for logical reasoning, and automatic n-gram metrics (Rouge) for diversity and memorization analysis.

**Task Characteristics:**
Tasks encompass open-ended generation (completing a story from a prompt) and constrained generation (instruction-following). Additionally, the framework includes diagnostic tasks for logical consistency, cause-and-effect reasoning, and factual grounding within a creative writing context.",28085f480ce456a376ebace9b899e3bc93dbc048:TinyStories,TinyStories,https://huggingface.co/datasets/roneneldan/TinyStories,2.0,GPU/Local,1.0,False,True,True,huggingface,none,text
26c397ae35fa0d521a6b30e578924861a06d8cdd,Evaluating the Creativity of LLMs in Persian Literary Text Generation,Evaluating the Creativity of LLMs in Persian Literary Text Generation,2025.0,2025.0,,Figurative Language & Rhetoric,Narrative & Story Writing; Lateral Thinking & Creative Problem Solving,high,"The paper's core focus is evaluating the 'linguistic sophistication' of Persian literary prose by quantifying rhetorical devices such as metaphors and similes, while also employing psychometric metrics (TTCT) typically associated with divergent thinking and creative problem solving.","**Creative Artifacts Evaluated**
The evaluation focuses on single-sentence Persian literary texts. These artifacts are generated based on five culturally significant themes: love, longing, friendship, nature, and wisdom. The study utilizes the CPers (Creativity in Persian) dataset, comprising 4,371 literary entries, to provide a baseline for these creative outputs.

**Creative Capabilities Assessed**
The framework measures divergent thinking through culturally adapted versions of the Torrance Tests of Creative Thinking (TTCT) metrics: originality, fluency, flexibility, and elaboration. Beyond these standard psychometric traits, the research assesses linguistic sophistication by quantifying the use of specific Persian rhetorical devices (figures of speech), including simile, metaphor, antithesis, and hyperbole.

**Evaluation Context**
This text-based study benchmarks six Large Language Models (LLMs) within a Persian linguistic and cultural context. It employs a hybrid evaluation strategy: an ""LLM-as-judge"" approach (using Claude 3.7 Sonnet and GPT-4o) is validated against human expert ratings using Intraclass Correlation Coefficients (ICC) to ensure scoring reliability.

**Task Characteristics**
The research involves three distinct task types: constrained generation (producing single-sentence literary prose from thematic prompts), meta-evaluation (assessing the capability of LLMs to act as creative critics), and analytical annotation (human-led identification and counting of figurative language). It emphasizes the challenge of cross-cultural creativity and the alignment of computational outputs with traditional literary standards.",26c397ae35fa0d521a6b30e578924861a06d8cdd:CPers (Creativity in Persian),CPers (Creativity in Persian),https://huggingface.co/datasets/teias-ai/CPers,1.0,API-Only,0.0,True,True,True,huggingface,openai; anthropic,text
2a5752c59f6a7e6175815db7b113d23a05271cce,CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation,CreatiLayout: Siamese Multimodal Diffusion Transformer for Creative Layout-to-Image Generation,2024.0,2024.0,,Graphic Design & Visual Layout,Visual Arts & Stylized Imagery,high,"The paper's core focus is on the spatial arrangement and composition of visual elements through layout-to-image generation, while also evaluating the aesthetic quality of the resulting images.","**Creative Artifacts Evaluated:**
The paper evaluates photorealistic images characterized by complex spatial compositions and precise attribute binding. It specifically examines structured visual layouts (bounding boxes paired with entity descriptions) and optimized spatial plans derived from global captions or suboptimal initial sketches.

**Creative Capabilities Assessed:**
Assessment focuses on spatial reasoning and compositional accuracy, specifically the ability to map textual attributes (color, texture, shape) to discrete image regions. It measures technical correctness in layout formatting, numeracy (object counting), and aesthetic quality. The work also evaluates high-level conceptual planning—the ability of an LLM to translate abstract ideas into viable spatial arrangements—and iterative refinement through layout optimization.

**Evaluation Context:**
The research employs a multimodal ""plan-then-generate"" pipeline involving LLMs for layout synthesis and Siamese Multimodal Diffusion Transformers for image generation. Evaluation is exceptionally granular, utilizing VLM-based Visual Question Answering (VQA) for region-wise accuracy, YOLO-based object detection for spatial fidelity, and standard metrics like FID, CLIP, and Pick score for global aesthetic and semantic alignment.

**Task Characteristics:**
Tasks span the creative workflow from open-ended ideation (generating layouts from global captions) to highly constrained generation (layout-to-image synthesis). It includes refinement tasks, where suboptimal layouts are optimized for better composition, and cross-modal translation, bridging linguistic descriptions with precise spatial coordinates.",2a5752c59f6a7e6175815db7b113d23a05271cce:LayoutSAM-Eval,LayoutSAM-Eval,https://creatilayout.github.io/,2.0,GPU/Local,1.0,False,True,True,huggingface,,multimodal
2b070923f46f1a6592e7975cfb01bcd4ba911b96,Unveiling the Invisible: Captioning Videos with Metaphors,Unveiling the Invisible: Captioning Videos with Metaphors,2024.0,2024.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper focuses specifically on generating metaphorical captions and conceptual mapping between literal visual cues and non-literal linguistic domains, which are core components of figurative language and lateral thinking.","**Creative Artifacts Evaluated:**
Single-line metaphorical captions that translate dynamic visual sequences into figurative language. These artifacts represent symbolic interpretations of video content rather than literal, descriptive summaries.

**Creative Capabilities Assessed:**
The benchmark measures conceptual mapping, divergent thinking, and the ability to bridge literal visual input with abstract linguistic output. Specific capabilities include creative originality, linguistic fluency, and ""Primary Concept Consistency,"" which evaluates the model's ability to maintain thematic relevance while navigating the semantic gap between the visual source and the metaphorical target.

**Evaluation Context:**
This multimodal (video-to-text) framework evaluates Multimodal Large Language Models (MLLMs). It utilizes a hybrid scoring approach: standard automatic NLP metrics (BLEU-4, CIDEr, BERT-F1), specialized semantic metrics (Average Concept Similarity and Distance) to quantify metaphorical depth, and human qualitative assessment focusing on creativity and consistency.

**Task Characteristics:**
The task is an open-ended, cross-modal generation challenge. It shifts the paradigm from standard descriptive video captioning to symbolic ideation. It requires models to perform complex cross-domain transfers, mapping concrete visual cues to non-literal, figurative domains to ""unveil"" invisible or implied meanings within the video.",2b070923f46f1a6592e7975cfb01bcd4ba911b96:Video Metaphor Captioning Dataset (VMCD),Video Metaphor Captioning Dataset (VMCD),https://github.com/abisekrk/video-metaphor-captioning,2.0,GPU/Local,1.0,False,False,True,github,none,video
2b5d234efd26e7377698cf16c901601a3d3c4e56,CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities,CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities,2022.0,2022.0,,Narrative & Story Writing,Functional & Professional Writing; Figurative Language & Rhetoric,high,"The paper focuses on a collaborative writing dataset specifically designed for fictional stories and persuasive argumentative essays, evaluating both narrative construction and rhetorical logic.","**Creative Artifacts Evaluated:**
The benchmark evaluates long-form text artifacts, specifically fictional stories and persuasive argumentative essays. These represent two distinct modes of writing: open-ended narrative construction and structured, logic-based rhetoric.

**Creative Capabilities Assessed:**
Assessment centers on linguistic fluency (grammaticality) and lexical diversity. Beyond surface-level quality, it measures ideational novelty through the introduction and reuse of named entities. Unique to this domain is the assessment of collaborative dynamics, including the model's ability to spark new ideas (divergent thinking) and the human’s sense of agency, satisfaction, and perceived ownership over the co-authored text.

**Evaluation Context:**
This is a text-based, human-AI collaborative framework utilizing GPT-3. The evaluation is uniquely multi-dimensional, synthesizing automatic linguistic metrics (via LanguageTool and Stanza), granular interaction event logs (tracking edits, suggestions, and ""mutuality""), and subjective human surveys to capture the nuances of the co-creative process rather than just the final output.

**Task Characteristics:**
Tasks involve real-time collaborative generation and refinement. They span from open-ended creative ideation to constrained argumentative reasoning, focusing on the model's capacity to serve as a ""co-author"" that provides contextually relevant, steerable suggestions that humans can accept, modify, or reject during a live writing session.",2b5d234efd26e7377698cf16c901601a3d3c4e56:CoAuthor,CoAuthor,https://cs.stanford.edu/~minalee/zip/chi2022-coauthor-v1.0.zip,1.0,API-Only,2.0,True,False,True,url,openai,text
31710aec2104caeec51e15c2af9669aa38da97aa,Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions,Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions,2024.0,2024.0,,Humor & Satire,Visual Arts & Stylized Imagery; Figurative Language & Rhetoric,high,"The paper focuses on evaluating AI's ability to comprehend and explain the ironic humor, social commentary, and subtextual contradictions within minimalist comic strips.","**Creative Artifacts Evaluated:**
Two-panel minimalist comics characterized by visual-textual juxtaposition, ironic humor, and social commentary.

**Creative Capabilities Assessed:**
The benchmark measures the capacity for humor comprehension, specifically the ability to identify and articulate contradictory narratives. It assesses conceptual depth (identifying underlying social philosophies), narrative synthesis (matching titles), and the ability to distinguish between literal visual content and subtextual irony. It evaluates whether models can decode the ""twist"" that constitutes the creative core of the artifact.

**Evaluation Context:**
A multimodal framework utilizing Vision-Language Models (VLMs). Evaluation is multifaceted, combining automated linguistic metrics (ROUGE-2, BERT Score) with LLM-as-a-judge (GPT-4) and human ratings for correctness, faithfulness, and completeness. This hybrid approach establishes a rigorous standard for assessing ""understanding"" beyond simple pattern recognition.

**Task Characteristics:**
The tasks span open-ended generation (writing literal descriptions and explaining contradictions) and discriminative judgment (multiple-choice selection for titles and philosophical themes). This dual approach tests both the generative ability to explain creative intent and the evaluative ability to recognize the most fitting creative labels for complex, ironic visual stimuli.",31710aec2104caeec51e15c2af9669aa38da97aa:YESBUT,YESBUT,https://vulab-ai.github.io/YESBUT_Homepage/,1.0,API-Only,4.0,True,True,True,huggingface,openai; google,multimodal
2d45cac81b28850a10f67fbd26928bab3bdd6342,Base Models Beat Aligned Models at Randomness and Creativity,Base Models Beat Aligned Models at Randomness and Creativity,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Poetry & Verse,high,"The study primarily investigates divergent thinking and strategic unpredictability through competitive games and statistical randomness, while also utilizing poetry as a metric for artistic creativity.","**Creative Artifacts Evaluated:**
The research evaluates single random integers (0–10), sequences of uniform random numbers, strategic move selections in competitive games (Rock Paper Scissors and Hide & Seek), and four-line poems centered on everyday topics.

**Creative Capabilities Assessed:**
The study measures divergent thinking through statistical unpredictability and the ability to maintain uniform distributions without repetition bias. It assesses strategic creativity via the deployment of non-deterministic mixed strategies against greedy adversaries. Artistic creativity is evaluated through the lenses of originality, aesthetic pleasantness, and overall human preference.

**Evaluation Context:**
This text-based study compares base LLMs against their instruction-tuned or RLHF-aligned counterparts. It utilizes a hybrid evaluation framework: automatic statistical scoring (Pearson χ² divergence and MSE) for randomness, performance-based win/loss metrics for strategic games, and human-led pairwise comparisons for poetic quality.

**Task Characteristics:**
Tasks encompass highly constrained stochastic generation (producing random sequences), competitive problem-solving (game theory), and semi-constrained artistic ideation (poetry). A distinguishing feature of this work is its focus on the ""alignment tax,"" specifically how fine-tuning for safety and helpfulness may diminish the raw stochasticity and unpredictability essential for creative and non-deterministic outputs.",2d45cac81b28850a10f67fbd26928bab3bdd6342:Random Number Generation,Random Number Generation,https://arxiv.org/abs/2505.00047,2.0,GPU/Local,2.0,False,False,True,unknown,openai; anthropic; google; together; replicate,text
3301fc6a084c7b09a60f24edcf5502c00e421989,ARN: Analogical Reasoning on Narratives,ARN: Analogical Reasoning on Narratives,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Figurative Language & Rhetoric,high,"The paper explicitly focuses on analogical mapping and identifying structural isomorphisms between narratives, which are central to the definition of Lateral Thinking & Creative Problem Solving.","**Creative Artifacts Evaluated:**
The benchmark focuses on short-form text narratives and stories. These artifacts are specifically constructed to contain complex relational structures and causal systems that can be abstracted and mapped across different thematic contexts.

**Creative Capabilities Assessed:**
The primary capability measured is conceptual depth through abstract pattern recognition. Specifically, the benchmark assesses ""system mapping""—the ability to identify structural isomorphisms between narratives while disregarding surface-level or lexical similarities. This evaluates convergent thinking and the cognitive capacity to recognize high-level relational patterns, which are foundational for creative synthesis, metaphor interpretation, and analogical transfer.

**Evaluation Context:**
This is a text-based benchmark designed for Large Language Models (LLMs). Unlike generative creativity tasks, it utilizes a discriminative, binary question-answering format. Evaluation is conducted via automatic scoring (Accuracy), measuring the model's success in selecting the structurally consistent analog from a pair of candidates.

**Task Characteristics:**
The task is defined by evaluation and judgment rather than open-ended generation. It requires cross-domain transfer, as models must map the underlying logic of a query narrative onto a candidate narrative that may belong to a different domain. It tests the ability to move beyond word-level overlap to grasp the abstract schema or ""system"" of a creative work.",3301fc6a084c7b09a60f24edcf5502c00e421989:Analogical Reasoning on Narratives (ARN),Analogical Reasoning on Narratives (ARN),https://github.com/zhivarsourati/ARN,1.0,API-Only,1.0,True,False,True,github,openai,text
31eb5852bf31206d247bef22c8636f48b0c25fe9,Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees,Iterative Hypothesis Generation for Scientific Discovery with Monte Carlo Nash Equilibrium Self-Refining Trees,2025.0,2025.0,,Scientific Discovery,,high,"The paper's core focus is the generation and iterative refinement of formal scientific hypotheses across STEM and social science disciplines, which directly aligns with the Scientific Discovery domain's definition.","**Creative Artifacts Evaluated:**
The paper evaluates the generation of formal scientific hypotheses across three distinct domains: social science (derived from raw web corpora), biomedicine (extracted from research literature pairs), and computer science (synthesized from structured paper components including methods and results).

**Creative Capabilities Assessed:**
Assessment focuses on ""creative synthesis"" through four primary dimensions: novelty (originality and non-triviality), significance (potential impact and conceptual depth), clarity (fluency and structural coherence), and verifiability (the technical feasibility and testability of the proposed idea).

**Evaluation Context:**
This text-based research utilizes a complex iterative refinement framework (Monte Carlo Nash Equilibrium Self-Refining Trees). Evaluation is multi-layered, combining automatic linguistic metrics (BERTScore), LLM-as-a-judge scoring (GPT-3.5), and rigorous human expert validation using 3-point scales to ensure domain-specific accuracy and scientific merit.

**Task Characteristics:**
The work centers on open-ended ideation and iterative refinement. It requires models to perform domain-specific synthesis, transforming raw background data into structured, testable scientific propositions. Unlike general creative writing, these tasks demand a high degree of convergent constraint—balancing divergent ""blue-sky"" thinking with the technical requirements of scientific validity and domain relevance.",31eb5852bf31206d247bef22c8636f48b0c25fe9:MOOSE,MOOSE,https://github.com/ZonglinY/MOOSE,1.0,API-Only,1.0,True,False,True,github,openai; anthropic; google,text
33fa4ce42102b19a6a6fe8a8e14ba8ab1e1c7c7a,WebNovelBench: Placing LLM Novelists on the Web Novel Distribution,WebNovelBench: Placing LLM Novelists on the Web Novel Distribution,2025.0,2025.0,,Narrative & Story Writing,Figurative Language & Rhetoric,high,"The paper focuses exclusively on the generation and evaluation of long-form novels, assessing core narrative elements like character development, plot progression, and stylistic fluency.","**Creative Artifacts Evaluated:**
Long-form Chinese web novels (structured as 10-chapter sequences) and award-winning classic literary novels (Mao Dun Literature Prize winners).

**Creative Capabilities Assessed:**
Narrative coherence, character development, plot progression, and stylistic fluency across eight specific narrative dimensions. The benchmark measures the capacity for sustained world-building and the ability to maintain thematic consistency over extended, multi-chapter contexts, distinguishing it from standard short-form creative writing.

**Evaluation Context:**
A text-based evaluation framework employing LLM-as-Judge (Deepseek-V3). It introduces a sophisticated statistical pipeline using Principal Component Analysis (PCA) for multi-dimensional weight aggregation and Empirical Cumulative Distribution Functions (ECDF) to map model outputs onto a percentile rank relative to a massive distribution of human-authored content.

**Task Characteristics:**
Constrained long-form generation (synopsis-to-story) and cross-domain validation. The research is unique for its ""distributional placement"" approach, which evaluates LLMs not in isolation, but by their relative standing within the actual production quality of the web novel industry. It bridges commercial fiction tropes with classic literary standards to ensure a comprehensive assessment of narrative mastery and creative endurance.",33fa4ce42102b19a6a6fe8a8e14ba8ab1e1c7c7a:WebNovelBench,WebNovelBench,https://huggingface.co/datasets/Oedon42/webnovelbench,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic; google; together,text
3410fd0fc85cb04be40e2dacc7f36668261f8620,"Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs","Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs",2025.0,2025.0,,Poetry & Verse,Figurative Language & Rhetoric,high,The paper presents a benchmark specifically for Arabic poetry across historical eras and focuses on the model's ability to decode and explain complex metaphorical and rhetorical structures within those verses.,"**Creative Artifacts Evaluated:**
The benchmark evaluates verse-by-verse textual explanations and critical interpretations of Arabic poetry. These artifacts span multiple historical eras and genres, requiring the model to produce analytical prose that decodes complex poetic structures, classical vocabulary, and rhetorical devices.

**Creative Capabilities Assessed:**
Assessment focuses on interpretive depth and cultural-linguistic decoding. Models are measured on their ability to navigate metaphorical density, maintain faithfulness to the source text, and demonstrate grammatical fluency. The evaluation specifically targets the model's capacity for nuanced analysis of layered meanings and historical context rather than mere surface-level translation.

**Evaluation Context:**
This text-based evaluation utilizes a hybrid scoring framework to assess LLM performance. It combines traditional lexical metrics (BLEU, chrF++) and semantic alignment (BERTScore, Textual Entailment) with high-level qualitative oversight. This includes ""LLM-as-a-judge"" (GPT-4o) for consistency and expert human ratings to specifically grade the interpretive richness of the generated explanations.

**Task Characteristics:**
The core task is constrained, domain-specific explanation generation. It differentiates itself from open-ended creative writing by requiring the model to bridge the gap between creative comprehension and critical exposition. It functions as a cross-era linguistic challenge, testing the model's ability to synthesize ancient poetic traditions into modern analytical frameworks.",3410fd0fc85cb04be40e2dacc7f36668261f8620:Fann or Flop,Fann or Flop,https://huggingface.co/datasets/mbzuai-oryx/FannOrFlop,1.0,API-Only,1.0,True,True,True,huggingface,openai; anthropic; google,text
33161a5a9b5dcb635b5a97475e6a6209a69ada7d,The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,2024.0,2024.0,,Scientific Discovery,Programming & Algorithmic Creativity; Functional & Professional Writing,high,"The paper explicitly focuses on the automated generation of research hypotheses and the full scientific research lifecycle, while also requiring the generation of functional experimental code and professional-grade scientific manuscripts.","**Creative Artifacts Evaluated:**
The system evaluates complete, LaTeX-formatted scientific manuscripts. These artifacts encompass novel research hypotheses, executable Python-based experimental code, bibliographies, and generated data visualizations such as performance plots and loss curves.

**Creative Capabilities Assessed:**
Assessment focuses on research novelty and originality, technical correctness of implemented algorithms, and the scientific utility of the findings. It measures conceptual depth through the interpretation of experimental data and the agent's ability to synthesize complex machine learning theories into coherent, peer-review-quality arguments. Both divergent thinking (during ideation) and convergent synthesis (during paper writing) are tested.

**Evaluation Context:**
The framework utilizes a fully automated, closed-loop pipeline where LLMs (e.g., GPT-4o, Claude 3.5 Sonnet) serve as both the generative researcher and the automated reviewer. Evaluation is text-and-code-centric, benchmarking the AI’s output against human conference decisions and NeurIPS-style scoring metrics to validate the quality of the discovery process.

**Task Characteristics:**
Tasks involve fully autonomous, open-ended scientific discovery across specialized machine learning domains like diffusion models and grokking. This includes iterative ideation, constrained experimentation within a fixed compute budget, and manuscript refinement. The process requires autonomous problem-solving and the ability to execute a complete research lifecycle without human intervention.",33161a5a9b5dcb635b5a97475e6a6209a69ada7d:The AI Scientist,The AI Scientist,https://github.com/SakanaAI/AI-Scientist,2.0,GPU/Local,3.0,False,False,True,github,openai; anthropic; google,text
33313b413695dcd396c6f558a3153cc6a572d5c1,ChatMusician: Understanding and Generating Music Intrinsically with LLM,ChatMusician: Understanding and Generating Music Intrinsically with LLM,2024.0,2024.0,,Music & Auditory Arts,,high,"The paper focuses on the generation and understanding of music using symbolic ABC notation, specifically evaluating harmonic logic, music theory reasoning, and rhythmic synchronization.","**Creative Artifacts Evaluated:**
The primary artifacts are musical compositions represented in ABC notation, a text-based symbolic format. These include monophonic and polyphonic melodies, structured musical phrases containing specific repeat signs, and text-based responses to college-level music theory and harmonic reasoning problems.

**Creative Capabilities Assessed:**
The research evaluates technical correctness (syntactic validity of ABC notation), structural coherence (phrase-level repetition and formal organization), and aesthetic musicality (human-perceived quality). It also measures domain-specific conceptual depth through music reasoning (harmony and inference) and controllability, which is the model’s ability to adhere to specific stylistic or melodic constraints during generation.

**Evaluation Context:**
The evaluation focuses on text-only Large Language Models (LLMs) performing music tasks without specialized audio encoders. It utilizes a hybrid scoring framework: automatic metrics for syntax parsing and structural repetition, human pairwise preference for subjective musicality, and zero-shot/few-shot accuracy for discriminative music theory benchmarks.

**Task Characteristics:**
Tasks encompass the full creative pipeline, including discriminative understanding (music theory exams), constrained generation (conditional synthesis based on motifs or styles), and open-ended creation. This approach treats music as a linguistic system, testing cross-domain transfer between general language abilities and symbolic musical logic.",33313b413695dcd396c6f558a3153cc6a572d5c1:MusicTheoryBench,MusicTheoryBench,https://github.com/hf-lin/ChatMusician,2.0,GPU/Local,2.0,False,True,True,huggingface,openai; huggingface,text
2fd0be13706830299fd92d78795afde3b53cec35,GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models,GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models,2025.0,2025.0,,Mathematical Reasoning,Lateral Thinking & Creative Problem Solving; Functional & Professional Writing,high,"The paper's core focus is on formal mathematical proofs and theoretical frameworks, while also explicitly evaluating non-obvious problem-solving insights and pedagogical communication.","**Creative Artifacts Evaluated:**
The benchmark evaluates structured mathematical outputs including formal proofs, original problem formulations, novel definitions, and unifying theoretical frameworks. It also assesses pedagogical explanations tailored to specific audiences and aesthetic rankings of mathematical solutions.

**Creative Capabilities Assessed:**
Assessment focuses on mathematical creativity through original problem-posing, lateral thinking (non-obvious insights), and the invention of alternative solving methods. It measures aesthetic judgment—specifically the recognition of ""elegance,"" symmetry, and conceptual insight—alongside abstract reasoning and the ability to transfer strategies across disparate mathematical domains. Technical correctness, logical rigor, and communicative clarity are foundational requirements.

**Evaluation Context:**
This is a text-based LLM benchmark utilizing expert human scoring. Evaluation employs a dual approach: binary accuracy (pass/fail) for objective problem-solving and nuanced, rubric-based scoring (0–3 scales) for subjective dimensions such as novelty, clarity, and ""mathematical beauty.""

**Task Characteristics:**
Tasks span a spectrum from constrained symbolic manipulation and computational fluency to open-ended ideation and self-directed learning from advanced research materials. The framework uniquely combines evaluative judgment (checking argument validity) with generative tasks requiring non-obvious analogies and the synthesis of seemingly unrelated theorems into cohesive frameworks.",2fd0be13706830299fd92d78795afde3b53cec35:GAUSS (General Assessment of Underlying Structured Skills in Mathematics),GAUSS (General Assessment of Underlying Structured Skills in Mathematics),https://gaussmath.ai,1.0,API-Only,26.0,True,True,True,huggingface,openai; google; huggingface,text
35a4557e99dd90821a18fe9076eabe1cb9c4b168,Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,2024.0,2024.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on benchmarking the generation and evaluation of humorous cartoon captions, specifically measuring irony, wordplay, and divergent thinking.","**Creative Artifacts Evaluated:**
The benchmark focuses on short-form humorous text captions designed for single-panel cartoons. These artifacts are intrinsically multimodal, requiring a semantic synthesis of linguistic wit and visual imagery to produce a comedic effect.

**Creative Capabilities Assessed:**
The tasks measure the ability to generate humor characterized by irony, incongruity, and wordplay. It assesses divergent thinking through caption diversity metrics (EAD and SBERT) and ""peak"" creativity via ""Best Pick"" win rates. Furthermore, it evaluates the capability for evaluative judgment—the discriminative ability to rank the quality of humorous content relative to human-perceived ground truth.

**Evaluation Context:**
This framework operates in a multimodal (image-to-text) context using a massive-scale dataset. Evaluation is comparative and ""mixed,"" utilizing human crowd-workers, expert judges, and LLMs (GPT-4) to determine win rates. It benchmarks model performance against established historical data from the New Yorker Caption Contest.

**Task Characteristics:**
The domain involves open-ended generation constrained by specific visual narratives. Tasks range from ideation (generating ten distinct captions per image) to evaluative ranking (judging which caption is funnier). This requires the model to function as both a creator and a critic within a subjective, culturally nuanced creative domain.",35a4557e99dd90821a18fe9076eabe1cb9c4b168:HumorousAI Benchmark,HumorousAI Benchmark,https://huggingface.co/datasets/yguooo/newyorker_caption_ranking,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic,multimodal
38179848e2d6a3ad373b1793848816111428ac36,OpenAGI: When LLM Meets Domain Experts,OpenAGI: When LLM Meets Domain Experts,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Visual Arts & Stylized Imagery; Music & Auditory Arts; Poetry & Verse,high,"The paper's core focus is on multi-step planning and tool orchestration to solve complex, non-linear tasks, while the outputs generated span multiple creative domains including visual arts, music, and poetry.","**Creative Artifacts Evaluated:**
The framework produces diverse multi-modal artifacts including synthesized images (text-to-image), edited visual content (image-to-image), structured text, poems, paintings, and musical compositions. These artifacts are typically the culmination of complex, multi-stage pipelines rather than single-prompt generations, representing a synthesis of outputs from various specialized domain models.

**Creative Capabilities Assessed:**
The primary focus is on multi-step planning and tool orchestration. The benchmark evaluates an LLM’s ability to decompose high-level creative prompts into logical sequences, demonstrating convergent thinking for task completion and divergent thinking for open-ended synthesis. It measures technical correctness, cross-modal alignment, and the imaginative depth required to map abstract concepts to executable workflows.

**Evaluation Context:**
OpenAGI operates in a multimodal ecosystem where LLMs (e.g., GPT-4, LLaMA) serve as controllers for specialized domain-expert models. Evaluation utilizes a hybrid approach: automatic metrics (CLIP Score for text-image alignment, BERT Score for linguistic quality, and ViT Score for visual similarity) are used for standardized benchmarks, while qualitative human assessment is employed for open-ended, creative tasks.

**Task Characteristics:**
Tasks are categorized into constrained, multi-step problem solving (185 benchmark tasks) and open-ended, imaginative generation. A distinguishing feature is the ""Reinforcement Learning from Task Feedback"" (RLTF) mechanism, which allows the model to refine its planning strategies for complex, non-linear creative workflows and cross-domain transfers.",38179848e2d6a3ad373b1793848816111428ac36:OpenAGI,OpenAGI,https://github.com/agiresearch/OpenAGI,2.0,GPU/Local,2.0,False,False,True,github,openai; huggingface,multimodal
38d45590794eda4d6d3bb1f15c293dd6911fac50,Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents,Hypothesis Generation for Materials Discovery and Design Using Goal-Driven and Constraint-Guided LLM Agents,2025.0,2025.0,,Scientific Discovery,Engineering & Technical Design; Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on generating novel scientific hypotheses for materials discovery, emphasizing technical feasibility, originality, and adherence to physical laws within a STEM context.","**Creative Artifacts Evaluated:**
The benchmark evaluates structured scientific hypotheses for materials discovery and design. These artifacts are textual proposals that describe novel material compositions, structural configurations, and synthesis mechanisms designed to meet specific functional objectives.

**Creative Capabilities Assessed:**
The assessment focuses on the balance between divergent and convergent thinking in a technical context. It measures innovation and novelty (originality) alongside scientific plausibility, feasibility, and testability (technical correctness). Additionally, it evaluates the model’s ability to ensure alignment with complex constraints and the potential impact of the proposed material design.

**Evaluation Context:**
This is a text-based evaluation of high-reasoning LLMs. It employs a mixed-methods scoring approach where both human domain experts and a frontier model (OpenAI-o1-preview) rate the hypotheses. The evaluation uses two primary frameworks: ""Closeness,"" which measures conceptual overlap with existing research, and ""Quality,"" which uses a 1–5 scale to judge the scientific merit and creative depth of the output.

**Task Characteristics:**
The task is a goal-driven, constrained generation exercise. It requires the model to transform specific research objectives and technical limitations into viable scientific strategies. Unlike general creative writing, this task necessitates domain-specific ideation where the ""creativity"" is strictly bounded by physical laws and manufacturing scalability, representing a specialized form of scientific problem-solving.",38d45590794eda4d6d3bb1f15c293dd6911fac50:MATDESIGN,MATDESIGN,https://github.com/shri071/Hypothesis-Generation-for-Materials-Discovery-and-Design-Using-Goal-Driven-and-Constraint-Guided-LLM,1.0,API-Only,1.0,True,False,True,github,openai; anthropic; google,text
37841d9036313b43d2a4069bc3f1493e9dc598da,Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews,Mind the Blind Spots: A Focus-Level Evaluation Framework for LLM Reviews,2025.0,2025.0,,Scientific Discovery,Functional & Professional Writing,high,"The paper evaluates LLMs' ability to perform scientific peer reviews, focusing on critical judgment of novelty and impact within STEM research, while also addressing the professional constraints of academic writing.","**Creative Artifacts Evaluated:**
The primary artifacts are **scientific peer reviews**, which consist of structured textual critiques evaluating research papers across specific dimensions such as **novelty, impact, methodology, and clarity**.

**Creative Capabilities Assessed:**
The framework assesses the LLM’s capacity for **critical judgment** and **conceptual depth**, specifically its ability to identify **originality (novelty)** and **usefulness (impact)** within scientific work. It measures **evaluative alignment**—how closely an LLM’s focus matches human experts—and **textual fluency** in a formal, academic context.

**Evaluation Context:**
This is a **text-based** evaluation of LLMs acting as automated reviewers. It employs a hybrid scoring model: **automatic text similarity** (ROUGE-L, BERTScore, BLEU-4) and **LLM-as-a-judge** focus analysis. The latter uses an LLM annotator to extract fine-grained facets, calculating **Kullback-Leibler Divergence** and **F1 scores** to quantify focus-level ""blind spots"" relative to human benchmarks.

**Task Characteristics:**
The task centers on **evaluation/judgment** and **constrained generation**. It requires the model to synthesize complex technical information and produce a critique that prioritizes specific evaluative facets, moving beyond simple summarization to provide structured, domain-specific feedback.",37841d9036313b43d2a4069bc3f1493e9dc598da:Focus-Level Evaluation Framework for LLM Reviews,Focus-Level Evaluation Framework for LLM Reviews,https://figshare.com/s/d5adf26c802527dd0f62,1.0,API-Only,1.0,True,False,True,url,openai,text
3cc4ad973029bdc4af58dc5038e92751b97073e0,PopBlends: Strategies for Conceptual Blending with Large Language Models,PopBlends: Strategies for Conceptual Blending with Large Language Models,2021.0,2021.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric; Functional & Professional Writing,high,The paper focuses on 'bisociation' and divergent thinking to generate novel ideation outputs through conceptual mapping and analogical bridges between disparate domains.,"**Creative Artifacts Evaluated:**
The system evaluates conceptual blends, specifically ""connecting concepts"" that serve as semantic bridges between pop culture entities and consumer products. Additional artifacts include entity attributes (activities and associations), textual scene descriptions, and final ideation outputs generated during user sessions.

**Creative Capabilities Assessed:**
Assessment focuses on cross-domain relevance, conceptual depth, and divergent thinking (measured by ideation volume). The framework evaluates the system’s ability to perform conceptual mapping and its utility in facilitating human creativity, specifically measuring the reduction of cognitive load (mental demand and effort) and the perceived quality of the resulting connections.

**Evaluation Context:**
The context involves text-based generation using GPT-3 and knowledge retrieval from Wikipedia. Evaluation is heavily human-centric, utilizing binary relevance ratings for automated outputs, Likert-scale scoring for attribute quality, and a within-subjects user study. The study compares the LLM-based system against traditional internet search baselines using NASA-TLX metrics and qualitative interviews.

**Task Characteristics:**
Tasks center on constrained generation and cross-domain transfer, requiring the model to synthesize disparate domains into coherent links. This involves attribute extraction, scene generation, and open-ended ideation, where the system acts as a collaborative partner to assist users in the ""bisociation"" process of creative discovery.",3cc4ad973029bdc4af58dc5038e92751b97073e0:PopBlends Evaluation Framework,PopBlends Evaluation Framework,https://arxiv.org/abs/2111.04920,3.0,Special,4.0,False,False,True,unknown,openai,multimodal
3e3f3617c9ab9ce815b2be871c192f13bab5cf0f,Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs,Do LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs,2024.0,2024.0,,Functional & Professional Writing,Narrative & Story Writing; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on journalistic angles and professional standards for news coverage, which are explicitly listed under Functional & Professional Writing, while also evaluating divergent ideation and narrative hooks.","**Creative Artifacts Evaluated:**
Narrative story angles (specific thematic directions or ""hooks"") and informational source suggestions (e.g., independent experts, affected stakeholders) derived from corporate or institutional press releases. The benchmark also evaluates the classification of contextualized news articles.

**Creative Capabilities Assessed:**
Measures divergent thinking through the generation of novel story perspectives and convergent thinking via alignment with professional journalistic standards. It specifically evaluates originality and usefulness (creativity) through expert human ratings, alongside the ability to identify ""contextualized"" coverage—content that adds external information and critical depth beyond the provided source text.

**Evaluation Context:**
A text-based, large-scale dataset comparing LLM outputs against a ground truth of professional journalism. Evaluation is hybrid, utilizing GPT-4 as a proxy for alignment (Precision/Recall/F1) and professional journalists for subjective creativity scoring on a 1–5 Likert scale.

**Task Characteristics:**
Combines discriminative classification (detecting high-effort journalism) with open-ended ideation (angle and source generation). The benchmark uniquely focuses on the ""planning"" phase of the creative writing process—strategic decision-making and information gathering—rather than final draft generation or simple summarization.",3e3f3617c9ab9ce815b2be871c192f13bab5cf0f:PressRelease Creative Planning Benchmark,PressRelease Creative Planning Benchmark,https://github.com/alex2awesome/press-releases-emnlp,1.0,API-Only,3.0,True,False,True,github,openai,text
3c8e90d2c4a3f4f69689e0bb782ad03b175c8353,Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity,Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity,2025.0,2025.0,,Narrative & Story Writing,Figurative Language & Rhetoric,high,"The paper focuses on evaluating and measuring creativity within literary prose and stylistic imitations, specifically addressing narrative coherence, linguistic novelty, and aesthetic quality in fictional writing.","**Creative Artifacts Evaluated:**
The research focuses on creative literary prose, specifically stylistic imitations of canonical authors and general creative writing passages. These artifacts are evaluated at the level of individual linguistic expressions as well as overall passage-level quality.

**Creative Capabilities Assessed:**
The paper assesses linguistic novelty (originality and surprise) and pragmatic coherence (sensicality and contextual flow). It specifically measures the ability of models to distinguish between ""productive"" novelty and ""non-pragmatic"" errors that disrupt narrative flow, ultimately evaluating how these distinct factors contribute to human-perceived aesthetic quality and writerly preference.

**Evaluation Context:**
This is a text-based meta-evaluation of creativity metrics. It utilizes an ""LLM-as-a-judge"" framework (GPT-5) to perform ""close reading"" tasks. Results are validated against a new dataset of expert human annotations, expert writer preferences (Style Mimic), and crowd-sourced quality rankings (LMArena), comparing these against traditional n-gram novelty baselines (CREATIVITY INDEX).

**Task Characteristics:**
The work centers on evaluative and discriminative tasks rather than open-ended generation. Key tasks include the identification and extraction of specific novel or nonsensical expressions within a text and the use of these features to predict human preference through hierarchical logistic regression. It shifts the focus of creativity assessment from statistical rarity to semantic and pragmatic judgment.",3c8e90d2c4a3f4f69689e0bb782ad03b175c8353:Close Reading Annotation Dataset,Close Reading Annotation Dataset,https://github.com/asaakyan/ngram-creativity,2.0,GPU/Local,2.0,False,False,True,github,openai,text
40fdf6a797754d0b09999ed44fa839c485d0f4ab,Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM,Creation-MMBench: Assessing Context-Aware Creative Intelligence in MLLM,2025.0,2025.0,,Functional & Professional Writing,Narrative & Story Writing; Poetry & Verse; Visual Arts & Stylized Imagery,high,"The benchmark evaluates a broad spectrum of creative text generation from visual prompts, with a significant portion of its tasks (marketing, professional reports, social media captions) falling under functional and professional writing, while also covering literary narratives, poetry, and visual design descriptions.","**Creative Artifacts Evaluated:**
The benchmark evaluates diverse text-based creative outputs generated from visual prompts across 51 fine-grained tasks. Specific artifacts include literary narratives, poetic compositions, marketing slogans, advertising copy, social media captions, visual design descriptions, and professional technical reports.

**Creative Capabilities Assessed:**
The primary focus is on ""context-aware creative intelligence."" This includes **Visual Factuality** (the ability to accurately incorporate visual elements into creative prose) and **Creative Quality** (measured via a reward score reflecting originality, stylistic flair, and engagement). The benchmark specifically assesses how visual instruction tuning impacts a model’s ability to synthesize perception with imaginative reasoning.

**Evaluation Context:**
Evaluation is performed in a multimodal-to-text framework, comparing MLLMs against text-only baselines. Scoring is conducted by a GPT-4o judge using a 1-10 Visual Factuality Score (VFS) and a -100 to +100 pairwise reward scale. Results are validated through human preference testing on a subset of the 765 test cases.

**Task Characteristics:**
Tasks consist of open-ended, image-constrained generation categorized into five domains: Literary Writing, Marketing & Promotion, Visual Design & Art, Daily Life & Social Media, and Professional & Technical Writing. These tasks require the model to move beyond simple captioning toward complex, context-sensitive ideation and synthesis.",40fdf6a797754d0b09999ed44fa839c485d0f4ab:Creation-MMBench,Creation-MMBench,https://github.com/open-compass/Creation-MMBench,2.0,GPU/Local,1.0,False,True,True,huggingface,openai,multimodal
4055e37ec2f1dd6c883001d12181a7131010882d,“A good pun is its own reword”: Can Large Language Models Understand Puns?,“A good pun is its own reword”: Can Large Language Models Understand Puns?,2024.0,2024.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper evaluates LLMs on puns and wordplay, which are core components of humor, while also requiring semantic disambiguation and lateral thinking to bridge unrelated word senses.","**Creative Artifacts Evaluated:**
The artifacts consist of short-form linguistic wordplay, specifically homographic and heterographic puns. These include punny sentences, binary classifications of humorous versus non-humorous text, and natural language explanations that decode the semantic ambiguity within a punchline.

**Creative Capabilities Assessed:**
The benchmark measures semantic disambiguation (identifying dual meanings), linguistic originality, and the ability to generate humor (funniness). It assesses both divergent thinking through constrained generation—requiring the model to bridge two unrelated senses—and convergent reasoning through the structural explanation of wordplay logic and pun-pair identification.

**Evaluation Context:**
This is a text-only evaluation of LLMs using a hybrid scoring model. It combines automatic statistical metrics (TPR/TNR, Cohen’s Kappa) with human qualitative ratings for funniness and success, alongside LLM-as-judge (GPT-4) pairwise comparisons. A unique feature is the use of dual-biased prompts to test the robustness of the model’s recognition capabilities against leading suggestions.

**Task Characteristics:**
The tasks span the full spectrum of creative processing: discriminative (pun recognition), interpretive (explanation of ambiguity), and constrained generation (creating puns from specific word-sense pairs). It focuses on the intersection of linguistic structure, semantic surprise, and the ""re-wording"" of intent.",4055e37ec2f1dd6c883001d12181a7131010882d:Pun Understanding Evaluation,Pun Understanding Evaluation,https://github.com/Zhijun-Xu/PunEval,1.0,API-Only,3.0,True,False,True,github,openai; anthropic; google,text
437cbaee4eaee0bf84abbe11750b86b091b9b756,MacGyver: Are Large Language Models Creative Problem Solvers?,MacGyver: Are Large Language Models Creative Problem Solvers?,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Engineering & Technical Design,high,"The paper explicitly focuses on 'out-of-the-box' reasoning and overcoming functional fixedness to solve physical problems, which aligns perfectly with the Lateral Thinking domain, while its emphasis on physical feasibility and mechanical utility connects to Engineering & Technical Design.","**Creative Artifacts Evaluated:**
Textual step-by-step solutions for complex physical problem-solving scenarios. These artifacts describe the unconventional repurposing of everyday objects (e.g., using a credit card as a scraper or a belt as a pulley) to overcome specific environmental constraints and resource scarcity.

**Creative Capabilities Assessed:**
The benchmark measures divergent thinking through the ability to override functional fixedness (identifying novel affordances for common items) and convergent thinking via feasibility and efficiency analysis. It specifically assesses physical reasoning, resourcefulness, and the capacity to correctly identify the solvability status of a problem.

**Evaluation Context:**
A text-based modality comparing state-of-the-art LLMs (GPT-4, PaLM2, Claude2, Llama2) against human performance. Evaluation is strictly human-led, utilizing fine-grained categorical scoring (e.g., ""feasible and efficient"" vs. ""infeasible"") to judge the practical viability and logic of generated plans.

**Task Characteristics:**
Constrained problem-solving requiring ""out-of-the-box"" reasoning. Tasks involve open-ended generation under resource limitations, iterative refinement through reflection, and the identification of impossible scenarios. This differentiates the work by focusing on the intersection of physical world logic and creative utility rather than purely linguistic or artistic creativity.",437cbaee4eaee0bf84abbe11750b86b091b9b756:MACGYVER,MACGYVER,https://github.com/allenai/MacGyver,3.0,Special,3.0,False,False,True,github,openai; anthropic; google,text
3e7f3f16347a0ece2cda872047ef886dca4c3e9d,"Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2","Out of Many, One: Designing and Scaffolding Proteins at the Scale of the Structural Universe with Genie 2",2024.0,2024.0,,Scientific Discovery,Engineering & Technical Design,high,"The paper describes a computational biology model for generating novel protein folds and functional scaffolds, which falls under STEM-based scientific research and technical protein engineering.","**Creative Artifacts Evaluated:**
The model generates three-dimensional protein backbone structures (50–500 residues), including entirely novel protein folds and functional scaffolds. Specific artifacts include single-motif scaffolds and complex multi-motif structures, such as a triple-epitope immunogen designed for respiratory syncytial virus (RSV) vaccine research.

**Creative Capabilities Assessed:**
Assessment focuses on structural originality (novelty compared to the Protein Data Bank and AlphaFold Database) and diversity (the breadth of the generated structural ""universe""). It also measures technical correctness through ""designability""—the ability of a generated structure to be realized by a sequence—and constrained creativity, specifically the model's capacity to integrate functional motifs into a stable, novel architecture without violating biological constraints.

**Evaluation Context:**
This research operates within the domain of computational structural biology. Evaluation is entirely automatic, utilizing a ""self-consistency"" pipeline where auxiliary models (AlphaFold2, ProteinMPNN) verify if generated sequences fold into the intended structures. Success is quantified via metrics like scRMSD, pLDDT, and TM-score clustering.

**Task Characteristics:**
Tasks range from open-ended, unconditional generation to highly constrained structural problem-solving. The multi-motif scaffolding tasks are particularly complex, requiring the model to creatively ""fill in"" the structural gaps between multiple functional segments whose relative orientations are not pre-defined.",3e7f3f16347a0ece2cda872047ef886dca4c3e9d:Unconditional Protein Generation Evaluation,Unconditional Protein Generation Evaluation,https://github.com/aqlaboratory/genie2,2.0,GPU/Local,1.0,False,False,True,github,none,text
40de5fd0c5b9d1fdc890b4b100845b716b676759,When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation,When to use Graphs in RAG: A Comprehensive Analysis for Graph Retrieval-Augmented Generation,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing,high,"The paper focuses on 'grounded divergent thinking' and the ability to bridge information gaps to solve 'what-if' problems, while using textual narratives and hypothetical scenarios as the primary medium for evaluation.","**Creative Artifacts Evaluated:**
The benchmark evaluates textual narratives, specifically hypothetical scenarios, novel situational projections, and speculative explanations. These artifacts are generated by extrapolating from retrieved structured data to create scenarios not explicitly contained within the source documents.

**Creative Capabilities Assessed:**
Assessment focuses on the model's ability to perform ""inference beyond retrieved content,"" emphasizing grounded divergent thinking. Key metrics include Faithfulness (FS) and Evidence Coverage (Cov), which measure the balance between imaginative extrapolation and factual integrity. It evaluates conceptual depth and the ability to synthesize fragmented knowledge into coherent, novel contexts.

**Evaluation Context:**
This is a text-centric LLM evaluation framework. It utilizes a hybrid scoring approach, combining automated lexical metrics (ROUGE-L) and accuracy (ACC) with LLM-as-a-judge for qualitative dimensions. The context is specifically designed to differentiate the performance of graph-based retrieval against traditional RAG in complex, multi-hop reasoning environments.

**Task Characteristics:**
Tasks involve constrained generation and open-ended ideation. The ""Creative Generation"" task is characterized by hypothetical reasoning and cross-domain transfer, requiring the model to bridge information gaps to solve ""what-if"" problems. This distinguishes the benchmark from standard summarization by demanding the construction of original scenarios based on retrieved evidence.",40de5fd0c5b9d1fdc890b4b100845b716b676759:GraphRAG-Bench,GraphRAG-Bench,https://github.com/GraphRAG-Bench/GraphRAG-Benchmark,1.0,API-Only,4.0,True,True,True,huggingface,openai,text
4343df06633bb912116d12a38387a9bc83cb936f,HumorDB: Can AI understand graphical humor?,HumorDB: Can AI understand graphical humor?,2024.0,2024.0,,Humor & Satire,Visual Arts & Stylized Imagery,high,"The paper focuses on benchmarking AI's ability to understand, quantify, and explain humor within graphical images, directly addressing the resolution of comedic incongruity and visual wit.","**Creative Artifacts Evaluated:**
Graphical humor, specifically curated, minimally contrastive image pairs where one version contains a humorous element and the other serves as a non-funny control.

**Creative Capabilities Assessed:**
Visual humor comprehension and discriminative judgment. This includes the ability to identify humorous intent, quantify funniness intensity, localize the specific visual source of humor, and generate factually accurate textual explanations of comedic mechanisms.

**Evaluation Context:**
A multimodal framework testing Vision-Language Models (VLMs) and Vision Transformers (ViTs). It employs a hybrid scoring approach: automatic metrics (Accuracy, RMSE, Recall) for classification and localization, alongside human-centric ratings (1–5 scale) for evaluating the semantic quality and factual grounding of generated explanations.

**Task Characteristics:**
The benchmark focuses on the *understanding* and *evaluation* of creative artifacts rather than their generation. Tasks range from constrained judgment (binary/pairwise choice and ordinal rating) to open-ended textual explanation. It uniquely incorporates mechanistic interpretability tasks, such as attention mapping and logit attribution, to determine if models focus on the same salient visual features as humans when perceiving humor.",4343df06633bb912116d12a38387a9bc83cb936f:HumorDB,HumorDB,https://huggingface.co/datasets/kreimanlab/HumorDB,2.0,GPU/Local,6.0,False,True,True,huggingface,openai; google; huggingface,multimodal
38558608dedc495dd748ea3381b08b0d2ffccf56,Seeing the Unseen: Visual Metaphor Captioning for Videos,Seeing the Unseen: Visual Metaphor Captioning for Videos,2024.0,2024.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper's core focus is on generating visual metaphors and figurative linguistic interpretations, while also measuring divergent thinking and the semantic leap between literal and abstract concepts.","**Creative Artifacts Evaluated:**
The research evaluates metaphorical text captions generated for videos and images. Unlike literal descriptions, these artifacts are figurative linguistic interpretations that map visual sequences to abstract or semantically distant concepts (e.g., describing a flickering light as a ""dying heartbeat"").

**Creative Capabilities Assessed:**
The benchmarks measure **originality** and **divergent thinking** through the Average Concept Distance (ACD) metric, which quantifies the semantic leap between literal visual content and metaphorical output. Additional assessments include **fluency**, **conceptual depth**, and **primary concept consistency**, ensuring the creative output remains grounded in the source video’s core subject while exhibiting high aesthetic and figurative quality.

**Evaluation Context:**
This is a multimodal (video-to-text) generation context utilizing Vision-Language Models (VLMs). Evaluation employs a hybrid framework: standard linguistic metrics (BLEU, CIDEr), a specialized automatic creativity metric (ACD), and human protocols that specifically score ""creativity"" as a distinct dimension from ""fluency"" and ""consistency.""

**Task Characteristics:**
The work focuses on **open-ended creative generation** and **cross-modal mapping**. It involves **cross-domain transfer** (leveraging image-based metaphoric pretraining for video tasks) and **synthetic ideation** (using LLMs to transform literal captions into metaphorical ones). The tasks differentiate themselves by requiring the model to ""see the unseen""—moving from literal recognition to figurative interpretation.",38558608dedc495dd748ea3381b08b0d2ffccf56:Metaphoric Image Captioning Pretraining,Metaphoric Image Captioning Pretraining,https://github.com/abisekrk/video-metaphor-captioning,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
44c717ce0ec31ff828c06abca417fea83421d256,Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation,Reason-SVG: Hybrid Reward RL for Aha-Moments in Vector Graphics Generation,2025.0,2025.0,,Graphic Design & Visual Layout,Programming & Algorithmic Creativity; Visual Arts & Stylized Imagery,high,"The paper focuses on generating vector-based logos and icons through SVG code, emphasizing spatial composition and typography integration, which directly bridges graphic design, code generation, and visual aesthetics.","**Creative Artifacts Evaluated:**
The primary artifacts are Scalable Vector Graphics (SVG) code files. These represent structured, resolution-independent visual designs, specifically icons, logos, and illustrations that frequently integrate stylized typographic elements into geometric compositions (Design-with-Text).

**Creative Capabilities Assessed:**
Evaluation focuses on technical proficiency (SVG syntax validity), aesthetic quality (visual appeal), and semantic consistency (alignment with natural language prompts). Crucially, the benchmark assesses the model’s capacity for spatial reasoning and compositional synthesis—specifically the ability to legibly and artistically integrate text strings within a vector-based visual layout.

**Evaluation Context:**
This is a multimodal domain bridging natural language, symbolic code, and visual rendering. The evaluation is hybrid, utilizing automatic computer vision metrics (FID, CLIPScore, HPSv2), code-specific validation (SVG Validity), and human expert assessment via Likert scales. The context emphasizes ""Aha-moments,"" targeting the model's ability to achieve sudden breakthroughs in complex design reasoning during the RL process.

**Task Characteristics:**
The task involves constrained generation, where the model must satisfy strict XML-based syntactic rules while fulfilling open-ended creative prompts. It requires cross-domain transfer between linguistic concepts and structured geometric code, demanding both divergent ideation for visual metaphors and convergent execution for functional, renderable code.",44c717ce0ec31ff828c06abca417fea83421d256:SVGX-DwT-10k Evaluation Set (DEval),SVGX-DwT-10k Evaluation Set (DEval),https://huggingface.co/xingxm,2.0,GPU/Local,1.0,False,True,True,huggingface,none,multimodal
43c81cd19ffd3170be14407e10e0229f879a8647,Multimodal Knowledge Alignment with Reinforcement Learning,Multimodal Knowledge Alignment with Reinforcement Learning,2022.0,2022.0,,Functional & Professional Writing,Narrative & Story Writing; Dialogue Generation & Social Interaction,high,"The paper evaluates the generation of a wide range of goal-oriented text types including instructional guides, news captions, and blog posts, while also assessing narrative coherence and multi-turn conversational dialogue.","**Creative Artifacts Evaluated:**
Narrative stories, instructional guides, blog posts, social media snippets, news-style captions, descriptive audio captions, and conversational dialogue responses.

**Creative Capabilities Assessed:**
Stylistic versatility (adapting tone across five distinct personas), narrative coherence, and contextual reasoning. The research measures divergent thinking through the generation of multiple distinct outputs from a single visual prompt and assesses the model’s ability to maintain informativeness and fluency during cross-modal alignment. It specifically evaluates the model's capacity for ""creative alignment""—generating relevant text for modalities (like audio or images) without having seen paired examples during training.

**Evaluation Context:**
A multimodal framework spanning image-to-text and audio-to-text domains. Evaluation utilizes a hybrid approach: automatic linguistic metrics (CIDEr, BLEU, METEOR) for structural accuracy and human evaluation on a 5-point Likert scale to judge subjective qualities like visual relevance, informativeness, and stylistic appropriateness.

**Task Characteristics:**
The work emphasizes open-ended generation and stylistic refinement. It features zero-shot/unpaired captioning, which requires creative cross-modal inference without direct supervision, alongside constrained generation (news context) and evaluative ranking within multi-turn, image-grounded dialogues. This combination tests both the generative and discriminative aspects of multimodal creativity across diverse media types.",43c81cd19ffd3170be14407e10e0229f879a8647:ESP dataset,ESP dataset,https://github.com/JiwanChung/esper,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
4aa220dcdf9f76d743661f2e7ab923345450e366,One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor,One Does Not Simply Meme Alone: Evaluating Co-Creativity Between LLMs and Humans in the Generation of Humor,2025.0,2025.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving; Visual Arts & Stylized Imagery,high,"The paper focuses on the generation and evaluation of internet memes, specifically assessing humor, wit, and irony, while also measuring cognitive metrics like divergent thinking and ideation fluency.","**Creative Artifacts Evaluated:**
The study focuses on multimodal internet memes, specifically the synthesis of established visual templates with novel, contextually humorous text captions. These artifacts represent a unique intersection of visual semiotics and linguistic wit.

**Creative Capabilities Assessed:**
Evaluation centers on humor, perceived creativity, and shareability (potential for social viral impact). Beyond the final product, the benchmark assesses process-oriented capabilities including ideation fluency (the volume of distinct ideas generated), divergent thinking, and the impact of AI on the creator's subjective cognitive workload (measured via NASA-TLX).

**Evaluation Context:**
This multimodal research utilizes a between-subjects human-in-the-loop framework. It compares three distinct conditions: autonomous AI generation (GPT-4o), solo human creation, and human-AI co-creation. Scoring is strictly human-centric, relying on subjective Likert scales to capture the nuances of wit, irony, and cultural relevance that automated metrics typically fail to detect.

**Task Characteristics:**
The tasks involve open-ended, humor-based generation and collaborative refinement. The benchmark is characterized by its focus on ""co-creativity,"" moving beyond simple prompting to evaluate how LLMs assist in the iterative ideation and selection phases of a culturally dependent and highly subjective creative task.",4aa220dcdf9f76d743661f2e7ab923345450e366:Co-Creative Meme Generation and Evaluation,Co-Creative Meme Generation and Evaluation,,3.0,Special,2.0,False,False,True,unknown,openai,multimodal
4a794f49571b7ffc624c8f1e156cc6fa0d9142d8,LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing,LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing,2025.0,2025.0,,Narrative & Story Writing,,high,The paper introduces a benchmark specifically for evaluating short-form narrative fiction and storytelling quality based on human preferences.,"**Creative Artifacts Evaluated:**
Short-form narrative fiction and creative stories specifically sourced from online writing communities (Reddit).

**Creative Capabilities Assessed:**
Narrative quality discernment, aesthetic judgment, and alignment with human subjective preferences. The benchmark measures the ""critic"" aspect of creativity—the ability to identify superior creative prose, emotional resonance, and storytelling efficacy—rather than the act of generation itself.

**Evaluation Context:**
A text-based meta-evaluation framework comparing the performance of zero-shot Large Language Models (LLMs) and specialized Reward Models (Bradley-Terry and Generative architectures). It utilizes a large-scale ground truth derived from social media signals (Reddit upvotes) and validates these findings through controlled human crowd-worker studies to ensure the reliability of automated creative assessment.

**Task Characteristics:**
Discriminative evaluation and comparative ranking. The tasks focus on binary preference selection and the training of reward models to simulate human-like appreciation of literature. This shifts the focus from open-ended generation to the challenge of reliable, automated judgment in subjective domains, testing whether AI can replicate the nuanced preferences of a human audience.",4a794f49571b7ffc624c8f1e156cc6fa0d9142d8:LitBench,LitBench,https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,2.0,GPU/Local,3.0,False,True,True,huggingface,openai; anthropic,text
466dce99f8226de1cc9557bceb8069934114deaf,SEED-Story: Multimodal Long Story Generation with Large Language Model,SEED-Story: Multimodal Long Story Generation with Large Language Model,2024.0,2024.0,,Narrative & Story Writing,Visual Arts & Stylized Imagery; Performing Arts & Video Synthesis,high,"The paper focuses on generating long-form narrative text interleaved with consistent visual imagery and animations, requiring both storytelling expertise and multimodal synthesis of static and temporal visual content.","**Creative Artifacts Evaluated:**
The primary artifacts are multimodal, long-form stories featuring interleaved narrative text and high-resolution animated images. These are structured as sequential storylines where visual and textual elements must maintain character and stylistic continuity over extended, multi-frame sequences.

**Creative Capabilities Assessed:**
The benchmark measures narrative fluency, story engagement, and the aesthetic quality of generated images. A core focus is on stylistic consistency—the ability to maintain a uniform visual style across an entire sequence—and image-text coherence. It also evaluates the model’s capacity for long-range dependency management, impacting the logical progression and thematic depth of the creative output.

**Evaluation Context:**
This research operates in a multimodal generation context, leveraging Large Language Models (LLMs) integrated with visual encoders. Evaluation is multifaceted, employing automatic metrics (FID, CLIP) for technical fidelity, GPT-4V for preference-based narrative assessment, and human user studies to judge subjective dimensions like engagement, diversity, and stylistic harmony.

**Task Characteristics:**
Tasks involve open-ended, auto-regressive generation of interleaved content and story visualization. The process requires constrained generation, where the model must synthesize new visual content based on preceding narrative context and visual history, necessitating high levels of cross-modal synthesis and temporal consistency.",466dce99f8226de1cc9557bceb8069934114deaf:StoryStream,StoryStream,https://github.com/TencentARC/SEED-Story,2.0,GPU/Local,3.0,False,True,True,huggingface,openai; huggingface,multimodal
442e3bc6b9773ed90faea2dfbbbd441d2c8108d0,DiffDesign: Controllable diffusion with meta prior for efficient interior design generation,DiffDesign: Controllable diffusion with meta prior for efficient interior design generation,2024.0,2024.0,,"3D, Spatial & Architectural Design",Visual Arts & Stylized Imagery,high,"The paper focuses on generating interior design renderings and floor plans for architectural workflows, which aligns directly with the 3D, Spatial & Architectural Design domain's focus on interior renderings and spatial reasoning.","**Creative Artifacts Evaluated:**
The primary artifacts are photorealistic interior design renderings and floor plans across diverse functional spaces, including commercial environments (hotel lobbies, cafés, clothing stores) and residential areas (studies, living rooms), paired with detailed textual design descriptions.

**Creative Capabilities Assessed:**
Evaluation focuses on aesthetic design appeal, task relevance (text-to-image alignment), and practical usability for professional architectural workflows. Technical capabilities include zero-shot cross-modal retrieval accuracy and computational efficiency (training speed), alongside generative quality measured through Fréchet Inception Distance (FID) and Inception Scores (IS).

**Evaluation Context:**
This research utilizes a multimodal (text-to-image) framework centered on diffusion models and the specialized DesignHelper dataset. It employs a hybrid evaluation strategy: automatic metrics for image quality and retrieval, and expert-led human scoring where 20 interior design professionals assess perceptual quality, design appeal, and professional utility on a 5-point scale.

**Task Characteristics:**
Tasks encompass constrained generation from specific architectural prompts, zero-shot image-text retrieval, and qualitative ideation across distinct room types. The work emphasizes efficient model refinement through meta-learning priors and validates component-level contributions to the creative output through human-scored ablation studies and win-rate comparisons.",442e3bc6b9773ed90faea2dfbbbd441d2c8108d0:DesignHelper,DesignHelper,https://github.com/Awesome-AutoDesign/DiffDesign,2.0,GPU/Local,2.0,False,False,True,github,,multimodal
4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f,Hypothesis Search: Inductive Reasoning with Language Models,Hypothesis Search: Inductive Reasoning with Language Models,2023.0,2023.0,,Programming & Algorithmic Creativity,Scientific Discovery; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating executable Python programs to represent symbolic hypotheses for inductive reasoning tasks, combining algorithmic synthesis with scientific hypothesis generation and abstract pattern discovery.","**Creative Artifacts Evaluated:**
The artifacts consist of symbolic hypotheses and executable Python programs. These represent abstract transformation rules for 2D visual grids (ARC), 1D sequences, string manipulations (SyGuS), and numerical list mappings. Unlike open-ended creative writing, these artifacts are functional scripts designed to encode logic and pattern-matching rules.

**Creative Capabilities Assessed:**
The research assesses inductive reasoning and divergent thinking through the generation of multiple candidate explanations for a single set of observations. It measures technical correctness (accuracy) and the ability to perform abstract pattern discovery. The focus is on the model's capacity for conceptual depth—moving from specific input-output examples to generalizable algorithmic principles.

**Evaluation Context:**
This is a text-based, code-centric evaluation of Large Language Models (LLMs). It utilizes automatic scoring based on Top-1 accuracy or the ability of synthesized programs to satisfy all provided test cases. The context is highly structured and symbolic, prioritizing logical validity over aesthetic or emotional impact.

**Task Characteristics:**
Tasks involve constrained generation and problem-solving within the framework of ""hypothesis search."" This is an iterative process of ideation and refinement where the model must discover hidden logic from minimal data points. It represents a form of creative logical synthesis, requiring the model to bridge the gap between divergent rule generation and convergent verification.",,,,,,,,,,,,
4c6f256693479c20141abf3a0f7865773da69571,Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions,Enumerate-Conjecture-Prove: Formally Solving Answer-Construction Problems in Math Competitions,2025.0,2025.0,,Mathematical Reasoning,Programming & Algorithmic Creativity; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the formal construction of mathematical conjectures and proofs for high-level competition problems, requiring both logical rigor and the discovery of non-obvious patterns using the Lean 4 formal language.","**Creative Artifacts Evaluated:**
The primary artifacts are formal mathematical conjectures (closed-form answers) and complete formal proofs authored in the Lean 4 interactive theorem prover. These represent specific mathematical objects—such as sequences, functions, or sets—that must satisfy complex, competition-level constraints.

**Creative Capabilities Assessed:**
The benchmarks measure ""creative construction,"" requiring models to move beyond standard logical deduction to hypothesize novel mathematical structures. Key capabilities include technical correctness (verified via Lean automation), conceptual depth in identifying non-obvious patterns, and convergent thinking to synthesize a singular, provable solution from an open-ended problem space.

**Evaluation Context:**
The evaluation is text-based (formal code) and targets high-complexity, competition-level mathematics (e.g., Putnam, IMO). It utilizes rigorous automatic evaluation through the Lean kernel, measuring both answer-construction accuracy (provable equivalence to ground truth) and end-to-end proof verification (Pass@32).

**Task Characteristics:**
Tasks involve constrained generation and formal problem-solving within a ""conjecture-then-prove"" workflow. This necessitates a multi-stage process: enumerating potential candidates, formulating a formal conjecture, and refining a rigorous proof. This distinguishes the work from standard theorem-proving by emphasizing the initial ""creative leap"" required to identify the target object before the deductive proof begins.",4c6f256693479c20141abf3a0f7865773da69571:ConstructiveBench,ConstructiveBench,https://huggingface.co/datasets/JackSun200312/ConstructiveBench,3.0,Special,1.0,False,True,True,huggingface,openai; none,text
46cff4ce8deed2ef1b00d483ed9a69f8183e3538,CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints,CS4: Measuring the Creativity of Large Language Models Automatically by Controlling the Number of Story-Writing Constraints,2024.0,2024.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,The paper focuses on evaluating the generation of short stories while specifically measuring 'constrained creativity' and the model's ability to solve the complex problem of integrating numerous disparate requirements into a coherent narrative.,"**Creative Artifacts Evaluated:**
The benchmark evaluates **short stories** generated through the revision of ""base stories."" These narratives are defined by their adherence to a high density of synthesized, specific constraints (ranging from 7 to 39), designed to prevent the model from relying on memorized training data.

**Creative Capabilities Assessed:**
The primary focus is **constrained creativity**, specifically the ability to maintain **narrative coherence**, **grammatical correctness**, and **reader likability** under extreme pressure. It measures **divergent thinking** through output diversity (self-perplexity and dist-n) and **technical adherence** via constraint satisfaction ratios. Unique metrics like **Quality Under n Constraints (QUCn)** and **Relative Creativity Score (RCSm,n)** quantify how an LLM’s creative performance scales or degrades as task complexity increases.

**Evaluation Context:**
This is a **text-based** framework utilizing a hybrid of **LLM-based judges** (e.g., GPT-3.5-Turbo) for qualitative assessment and **automated linguistic metrics** for diversity. The context is defined by high-complexity, synthesized prompts that force the generation of novel content rather than stochastic parroting.

**Task Characteristics:**
The core task is **constrained story generation** via iterative refinement. It shifts from open-ended ideation to **highly-constrained problem-solving**, requiring the model to integrate numerous disparate requirements into a singular narrative. It also includes **evaluative judgment** tasks where LLMs perform pairwise comparisons and satisfaction checks.",46cff4ce8deed2ef1b00d483ed9a69f8183e3538:CS4 (Comparing the Skill of Creating Stories by Controlling the Synthesized Constraint Specificity),CS4 (Comparing the Skill of Creating Stories by Controlling the Synthesized Constraint Specificity),https://github.com/anirudhlakkaraju/cs4_benchmark,2.0,GPU/Local,5.0,False,False,True,github,openai,text
50e66c2b936b6487b857af18596f5ce3a8ec4f59,Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models,Getting Serious about Humor: Crafting Humor Datasets with Unfunny Large Language Models,2024.0,2024.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper explicitly focuses on the generation and manipulation of satirical news headlines and humorous social media content, evaluating the model's ability to handle wit, irony, and cultural subtext.","**Creative Artifacts Evaluated:**
The research focuses on satirical news headlines (modeled after *The Onion*) and code-mixed (English-Hindi) social media posts/tweets. These artifacts represent short-form, high-context textual humor requiring specific cultural and linguistic nuances.

**Creative Capabilities Assessed:**
The primary capability assessed is the manipulation of humor through ""unfunning""—the precise removal of wit and irony while preserving core semantic meaning—and its inverse, humor generation (injecting satire into serious text). The study measures linguistic fluency, semantic coherence, and the ability to navigate complex cross-lingual creative constraints in low-resource settings.

**Evaluation Context:**
This text-centric study evaluates LLMs (e.g., GPT-4, Mistral) using a hybrid scoring framework. It combines holdout accuracy from fine-tuned classifiers (RoBERTa, XLM-R), human subjective ratings for ""funniness,"" ""realness,"" and ""grammaticality,"" and lexical metrics such as edit distance and Type-Token Ratio (TTR) to measure the extent of creative transformation.

**Task Characteristics:**
Tasks involve constrained generation through iterative editing and bidirectional transformation (humorous-to-serious and vice-versa). The methodology includes an evaluation/filtering component where LLMs act as judges of their own creative output, highlighting the difficulty of maintaining coherence during the transformation of high-context, satirical content.",50e66c2b936b6487b857af18596f5ce3a8ec4f59:Unfun Corpus,Unfun Corpus,https://github.com/zacharyhorvitz/Getting-Serious-With-LLMs,2.0,GPU/Local,2.0,False,False,True,github,openai,text
548ef466e61871aced22fd4cceaf4cc8348a9567,RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines,RPGBENCH: Evaluating Large Language Models as Role-Playing Game Engines,2025.0,2025.0,,Dialogue Generation & Social Interaction,Narrative & Story Writing,high,"The paper focuses on evaluating LLMs as role-playing game engines, emphasizing persona-driven consistency, interactive dialogue, and the generation of structured world-building narratives.","**Creative Artifacts Evaluated:**
The benchmark evaluates structured text-based RPG world specifications—including character profiles, world states, and branching narrative paths—and dynamic gameplay narratives produced during interactive sessions. These artifacts represent a blend of static world-building and real-time, state-dependent storytelling.

**Creative Capabilities Assessed:**
Assessment focuses on the intersection of narrative flair and logical rigor. Key capabilities include technical correctness (adherence to game mechanics and variable updates), structural integrity (graph reachability and world validity), and role-playing consistency (maintaining factual and personality-driven coherence). It also measures ""interestingness"" and action quality within open-ended simulations.

**Evaluation Context:**
The context is a text-only modality where LLMs function as autonomous game engines. Evaluation is uniquely hybrid, employing automatic graph-based validity checking (via BFS), LLM-as-a-judge for qualitative traits like personality (TIPI framework), and human validation for narrative depth and engagement.

**Task Characteristics:**
Tasks involve a dual-phase process: constrained generation of valid, playable game structures and open-ended, state-aware simulation. This requires the model to transition from creative ideation (world-building) to complex system management (tracking variables and event conditions) while responding to unpredictable user-driven inputs, distinguishing it from static narrative generation.",548ef466e61871aced22fd4cceaf4cc8348a9567:RPGBENCH,RPGBENCH,https://github.com/boson-ai/rpgbench-public,1.0,API-Only,2.0,True,True,True,huggingface,openai,text
4d37c6d51dc01309a5e4fe59d90a9b648f909eb5,The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories,The GPT-WritingPrompts Dataset: A Comparative Analysis of Character Portrayal in Short Stories,2024.0,2024.0,,Narrative & Story Writing,,high,"The paper specifically analyzes character portrayal, emotional dimensions, and narrative bias within short stories and creative writing prompts.","**Creative Artifacts Evaluated:**
The primary artifacts are short stories (narrative fiction) paired with their corresponding creative prompts. The dataset includes human-authored texts sourced from the Reddit WritingPrompts community and machine-generated counterparts produced by GPT-3.5.

**Creative Capabilities Assessed:**
The research assesses character portrayal and narrative bias within storytelling. Specifically, it measures the emotional and social dimensions of protagonists, including valence, arousal, dominance, and social power. It also evaluates descriptive attributes such as physical appearance and intellect, focusing on how LLMs differentiate character traits compared to human benchmarks.

**Evaluation Context:**
This is a text-centric comparative study between LLMs and human writers. Evaluation is performed through automatic, lexicon-based, and embedding-based methods. It utilizes NRC-VAD for emotional dimensions and word2vec axis projection/cosine similarity to derive mean z-scored values for character attributes, providing a quantitative map of narrative archetypes.

**Task Characteristics:**
The work involves constrained generation (producing stories from specific, pre-defined prompts) and comparative analysis. It focuses on identifying systemic differences in characterization, highlighting how AI-driven creative writing may exhibit different stereotypical biases or ""flattened"" emotional arcs compared to human-authored narrative prose.",4d37c6d51dc01309a5e4fe59d90a9b648f909eb5:GPT-WritingPrompts Dataset,GPT-WritingPrompts Dataset,https://github.com/KristinHuangg/gpt-writing-prompts,2.0,GPU/Local,1.0,False,True,True,huggingface,openai,text
53a64d333bb5033bc1e9f7bcfd166249ef7b7b0c,Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models,Poetry in Pixels: Prompt Tuning for Poem Image Generation via Diffusion Models,2025.0,2025.0,,Visual Arts & Stylized Imagery,Poetry & Verse; Figurative Language & Rhetoric,high,"The paper focuses on the cross-domain synthesis of visual art from poetic text, specifically evaluating the translation of metaphorical and figurative language into aesthetic imagery.","**Creative Artifacts Evaluated:**
The research evaluates concise textual summaries of poems and generated images that visually interpret poetic themes, emotions, and plots. It specifically distinguishes between general poetry (via the PoemSum dataset) and the niche domain of children’s poetry (via the new MiniPo dataset).

**Creative Capabilities Assessed:**
The study assesses interpretive depth and the ability to translate metaphorical language into visual symbolism. Key capabilities include thematic alignment, emotional resonance, and conceptual depth. Evaluation measures aesthetic quality and alignment through human expert ratings, alongside technical metrics for linguistic fluency and semantic consistency between text and image.

**Evaluation Context:**
This multimodal study utilizes LLMs for text processing and diffusion models for image synthesis. The evaluation framework is hybrid, combining traditional NLP metrics (ROUGE, BLEU, METEOR) with vision-language contrastive losses (ITC, ITM) and expert human judgment on a 1–5 scale to bridge the gap between abstract verse and concrete imagery.

**Task Characteristics:**
Tasks focus on cross-domain transfer (text-to-image) and constrained generation (summarization). This requires the models to perform complex interpretation and visualization, moving from figurative, open-ended creative text to concrete visual ideation while maintaining the original work's thematic integrity.",53a64d333bb5033bc1e9f7bcfd166249ef7b7b0c:PoemSum,PoemSum,https://github.com/SofeeyaJ/Poetry-In-Pixels-Coling2025,2.0,GPU/Local,2.0,False,False,True,github,openai,multimodal
53d193a9fb82ce0c2b5d368071b405f63dd98c90,CCEdit: Creative and Controllable Video Editing via Diffusion Models,CCEdit: Creative and Controllable Video Editing via Diffusion Models,2023.0,2023.0,,Performing Arts & Video Synthesis,Visual Arts & Stylized Imagery,high,"The paper focuses on video-to-video generation and editing, emphasizing temporal consistency and motion dynamics while also evaluating artistic stylization and aesthetic quality.","**Creative Artifacts Evaluated:**
The benchmark evaluates edited video sequences derived from source footage, specifically focusing on stylized transformations (e.g., oil paintings, 3D animations), object substitutions (e.g., replacing a dog with a robotic tiger), and environmental modifications. These artifacts are categorized by a ""Fantasy Level,"" which distinguishes between realistic, grounded edits and highly imaginative, non-veridical creative scenes.

**Creative Capabilities Assessed:**
The primary capability assessed is the model’s ability to balance structural controllability with creative divergence. This includes aesthetic quality, temporal consistency (maintaining visual identity across frames), and semantic alignment with complex text prompts. The ""Fantasy Level"" specifically measures the model's capacity for imaginative synthesis—how effectively it can deviate from the source material’s literal content while maintaining its underlying motion and structure.

**Evaluation Context:**
This is a multimodal generative task utilizing diffusion models. Evaluation is conducted through a hybrid framework: human Mean Opinion Scores (MOS) assess subjective aesthetics and overall impression, while automated metrics like PickScore (human preference simulation), Text Alignment (Tex-Ali), and Temporal Consistency (Tem-Con) provide objective benchmarks for semantic fidelity and technical stability.

**Task Characteristics:**
The task is defined as constrained video-to-video generation, requiring high-fidelity refinement and cross-domain transfer. It involves open-ended creative interpretation of text prompts within the rigid structural bounds of a pre-existing video, testing the model's ability to perform complex scene re-composition and stylistic adaptation without losing temporal coherence.",53d193a9fb82ce0c2b5d368071b405f63dd98c90:BalanceCC,BalanceCC,https://github.com/RuoyuFeng/CCEdit,2.0,GPU/Local,1.0,False,True,True,huggingface,none,multimodal
464edfd902f652d3ab6a25dbb6d9fa47cc3246a9,MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies,MusicLDM: Enhancing Novelty in text-to-music Generation Using Beat-Synchronous mixup Strategies,2023.0,2023.0,,Music & Auditory Arts,Programming & Algorithmic Creativity,high,"The paper focuses on high-fidelity text-to-music generation and evaluates musicality and rhythmic coherence, while introducing novel algorithmic strategies to enhance generative novelty.","**Creative Artifacts Evaluated:**
High-fidelity audio music samples generated from natural language textual descriptions.

**Creative Capabilities Assessed:**
The evaluation prioritizes **novelty** and the mitigation of plagiarism risk (originality) alongside **textual relevance** (semantic alignment). It also measures **musicality**—specifically rhythmic, melodic, and textural coherence—and **aesthetic quality**, including audio clarity and the absence of sonic artifacts.

**Evaluation Context:**
This research operates in a multimodal text-to-audio domain using Latent Diffusion Models (LDM). It employs a hybrid evaluation strategy: standard automatic metrics (FD, IS, KL) for distributional quality, novel CLAP-based (Contrastive Language-Audio Pretraining) metrics for semantic similarity and nearest-neighbor plagiarism detection (SIMAA), and subjective human listening tests to capture nuanced musical attributes that automatic systems may overlook.

**Task Characteristics:**
The core task is open-ended, text-conditioned music generation. It distinguishes itself through constrained generation using beat-synchronous mixup strategies, which aim to enhance creative novelty by preventing the model from memorizing training data. The work represents a shift from pure quality optimization toward a balance between generative fluency and the production of distinct, non-derivative creative content.",464edfd902f652d3ab6a25dbb6d9fa47cc3246a9:CLAP-based Novelty and Relevance Metrics,CLAP-based Novelty and Relevance Metrics,https://github.com/RetroCirce/MusicLDM/,2.0,GPU/Local,2.0,False,False,True,unknown,,multimodal
54f57b46db8caa48fcd8519b3bf081c4e4c0c2d6,MuSe '24: The 5th Multimodal Sentiment Analysis Challenge and Workshop: Social Perception & Humor,MuSe '24: The 5th Multimodal Sentiment Analysis Challenge and Workshop: Social Perception & Humor,2024.0,2024.0,,Humor & Satire,Dialogue Generation & Social Interaction,high,"The paper explicitly focuses on the recognition of humor and the decoding of social attributes within spontaneous multimodal interactions, directly mapping to the Humor & Satire and Social Interaction domains.","**Creative Artifacts Evaluated:**
Spontaneous multimodal social interactions and comedic expressions captured via video, audio, and text transcriptions. Unlike scripted media, these artifacts consist of naturalistic human behavior and unscripted humorous instances occurring in real-world social contexts.

**Creative Capabilities Assessed:**
The benchmarks assess the capacity for humor recognition—a proxy for understanding linguistic and situational incongruity, timing, and social playfulness. Additionally, the work measures the ability to decode nuanced social attributes (e.g., assertiveness, warmth, and likability), which reflects social intelligence and the interpretation of affective impact.

**Evaluation Context:**
The evaluation utilizes a multimodal framework (integrating text, audio, and visual features) with automatic scoring. Humor detection is measured via Area Under the Curve (AUC), while social perception is quantified using the Mean Pearson's Correlation Coefficient ($\rho$) across 16 attributes. A defining feature is the cross-cultural and cross-lingual context, which tests the model's ability to generalize creative and social signals across diverse human populations.

**Task Characteristics:**
These are primarily discriminative and evaluative tasks focused on understanding and judgment rather than generation. They involve binary classification (detecting the presence of humor) and multi-target regression (predicting social perception scores). The tasks emphasize cross-domain transfer and the challenge of modeling subjective, culturally-dependent human experiences.",54f57b46db8caa48fcd8519b3bf081c4e4c0c2d6:Social Perception Sub-Challenge (MuSe-Perception),Social Perception Sub-Challenge (MuSe-Perception),https://www.muse-challenge.org/,2.0,GPU/Local,1.0,False,False,True,request_access,none,multimodal
5678a6e9bd9b4c56f45219e9ccf1a5b6356963d4,MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding,MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding,2021.0,2021.0,,Figurative Language & Rhetoric,Poetry & Verse,high,"The paper's core focus is the generation of metaphors and symbolic language through cross-domain mapping, while also applying these techniques to the creation and enhancement of poetic quatrains.","**Creative Artifacts Evaluated:**
Metaphorical sentences and four-line poem quatrains. The focus is specifically on figurative language produced by substituting literal verbs with symbolic, non-literal alternatives within a given context.

**Creative Capabilities Assessed:**
The benchmarks evaluate metaphoricity (the strength and appropriateness of conceptual mappings), creativity (originality of expression), and semantic preservation (the ability to maintain the literal sentence's underlying meaning). It also assesses linguistic fluency and aesthetic quality through human preference for model-enhanced poetry over original human-written versions.

**Evaluation Context:**
This text-based research utilizes Large Language Models (LLMs) with a focus on discriminative decoding techniques to guide symbolic selection. Evaluation is a hybrid of automatic metrics (SBERT semantic similarity, BERTScore, BLEU-2) and human qualitative assessment using 1-5 Likert scales and head-to-head preference tests.

**Task Characteristics:**
The tasks involve constrained generation and refinement. Specifically, the model performs literal-to-metaphorical transformation through targeted verb replacement. This requires cross-domain transfer from concrete actions to abstract symbols, functioning both as a standalone generator and a collaborative tool for poetic enhancement.",5678a6e9bd9b4c56f45219e9ccf1a5b6356963d4:Metaphor Generation from Literal Sentences,Metaphor Generation from Literal Sentences,https://github.com/tuhinjubcse/MetaphorGenNAACL2021,2.0,GPU/Local,1.0,False,False,True,github,none,text
59306cd073f3ba0db6e191be9f2bfa6ed41e1f44,AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies,AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies,2025.0,2025.0,,Engineering & Technical Design,Scientific Discovery; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation of analog circuit topologies, which is explicitly mentioned in the Engineering & Technical Design domain, while also emphasizing the discovery of novel structures and 'outside-the-box' problem solving.","**Creative Artifacts Evaluated:**
The engine generates novel **analog circuit topologies** represented as netlists or schematics. Specific artifacts include **operational amplifiers (Op-Amps)**, **power converters**, **bandgap references**, and **transconductance amplifiers**.

**Creative Capabilities Assessed:**
The framework measures **technical correctness** (simulatability and connectivity validity), **originality** (structural novelty relative to a training dataset), and **functional utility** (performance via Figure-of-Merit after parameter sizing). It also evaluates **zero-shot generalization**, assessing the model's ability to ideate functional structures for unseen circuit categories.

**Evaluation Context:**
Evaluation is **multimodal and hybrid**, combining automated simulation-based scoring with **algorithmic optimization** (genetic algorithms for component sizing). It utilizes **quantitative metrics** for novelty and scalability alongside **human qualitative assessment** of schematic visualizations rendered in industry-standard design tools.

**Task Characteristics:**
Tasks involve **constrained generation** within the rigorous physical bounds of electrical engineering, **open-ended discovery** of non-obvious topologies, and **cross-domain transfer** (zero-shot generation). The process emphasizes **divergent thinking** in structural design followed by **convergent refinement** through performance-driven optimization, distinguishing it from purely aesthetic or linguistic creativity.",59306cd073f3ba0db6e191be9f2bfa6ed41e1f44:Analog Circuit Topology Generation,Analog Circuit Topology Generation,https://github.com/xz-group/AnalogGenie,3.0,Special,4.0,False,False,True,github,none,text
5976110766078421c0cf16d69bca1fe26787e6f7,Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team,Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team,2025.0,2025.0,,Scientific Discovery,Dialogue Generation & Social Interaction; Functional & Professional Writing,high,The paper focuses on the generation and evaluation of novel scientific research ideas and abstracts using a multi-agent collaborative framework that simulates professional research team dynamics.,"**Creative Artifacts Evaluated:**
The primary artifacts are **scientific research abstracts** within the domains of Computer Science and Health Sciences. These represent condensed, high-level conceptualizations of potential research contributions, focusing on the synthesis of novel methodologies, problem statements, and hypothesized results.

**Creative Capabilities Assessed:**
The framework evaluates **scientific originality** and **potential impact**. Specifically, it measures **historical and contemporary novelty** (the degree to which the generated idea diverges from past and present literature) and **conceptual influence** (the predicted impact on the field). It assesses the system's capacity for **divergent ideation** and **convergent refinement** through multi-agent collaboration.

**Evaluation Context:**
This is a **text-based LLM evaluation** focused on multi-agent dynamics. Unlike general creative writing, it utilizes **specialized automatic metrics**—Historical Dissimilarity (HD), Contemporary Dissimilarity (CD), and Contemporary Impact (CI)—to provide a quantitative proxy for scientific creativity, bypassing traditional human peer review for rapid benchmarking.

**Task Characteristics:**
The core task is **open-ended scientific ideation** and **collaborative refinement**. It involves a multi-agent ""research team"" performing **constrained generation** where the constraints are defined by the existing body of scientific knowledge. The process emphasizes **dynamic knowledge exchange** and **structured peer review** among agents to optimize the trade-off between radical novelty and domain relevance.",5976110766078421c0cf16d69bca1fe26787e6f7:Computer Sciences Dataset,Computer Sciences Dataset,https://github.com/Link-AGI/AutoAgents,2.0,GPU/Local,1.0,False,False,True,github,none,text
5642a971326526427844939355e9b0e6309ea748,AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity,AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper evaluates associative thinking and visual-conceptual mapping through pareidolia, which requires identifying structural isomorphisms (Lateral Thinking) and cross-domain metaphorical similarities (Figurative Language).","**Creative Artifacts Evaluated:**
The benchmark focuses on images of natural phenomena—such as clouds, rock formations, and coastlines—that exhibit structural resemblances (pareidolia) to common man-made objects or distinct semantic categories.

**Creative Capabilities Assessed:**
The primary focus is on associative thinking and visual-conceptual mapping. The benchmark measures a model's ability to bridge disparate domains (nature vs. everyday objects) through metaphorical similarity. It specifically assesses the robustness of these associations by testing the model’s resilience against internal ambiguity (unreasonable answer choices) and external ambiguity (plausible but incorrect distractors).

**Evaluation Context:**
Multimodal Large Language Models (MLLMs) are evaluated in an image-to-text modality. Evaluation is fully automatic, utilizing Top-1 and weighted average accuracy across varying task complexities (4-choice, 7-choice, and 10-choice formats). The framework distinguishes associative creativity from general cognitive ability by correlating performance with the MMMU benchmark.

**Task Characteristics:**
Tasks consist of constrained identification and visual association. The methodology employs a comparative analysis using three distinct sets—Original (Ori), Internal Ambiguity (Int), and External Ambiguity (Ext)—to isolate the model's capacity for precise ideational association from its susceptibility to linguistic or visual noise.",5642a971326526427844939355e9b0e6309ea748:AssoCiAm,AssoCiAm,,2.0,GPU/Local,1.0,False,False,True,github,openai; anthropic; google,multimodal
5bd0fa95a23e23259e2a5df187d69fe9da6ff290,Learning to Evaluate Humor in Memes Based on the Incongruity Theory,Learning to Evaluate Humor in Memes Based on the Incongruity Theory,2022.0,2022.0,,Humor & Satire,Visual Arts & Stylized Imagery,high,"The paper focuses on evaluating humor and incongruity resolution in internet memes, which are explicitly listed under the Humor & Satire domain, while the multimodal nature of the task involves the interpretation of visual imagery.","**Creative Artifacts Evaluated:**
The study focuses on internet memes, specifically multimodal pairings of static images and natural language captions. These artifacts are characterized by the complex semiotic interplay between visual context and textual punchlines, where the meaning of the text is often transformed by the accompanying image.

**Creative Capabilities Assessed:**
The primary capability assessed is humor recognition, specifically the detection of ""incongruity""—the psychological gap between expected and actual outcomes. The benchmark measures a model's ability to perceive the subtle semantic dissonance and subsequent resolution that constitutes a creative joke, evaluating whether the model can identify the ""click"" of a successful humorous juxtaposition.

**Evaluation Context:**
This is a multimodal evaluation involving Vision-Language Models (VLMs). To address the inherent subjectivity of humor, the ground truth was established through human annotation using Best-Worst Scaling (BWS). Model performance is measured via automatic classification accuracy across 10-fold cross-validation, testing the computational capacity to align with human-perceived humor levels.

**Task Characteristics:**
The core task is a discriminative evaluation/judgment task. Unlike generative tasks, it requires the model to act as a critic, performing binary classification to distinguish humorous content from non-humorous pairings. It emphasizes the ""understanding"" phase of the creative process, specifically how incongruous elements are integrated and resolved across different modalities.",5bd0fa95a23e23259e2a5df187d69fe9da6ff290:Meme Humor Dataset (Ours),Meme Humor Dataset (Ours),,2.0,GPU/Local,1.0,False,False,True,unknown,,multimodal
5b2339bb7a51fc29d90a7f231999740c66695f51,EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation,EXPOTION: Facial Expression and Motion Control for Multimodal Music Generation,2025.0,2025.0,,Music & Auditory Arts,Performing Arts & Video Synthesis,high,"The paper focuses on generating musical compositions synchronized with human facial expressions and body motion, combining auditory creation with temporal and expressive cues from performance.","**Creative Artifacts Evaluated:**
The primary artifacts are synchronized musical compositions and soundtracks. Specifically, these are high-fidelity audio clips designed to mirror the nuanced affective states of human facial micro-expressions and the rhythmic patterns of physical body movements.

**Creative Capabilities Assessed:**
The benchmark measures the model’s capacity for affective translation (converting visual emotional cues into auditory moods) and temporal precision (aligning musical beats with physical motion). It evaluates aesthetic musicality, semantic consistency with text prompts, and divergent creativity—the ability to generate novel, non-obvious musical ideas that maintain expressive coherence with the visual input.

**Evaluation Context:**
This is a complex multimodal framework involving video, text, and audio. Evaluation is ""mixed,"" synthesizing subjective human ratings of creativity and consistency with a suite of automatic metrics. These include CLAP and LanguageBind for cross-modal semantic alignment, alongside signal-based metrics like FAD-VGG, Tempo Error, and Beat F1-score to assess technical audio quality and rhythmic synchronization.

**Task Characteristics:**
The task is defined as constrained multimodal generation and cross-domain transfer. It requires mapping non-verbal human behavioral cues (facial/motion) to musical structures, necessitating a balance between strict temporal constraints and the open-ended nature of artistic composition. It moves beyond simple text-to-audio by requiring fine-grained control over expression and motion.",5b2339bb7a51fc29d90a7f231999740c66695f51:EXPOTION Multimodal Music Generation,EXPOTION Multimodal Music Generation,https://github.com/xinyueli2896/Expotion.git,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
5c24dd41f46fd7107997b0a46e1207e0fed63b34,AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,AnaloBench: Benchmarking the Identification of Abstract and Long-context Analogies,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing,high,"The paper explicitly focuses on analogical reasoning and identifying structural isomorphisms, which are core components of the Lateral Thinking domain, while using narrative stories as the primary medium for evaluation.","**Creative Artifacts Evaluated:**
The benchmark focuses on **short-form narrative stories** characterized by complex relational structures. These artifacts are specifically curated to contain deep thematic or structural patterns that can be mapped across different narrative contexts, moving beyond simple keyword matching.

**Creative Capabilities Assessed:**
The tasks measure **analogical reasoning** and **abstract conceptualization**, specifically the ability to prioritize **deep relational similarity** over superficial surface-level commonalities. It assesses **convergent thinking** through the identification of structural isomorphisms and the capacity to maintain **conceptual depth** when processing long-context textual inputs.

**Evaluation Context:**
Evaluation is strictly **text-based**, targeting the cognitive performance of **Large Language Models (LLMs)**. The framework employs a hybrid scoring model: **automatic retrieval metrics** (MAP, Precision@K, MRR) for large-scale banks and **comparative accuracy** against human baselines for small-scale selection, emphasizing performance in high-noise environments.

**Task Characteristics:**
The tasks center on **discriminative judgment** and **information retrieval** rather than open-ended generation. They require **cross-domain transfer** (mapping a story’s logic from one domain to another) and **constrained evaluation**, testing the model's ability to identify creative parallels within both small (4-option) and large (200-story) datasets.",5c24dd41f46fd7107997b0a46e1207e0fed63b34:ANALOBENCH,ANALOBENCH,https://huggingface.co/datasets/jhu-clsp/AnaloBench,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic; google,text
5ac80c35214f8d49625cb1c1d899846a65ef0599,Exploring Scientific Hypothesis Generation with Mamba,Exploring Scientific Hypothesis Generation with Mamba,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation and evaluation of novel scientific hypotheses through literature synthesis and constrained ideation, directly aligning with Scientific Discovery while also requiring Lateral Thinking & Creative Problem Solving to bridge existing knowledge gaps.","**Creative Artifacts Evaluated:**
The primary artifacts are literature-informed scientific hypotheses. These consist of specific target sentences that propose novel research directions, mechanisms, or methodologies within the domain of Natural Language Processing (NLP), synthesized from provided background context and seed terms.

**Creative Capabilities Assessed:**
The evaluation measures four distinct dimensions of scientific creativity: novelty (originality relative to existing literature), soundness (logical consistency and scientific feasibility), relevance (alignment with the provided background context), and clarity (the effectiveness and fluency of the communication).

**Evaluation Context:**
This research operates in the text modality, specifically comparing the performance of State Space Models (Mamba) against traditional Transformers in long-context scenarios. The evaluation framework is a hybrid of automatic linguistic metrics (ROUGE-L, BERTScore), LLM-as-judge qualitative assessments (using Claude-3.5), and rigorous human expert validation to determine the ""effectiveness"" of the generated ideas.

**Task Characteristics:**
The core task is a form of constrained ideation and scientific reasoning. It requires open-ended generation where the model must synthesize complex scientific background information to bridge existing knowledge with new, untested propositions. Unlike standard question-answering, this task emphasizes creative synthesis and literature-informed reasoning to generate plausible yet original research contributions.",5ac80c35214f8d49625cb1c1d899846a65ef0599:SciMON,SciMON,https://github.com/fglx-c/Exploring-Scientific-Hypothesis-Generation-with-Mamba,2.0,GPU/Local,1.0,False,False,True,github,openai,text
5b9f4feec0a7305574dbdd8529ebae523e2be3e9,GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced Aesthetic Text Glyph Layouts,GLDesigner: Leveraging Multi-Modal LLMs as Designer for Enhanced Aesthetic Text Glyph Layouts,2024.0,2024.0,,Graphic Design & Visual Layout,Visual Arts & Stylized Imagery,high,"The paper specifically addresses the spatial arrangement, typography, and visual composition of text glyphs in logo design, which aligns perfectly with the Graphic Design & Visual Layout domain.","**Creative Artifacts Evaluated:**
The primary artifacts are aesthetic text logo layouts, specifically the spatial arrangement, scaling, and orientation of individual glyphs (characters) within a graphic design framework. These include both synthetic designs derived from templates and complex, real-world logo compositions.

**Creative Capabilities Assessed:**
Assessment focuses on aesthetic quality, visual balance, and spatial reasoning. Key capabilities include the ability to maintain glyph ratio consistency, minimize unintended overlaps (Overlap IoU), and adhere to specific natural language design constraints (e.g., ""horizontal alignment""). The model demonstrates both divergent thinking in unconstrained generation and convergent thinking when following user-specified layout rules.

**Evaluation Context:**
The research employs Vision-Language Models (VLMs) across three datasets: the reused TextLogo3K, the synthetic SynTextLogo, and the real-world GenTextLogo. Evaluation is a hybrid of automatic geometric metrics (FID, IS, Visual Balance) and human-centric qualitative assessments, including preference rates and constraint violation ratios (ViO).

**Task Characteristics:**
Tasks involve both open-ended generation and constrained design. The model must translate abstract text inputs into concrete spatial coordinates, functioning as an automated designer. This requires interpreting natural language instructions to perform complex layout ideation and refinement, bridging the gap between linguistic concepts and visual-spatial execution.",5b9f4feec0a7305574dbdd8529ebae523e2be3e9:TextLogo3K,TextLogo3K,https://github.com/yizhiwang96/TextLogoLayout,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
5e2fabb7105edb8a904c7a984c3009a5ab34eaf7,ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition,ZeroSumEval: Scaling LLM Evaluation with Inter-Model Competition,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Mathematical Reasoning; Programming & Algorithmic Creativity; Dialogue Generation & Social Interaction,high,"The paper introduces a competitive evaluation framework centered on strategic reasoning and adversarial problem-solving, requiring models to find non-obvious solutions in zero-sum games, social engineering, and coding exploits.","**Creative Artifacts Evaluated:**
The framework evaluates strategic game moves (FEN notation, betting actions), solvable mathematical problems, persuasive debate arguments, social engineering prompts for password extraction, and Python sandbox/exploit code.

**Creative Capabilities Assessed:**
Assessment focuses on strategic reasoning, adversarial problem-solving, and technical correctness. It specifically measures ""adversarial creativity""—the ability to generate novel social engineering tactics or complex defensive code structures—alongside persuasive fluency, logical consistency, and convergent thinking within game-theoretic constraints.

**Evaluation Context:**
ZeroSumEval utilizes a dynamic, multi-player competitive environment where LLMs interact in text-based, zero-sum scenarios. Evaluation is primarily automated through win/loss outcomes and code execution, supplemented by LLM-as-a-judge juries for subjective tasks. Performance is quantified using Bradley-Terry (BT) ratings to establish a relative skill hierarchy.

**Task Characteristics:**
The benchmark features interactive, competitive tasks rather than static prompts. These include constrained generation (Chess/Poker), open-ended adversarial dialogue (Gandalf), and symmetric problem-solving (MathQuiz/PyJail). Tasks often require models to alternate between ""creator/defender"" and ""solver/attacker"" roles, necessitating a blend of generative skill, evaluative judgment, and cross-domain application of logic and deception.",5e2fabb7105edb8a904c7a984c3009a5ab34eaf7:ZeroSumEval,ZeroSumEval,https://github.com/facebookresearch/ZeroSumEval,1.0,API-Only,7.0,True,False,True,github,openai; anthropic,text
5bbc93573edc73b360f76ae3ba87fe698188ab7c,CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation,CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation,2025.0,2025.0,,Scientific Discovery,Programming & Algorithmic Creativity; Functional & Professional Writing,high,"The paper explicitly focuses on the end-to-end process of generating scientific hypotheses and experimental designs, while utilizing executable code and technical reports as the primary artifacts of this discovery process.","**Creative Artifacts Evaluated:**
The system produces structured research proposals, executable Python experimental code, scientific reports, and novel hypotheses. It also generates specialized agent architectures—such as graph-based or affordance-predicting agents—designed to test specific scientific questions within simulated environments.

**Creative Capabilities Assessed:**
Evaluation focuses on scientific novelty, technical soundness, and ""interestingness."" It measures the system's capacity for divergent thinking through the proposal of unique experimental variables and its ability to maintain technical correctness and reproducibility in generated code. The assessment balances conceptual depth with functional utility.

**Evaluation Context:**
The framework operates across text and code modalities. It employs a hybrid evaluation strategy: human expert peer reviews (mimicking conference standards), domain-expert code verification for replication, and automated performance metrics (e.g., task completion scores, correlation coefficients) within virtual laboratories like DiscoveryWorld and ScienceWorld.

**Task Characteristics:**
Tasks involve end-to-end scientific discovery, encompassing ideation, experimental design, and execution. This includes open-ended generation of research questions and highly constrained code generation for specific simulation environments, requiring a transition from abstract reasoning to concrete, functional problem-solving and cross-domain transfer.",5bbc93573edc73b360f76ae3ba87fe698188ab7c:CodeScientist Scientific Discovery Evaluation,CodeScientist Scientific Discovery Evaluation,https://github.com/allenai/codescientist,3.0,Special,1.0,False,False,True,github,openai,text
60b21c4f48daf76465365c45040bcf08c046c573,QUDsim: Quantifying Discourse Similarities in LLM-Generated Text,QUDsim: Quantifying Discourse Similarities in LLM-Generated Text,2025.0,2025.0,,Narrative & Story Writing,Functional & Professional Writing,high,The paper develops a metric to evaluate the structural originality and discourse diversity of LLM-generated narrative and expository prose by analyzing underlying rhetorical skeletons and formulaic patterns.,"**Creative Artifacts Evaluated:**
The research evaluates LLM-generated narrative and expository prose, specifically focusing on segmented discourse units. These artifacts are analyzed as hierarchical structures where each segment is mapped to a ""Question Under Discussion"" (QUD), representing the underlying logical skeleton of the text rather than its surface-level content.

**Creative Capabilities Assessed:**
The framework assesses structural originality and discourse diversity. By measuring the degree of formulaic writing and structural repetitiveness, it evaluates a model’s ability to deviate from rigid, predictable rhetorical patterns. It specifically quantifies the ""sameness"" of the underlying discourse architecture across multiple generated outputs, penalizing models that rely on repetitive structural templates.

**Evaluation Context:**
This is a text-based evaluation context that utilizes a hybrid approach. It employs human-annotated alignment sets to validate the metric (using F1 scores) and automated metrics (QUDsim) to analyze large-scale LLM corpora. Unlike standard semantic similarity tools, this context focuses on the functional and rhetorical alignment of text segments across different generative models.

**Task Characteristics:**
The tasks involve comparative discourse analysis and structural pattern recognition. Specifically, the work focuses on segment-level alignment, discourse homogeneity analysis, and ""template tracking""—the identification of recurring sequences of discourse moves. These tasks are designed to diagnose systemic formulaic behavior in open-ended generation by identifying hidden structural ""skeletons"" shared between disparate texts.",60b21c4f48daf76465365c45040bcf08c046c573:QUDsim,QUDsim,https://github.com/AlliteraryAlligator/QUDsim,1.0,API-Only,3.0,True,False,True,github,openai,text
611fcb2c87b53a0e4973e8c94bbda61f92018a9a,GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art,GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Dialogue Generation & Social Interaction; Lateral Thinking & Creative Problem Solving,high,"The paper evaluates 'GOD-level' video comments characterized by wit, humor, and internet subculture, while explicitly assessing metaphors, role-playing (role immersion), and divergent thinking (entity innovation).","**Creative Artifacts Evaluated:**
The benchmark focuses on ""GOD-level"" video comments—highly creative, contextually nuanced text responses to video content. These artifacts are characterized by specific linguistic devices including humor, metaphors, and role immersion (role-playing) tailored to the visual and social context of the video.

**Creative Capabilities Assessed:**
GODBench measures both discriminative and generative abilities. It assesses a model's capacity for quality discrimination (selecting, ranking, and classifying comments), interpretive depth (identifying creative dimensions like metaphor), and generative originality. A key focus is placed on divergent thinking through ""entity innovation,"" as well as emotional resonance, stylistic impact, and the ability to maintain contextual relevance to visual stimuli.

**Evaluation Context:**
This is a multimodal evaluation (Video + Text) designed for Multimodal Large Language Models (VLMs). It utilizes a hybrid scoring framework: automatic metrics (Accuracy, NDCG, BLEU, ROUGE) for discriminative tasks, GPT-4o-based qualitative scoring for generative tasks (focusing on creativity and style), and human preference validation.

**Task Characteristics:**
The benchmark spans constrained discriminative judgment (multiple-choice selection and ranking), interpretive classification (tagging creative dimensions), and open-ended generation. It uniquely emphasizes the intersection of internet subculture and creative association, requiring models to bridge the gap between visual understanding and witty, non-obvious linguistic expression.",611fcb2c87b53a0e4973e8c94bbda61f92018a9a:GODBench,GODBench,https://github.com/stan-lei/GODBench-ACL2025,2.0,GPU/Local,6.0,False,False,True,github,openai,multimodal
6368da8af071a063f92b03460b5f1cf97338950a,Measuring LLM Novelty As The Frontier Of Original And High-Quality Output,Measuring LLM Novelty As The Frontier Of Original And High-Quality Output,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Poetry & Verse,high,"The paper establishes a benchmark for evaluating LLM novelty across three distinct creative tasks: story completion, constrained poetry, and 'MacGyver-style' practical problem-solving which requires lateral thinking and bypassing functional fixedness.","**Creative Artifacts Evaluated**
The benchmark evaluates short-form narrative prose (story completions based on the TinyStories dataset), single-line constrained poetry (following specific CoPoet stylistic instructions), and descriptive solutions for physical ""MacGyver-style"" problems requiring the creative application of common household objects.

**Creative Capabilities Assessed**
The framework measures the ""Novelty Frontier,"" specifically balancing linguistic originality—defined as the statistical rarity of n-grams (n=4, 5, 6) relative to reference data—with task-specific quality. It assesses the model’s ability to maintain coherence, adhere to complex stylistic constraints, and demonstrate divergent thinking in practical problem-solving scenarios without sacrificing functional utility.

**Evaluation Context**
This text-based evaluation employs a hybrid scoring methodology. It synthesizes objective, automated metrics for originality (unseen n-gram fractions) with subjective quality assessments (0-5 scale) provided by a high-reasoning LLM-as-a-judge (o3-mini). The final score is the harmonic mean of these two dimensions, penalizing models that are original but nonsensical or high-quality but derivative.

**Task Characteristics**
The tasks encompass open-ended narrative generation, highly constrained creative writing, and practical ideation. This multi-domain approach differentiates between purely aesthetic creativity and functional creativity, requiring models to navigate the trade-off between generating statistically novel sequences and satisfying rigorous task requirements.",6368da8af071a063f92b03460b5f1cf97338950a:Measuring LLM Novelty as the Frontier of Original and High-Quality Output,Measuring LLM Novelty as the Frontier of Original and High-Quality Output,https://github.com/YuehHanChen/quantifying-novelty,1.0,API-Only,3.0,True,False,True,github,openai,text
640bc34dc18657b2f9707277f455a573e40f7368,D ELTA S CORE : Story Evaluation with Perturbations,D ELTA S CORE : Story Evaluation with Perturbations,2023.0,2023.0,,Narrative & Story Writing,,high,"The paper focuses exclusively on the evaluation of AI-generated textual narratives, specifically assessing dimensions such as coherence, thematic relatedness, and narrative tension in short and long-form fiction.","**Creative Artifacts Evaluated:**
The research focuses on AI-generated textual narratives, specifically short-form five-sentence stories (ROCStories) and longer, more complex creative fiction derived from user-provided prompts (WritingPrompts).

**Creative Capabilities Assessed:**
The evaluation measures five fine-grained narrative dimensions: linguistic fluency, global and local coherence, thematic relatedness to the initial prompt, internal logicality (causal and temporal consistency), and subjective interestingness (the capacity to engage a reader through novelty or narrative tension).

**Evaluation Context:**
Operating within the text modality, the study assesses the performance of automated metrics against human-annotated ground truths. It utilizes a 1–5 ordinal scale for human ratings and employs Kendall correlation to determine how well LLM-based evaluation metrics align with human perception. The context is specifically the meta-evaluation of story quality assessment tools rather than the generation process itself.

**Task Characteristics:**
The work centers on the evaluation and judgment of both open-ended and constrained narrative generation. It distinguishes itself through a perturbation-based approach, where the metric's discriminative power is tested by its ability to detect intentional degradations in story structure. This shifts the focus from simple n-gram overlap to a deeper semantic and structural understanding of narrative quality.",640bc34dc18657b2f9707277f455a573e40f7368:Story Generation Evaluation on ROCStories and WritingPrompts,Story Generation Evaluation on ROCStories and WritingPrompts,https://github.com/ZhuohanX/DeltaScore,2.0,GPU/Local,1.0,False,False,True,github,openai,text
62cfa51a4d32b311c1385afc2207c0056369e231,MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing,MixAssist: An Audio-Language Dataset for Co-Creative AI Assistance in Music Mixing,2025.0,2025.0,,Music & Auditory Arts,Dialogue Generation & Social Interaction; Engineering & Technical Design; Functional & Professional Writing,high,"The paper focuses on AI assistance for music mixing, requiring a blend of musical knowledge, technical audio engineering expertise, and the ability to provide pedagogical and instructional feedback through dialogue.","**Creative Artifacts Evaluated:**
Instructional dialogue, technical mixing recommendations, and pedagogical feedback tailored to specific multitrack or stereo audio segments. The outputs are textual responses designed to guide the creative process of music production and audio engineering.

**Creative Capabilities Assessed:**
Technical accuracy in audio engineering principles (e.g., equalization, dynamic range control, spatial positioning), pedagogical helpfulness, and linguistic fluency. The benchmark specifically measures the assistant's ability to provide novel, creative suggestions that facilitate co-creative problem-solving and aesthetic refinement in a studio context.

**Evaluation Context:**
A multimodal (audio-text) environment utilizing Large Language Models. Evaluation is multi-faceted, integrating automated linguistic metrics (BLEU, BERTScore) with LLM-as-a-judge rankings for technical merit, human expert preference comparisons, and real-time user interaction studies assessing naturalness, novelty, and creative inspiration.

**Task Characteristics:**
Open-ended, instructional generation within a co-creative framework. The task emphasizes cross-modal reasoning—translating perceived audio characteristics into actionable creative advice—and focuses on the ""AI-as-assistant"" role in iterative artistic refinement and ideation rather than autonomous artifact generation. It bridges the gap between technical signal processing and creative linguistic instruction.",62cfa51a4d32b311c1385afc2207c0056369e231:MIXASSIST,MIXASSIST,https://huggingface.co/datasets/mclemcrew/MixAssist,2.0,GPU/Local,1.0,False,True,True,huggingface,openai,multimodal
6784651527c5c25cf7c202124ef55e2fcea58a96,Arabic Automatic Story Generation with Large Language Models,Arabic Automatic Story Generation with Large Language Models,2024.0,2024.0,,Narrative & Story Writing,,high,"The paper focuses exclusively on the generation and evaluation of short-form narrative stories in Arabic, emphasizing narrative coherence and creative prose.","**Creative Artifacts Evaluated:**
The primary artifacts are short-form Arabic narrative stories generated in response to specific textual prompts. These stories represent creative prose produced within a morphologically rich and linguistically complex language context.

**Creative Capabilities Assessed:**
The framework measures linguistic fluency, narrative coherence, and internal consistency (logical flow). It specifically evaluates ""Instruction Following""—the ability to adhere to specific thematic or structural constraints—and ""Variety Adherence,"" which assesses the lexical and stylistic diversity of the generated text. These metrics capture both the technical accuracy of the Arabic language and the creative depth of the storytelling.

**Evaluation Context:**
This research operates within the Arabic Natural Language Generation (NLG) domain, employing a hybrid evaluation paradigm. It utilizes ""LLM-as-a-judge"" by prompting GPT-4 to act as an Arabic language expert, providing 1–5 Likert scale scores. This is complemented by human evaluation, where native Arabic speakers perform pairwise preference rankings to provide a ground-truth assessment of creative quality.

**Task Characteristics:**
The tasks focus on constrained generation, where models must synthesize original narratives while respecting predefined prompt boundaries. Additionally, the framework includes an evaluative judgment task, testing the capability of LLMs to serve as reliable critics of creative work in a non-English, lower-resource language setting.",6784651527c5c25cf7c202124ef55e2fcea58a96:Arabic Story Generation Evaluation Framework,Arabic Story Generation Evaluation Framework,https://github.com/UBC-NLP/arastories,3.0,Special,2.0,False,False,True,github,openai,text
6394296dd9ac7265b9854d929a6b6f5515b7acb0,Can Multimodal Large Language Model Think Analogically?,Can Multimodal Large Language Model Think Analogically?,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper explicitly focuses on analogical mapping, structural isomorphisms, and cross-domain transfer, which are core components of the Lateral Thinking & Creative Problem Solving domain.","**Creative Artifacts Evaluated**
The evaluation focuses on **multimodal analogical completions**, specifically structured pairs consisting of textual verbs and representative images of nouns (e.g., ""peeling an orange"" mapped to ""skinning a potato""). These artifacts function as conceptual bridges, where the model must synthesize or retrieve the correct visual/textual entity to complete a relational quartet.

**Creative Capabilities Assessed**
The benchmarks measure **conceptual depth** and **convergent thinking** by requiring models to identify and apply implicit semantic relations. By testing models on unfamiliar, zero-shot scenarios, the paper assesses **cross-domain transfer** and the ability to recognize structural isomorphisms—foundational cognitive traits for creative problem-solving and ""out-of-the-box"" reasoning.

**Evaluation Context**
The context is strictly **multimodal (text-image)**, utilizing Vision-Language Models (VLMs). Evaluation primarily employs **automatic metrics** (Hits@k, MRR, and Accuracy) to measure retrieval and prediction precision, supplemented by **human evaluation** to validate the model's performance on the novel MBARD dataset.

**Task Characteristics**
Tasks involve **constrained generation** (predicting a missing entity) and **analogical validity judgment** (evaluating the correctness of a proposed analogy). Unlike open-ended synthesis, these tasks emphasize **relational problem-solving** and **ideation through mapping**, focusing on the model's capacity to transfer logic from a source domain to a target domain in a zero-shot manner.",6394296dd9ac7265b9854d929a6b6f5515b7acb0:MARS,MARS,https://github.com/Remwlp/MLLM_Analogy,2.0,GPU/Local,3.0,False,False,True,github,openai; anthropic; google,multimodal
658a67be17e32998480ff51ba4c0d832e86db441,MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria,MLLM-Bench: Evaluating Multimodal LLMs with Per-sample Criteria,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Functional & Professional Writing,high,"The benchmark specifically targets the 'Creation' cognitive level, focusing on ideation and the synthesis of visual information into novel outputs, while explicitly evaluating artifacts like creative narratives and analytical explanations.","**Creative Artifacts Evaluated:**
The benchmark evaluates multimodal outputs including creative narratives, descriptive interpretations of complex visual scenes, and structured analytical explanations. Specifically, within the ""Creation"" cognitive level, the artifacts consist of novel textual content generated in response to diverse image-instruction pairs.

**Creative Capabilities Assessed:**
Capabilities are measured across a hierarchy of six cognitive levels. At the highest level, ""Creation"" is assessed through originality, instruction-following, and the ability to synthesize visual information into coherent, imaginative outputs. The ""per-sample criteria"" method specifically targets the model's ability to meet nuanced, task-specific requirements rather than matching a single ground-truth answer.

**Evaluation Context:**
This multimodal framework evaluates 26 MLLMs using an AI-as-a-judge approach. High-capacity models (GPT-4-Turbo, Claude-3-Opus) perform pairwise comparisons against an anchor model. The evaluation is grounded in human alignment validation, measuring the agreement rate between AI judges and human experts on open-ended, subjective tasks.

**Task Characteristics:**
Tasks are categorized into 42 distinct capabilities across six cognitive dimensions (Remembering to Creating). These involve open-ended generation and complex reasoning where models must interpret visual stimuli to perform ideation, descriptive synthesis, and cross-domain knowledge application without a fixed gold standard.",658a67be17e32998480ff51ba4c0d832e86db441:MLLM-Bench,MLLM-Bench,https://github.com/FreedomIntelligence/MLLM-Bench,2.0,GPU/Local,2.0,False,False,True,github,openai; anthropic,multimodal
69723e2a291b5944b58f0e2bdb8e832a1b3b49ef,Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash,Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Dialogue Generation & Social Interaction,high,"The paper evaluates 'deceptive creativity' and divergent thinking through the game Balderdash, which requires agents to solve the creative problem of generating plausible falsehoods and identifying truth within a multi-agent, game-theoretic simulation.","**Creative Artifacts Evaluated:**
Fictitious, plausible-sounding dictionary definitions for obscure or common English vocabulary words.

**Creative Capabilities Assessed:**
Deceptive creativity (the ability to generate believable falsehoods that mimic authoritative lexicographic styles), linguistic fluency, and divergent thinking. It also measures convergent reasoning through the ability to distinguish truth from fabrication in an adversarial context.

**Evaluation Context:**
A multi-agent, text-only interactive simulation framework. Scoring is a hybrid of automated and social metrics: a ""Judge LLM"" determines semantic equivalence (validated against human-annotated ground truth), while peer-agent voting patterns provide empirical performance data such as the Deception Ratio (DR) and Correct Guess Ratio (CGR).

**Task Characteristics:**
The framework utilizes a combination of open-ended generation (ideating deceptive definitions) and constrained evaluation (identifying the correct definition from a list). It is characterized by social, game-theoretic interactions where success is defined by the ability to mislead others while avoiding being misled, moving beyond static generation to interactive, adversarial creativity.",69723e2a291b5944b58f0e2bdb8e832a1b3b49ef:LLM-MA Balderdash,LLM-MA Balderdash,https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash,2.0,GPU/Local,2.0,False,False,True,request_access,openai; huggingface,text
6cf85f6782322e4669deb1eefda341c31b93ca9b,BannerAgency: Advertising Banner Design with Multimodal LLM Agents,BannerAgency: Advertising Banner Design with Multimodal LLM Agents,2025.0,2025.0,,Graphic Design & Visual Layout,Functional & Professional Writing,high,"The paper focuses on the automated generation and evaluation of advertising banners, emphasizing layout composition, spatial reasoning, and brand identity synergy, while also incorporating marketing copywriting.","**Creative Artifacts Evaluated:**
The primary artifacts are digital advertising banners generated from multimodal inputs. These designs must synthesize specific brand logos with textual descriptions across diverse commercial sectors, including electronics, fashion, and food and beverage.

**Creative Capabilities Assessed:**
The benchmark measures domain-specific creative intelligence, focusing on brand identity synergy, target audience alignment, and the persuasive effectiveness of call-to-action (CTA) elements. It assesses aesthetic quality, copywriting fluency, and spatial reasoning (e.g., logo placement suitability and layout logic). Furthermore, it evaluates iterative refinement—the ability of an agent to improve a design based on specific feedback.

**Evaluation Context:**
This framework utilizes a multimodal evaluation strategy combining human expert judgment, LLM-based (GPT-4o) scoring, and automatic metrics (FID). It employs a sophisticated rubric of 14 metrics: six advertising-specific criteria (e.g., Brand Identity Synergy) and eight general design principles (e.g., Visual Balance, Color Harmony), alongside pairwise human preference studies to validate LLM alignment.

**Task Characteristics:**
The core task is constrained multimodal generation, requiring the integration of existing visual assets (logos) with generated marketing copy. It emphasizes ""agentic"" design workflows, where the system interprets open-ended requests, plans layouts, and executes multi-step refinements, distinguishing it from standard one-shot text-to-image generation.",6cf85f6782322e4669deb1eefda341c31b93ca9b:BannerRequest400,BannerRequest400,https://github.com/sony/BannerAgency,1.0,API-Only,1.0,True,False,True,github,openai; anthropic,multimodal
701658eaa26a9350792b88cc07fea98399a94791,PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation,PodEval: A Multimodal Evaluation Framework for Podcast Audio Generation,2025.0,2025.0,,Functional & Professional Writing,Dialogue Generation & Social Interaction,high,The framework specifically evaluates the semantic coverage and alignment of technical and professional documentation (Functional & Professional Writing) for use in LLM-driven podcast generation systems (Dialogue Generation & Social Interaction).,"**Creative Artifacts Evaluated:**
The artifacts evaluated are not traditional artistic works but rather **synthetic test suites** (natural language questions) and **structured knowledge corpora**. These include technical product documentation (415 chunks), specialized financial documents, and ""distractor"" corpora containing irrelevant biological information about birds.

**Creative Capabilities Assessed:**
The framework assesses **semantic thoroughness** and **conceptual breadth**. Rather than evaluating originality or aesthetics, it measures **alignment precision**—the ability of a test set to comprehensively map the information density of a source domain. It also evaluates **discriminative accuracy**, specifically the capacity to identify gaps in knowledge coverage and distinguish between relevant domain-specific data and irrelevant noise.

**Evaluation Context:**
This is a **text-based** evaluation environment tailored for **LLM-driven Retrieval-Augmented Generation (RAG) systems**. It utilizes **automatic scoring** methods grounded in **embedding-based cosine distance**. Metrics include basic coverage, weighted coverage, and multi-cluster coverage, providing a quantitative audit of how well a small sample of questions represents a large-scale documentation set.

**Task Characteristics:**
The tasks focus on **evaluation, judgment, and knowledge auditing** rather than open-ended generation. They involve **constraint-based mapping** and **noise detection**. Specifically, the tasks require **semantic clustering** to identify ""incomplete coverage"" within a test set and **cross-domain filtering** to detect irrelevant knowledge, ensuring the evaluation data is both exhaustive and domain-pure.",,,,,,,,,,,,
6d5e03343e4982a33bd1633975b0e3ea7b32e79a,SS-Bench: A Benchmark for Social Story Generation and Evaluation,SS-Bench: A Benchmark for Social Story Generation and Evaluation,2024.0,2024.0,,Functional & Professional Writing,Narrative & Story Writing,high,"The paper focuses on generating Social Stories, which are explicitly defined as functional, pedagogical, and instructional narratives designed for clinical use, aligning with the pedagogical design and professional constraints of Functional & Professional Writing.","**Creative Artifacts Evaluated:**
The benchmark evaluates ""Social Stories,"" which are short, highly structured pedagogical narratives specifically designed to assist children with autism in navigating social situations. These artifacts are functional, instructional texts rather than open-ended fiction.

**Creative Capabilities Assessed:**
Assessment focuses on the ability to synthesize creative narrative with rigorous structural constraints. Key capabilities include descriptive orientation (guiding behavior through observation), situational safety (ensuring advice is socially and physically appropriate), empathy, and structural clarity. The benchmark measures the model's ability to maintain a specific ratio of sentence types (descriptive vs. directive) as defined by clinical standards.

**Evaluation Context:**
This is a text-based generation task for Large Language Models (LLMs). It utilizes a tripartite evaluation framework: human expert ratings for pedagogical integrity and safety, LLM-as-a-judge (GPT-4) for qualitative attributes like empathy and coherence, and automatic NLP metrics (BLEU, ROUGE, BERTScore) for lexical and semantic alignment with ground-truth examples.

**Task Characteristics:**
The task is defined as highly constrained creative generation. Unlike generic storytelling, it requires domain-specific knowledge transfer from clinical pedagogy to narrative construction. It involves balancing creative engagement with strict adherence to the ""Social Story"" formula, making it a safety-critical application of constrained writing.",6d5e03343e4982a33bd1633975b0e3ea7b32e79a:SS-GEN,SS-GEN,https://github.com/MIMIFY/SS-Bench,2.0,GPU/Local,1.0,False,True,True,huggingface,openai; huggingface,text
6b5a7f5a2edf0c752905a07ab83661ce7dc4954c,Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation,Speak-to-Structure: Evaluating LLMs in Open-domain Natural Language-Driven Molecule Generation,2024.0,2024.0,,Scientific Discovery,Engineering & Technical Design,high,"The paper focuses on the generation and optimization of novel molecular structures for chemical discovery, emphasizing technical feasibility, originality, and functional utility within a STEM context.","**Creative Artifacts Evaluated:**
The primary artifacts are novel molecular structures, represented as SMILES strings. These function as chemical designs or blueprints for potential synthetic compounds, ranging from modified versions of existing drugs to entirely de novo chemical entities.

**Creative Capabilities Assessed:**
The benchmark measures technical correctness (chemical validity), originality (novelty relative to the Zinc-250K database), and functional utility (optimizing properties like LogP, molecular refractivity, or drug-likeness). It specifically evaluates the model's capacity for divergent thinking (generating diverse novel structures from a single prompt) and convergent thinking (adhering to strict structural constraints such as specific atom/bond counts or functional group requirements).

**Evaluation Context:**
Evaluation is entirely automatic, utilizing cheminformatics metrics to assess structural similarity (Tanimoto) and property changes. The context is a text-to-structure paradigm where natural language prompts drive the generation of complex, structured chemical data, framing molecule discovery as a creative design task rather than a simple retrieval or translation problem.

**Task Characteristics:**
Tasks span the creative lifecycle: constrained generation (creating molecules from scratch with specific structural parameters), iterative refinement (adding, deleting, or substituting functional groups), and goal-oriented optimization (modifying structures to achieve desired chemical properties). This framing emphasizes one-to-many mappings, where multiple valid ""creative"" solutions exist for a single prompt.",6b5a7f5a2edf0c752905a07ab83661ce7dc4954c:Speak-to-Structure (S2-Bench),Speak-to-Structure (S2-Bench),https://openreview.net/forum?id=V2K66z1k7a,2.0,GPU/Local,9.0,False,True,True,huggingface,openai; anthropic; google,text
6c6cb32f026fccc11c98c36913651b992d477a56,Divergent Creativity in Humans and Large Language Models,Divergent Creativity in Humans and Large Language Models,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Poetry & Verse,high,"The study's core focus is the measurement of divergent thinking and semantic distance, which are central to lateral thinking, while utilizing specific creative tasks such as flash fiction, movie synopses, and haikus.","**Creative Artifacts Evaluated:**
The research evaluates four distinct text-based artifacts: lists of ten semantically distant nouns, haikus (constrained by 5-7-5 syllable structures), 50-word movie synopses, and 200-word flash fiction narratives.

**Creative Capabilities Assessed:**
The study focuses on divergent thinking, specifically measuring semantic distance and conceptual originality. It assesses the ability to bridge disparate semantic domains (Divergent Semantic Integration) and structural richness (Lempel-Ziv complexity). Subjective dimensions, including overall creativity and narrative quality, are evaluated through Likert scales.

**Evaluation Context:**
This is a text-based comparative study between humans and LLMs. Evaluation is primarily computational, utilizing GLoVe-based cosine similarity for word-level tasks and BERT-based embeddings for sentence-level divergence. Longer-form artifacts employ a mixed-methods approach, combining these automated metrics with subjective ratings from both human judges and GPT-4.

**Task Characteristics:**
Tasks span a spectrum from constrained ideation (generating words using specific linguistic strategies) to constrained creative synthesis (writing within strict word or syllable limits). The framework emphasizes ""divergent"" generation—the ability to maximize conceptual distance—rather than ""convergent"" accuracy, distinguishing it from standard NLP benchmarks focused on logic or factual retrieval.",6c6cb32f026fccc11c98c36913651b992d477a56:Creative Writing Tasks,Creative Writing Tasks,https://github.com/AntoineBellemare/DAT_GPT,1.0,API-Only,3.0,True,False,True,github,openai; anthropic,text
706e82e7d10541e8e495def9dfc23aba461bbf9e,Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning,Improving Linguistic Diversity of Large Language Models with Possibility Exploration Fine-Tuning,2024.0,2024.0,,Dialogue Generation & Social Interaction,Narrative & Story Writing; Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates LLM creativity through conversational dialogue and demographic-rich personas, while also incorporating narrative completion tasks to measure divergent thinking and linguistic diversity.","**Creative Artifacts Evaluated:**
The evaluation focuses on conversational dialogue responses, demographic-rich personas (encompassing age, gender, location, and occupation), and narrative story endings.

**Creative Capabilities Assessed:**
The primary focus is on divergent thinking and linguistic variety. The benchmarks measure semantic diversity (via SBERT similarity), lexical uniqueness (Distinct-n), and the breadth of persona representation (Shannon entropy of attributes). Creative ""usefulness"" is assessed through coherence and logical consistency, ensuring that increased diversity does not result in hallucinations or nonsensical outputs.

**Evaluation Context:**
This is a text-centric LLM evaluation framework. It utilizes a hybrid scoring methodology: traditional n-gram metrics for lexical diversity, embedding-based similarity for semantic breadth, and LLM-as-a-judge (specifically GPT-4o and Llama 3) to evaluate qualitative coherence and perform automated attribute extraction for persona analysis.

**Task Characteristics:**
The research centers on ""one-to-many"" open-ended generation. It prioritizes ""possibility exploration,"" where a single prompt must trigger multiple, semantically distinct outputs. Tasks include unconstrained dialogue generation, persona-driven ideation, and divergent narrative completion. This distinguishes the work from standard benchmarks that typically reward convergence toward a single ""ground truth"" or ""best"" response.",706e82e7d10541e8e495def9dfc23aba461bbf9e:Open-domain Dialogue Generation for Diversity,Open-domain Dialogue Generation for Diversity,https://github.com/mailong25/peft_diversity,2.0,GPU/Local,2.0,False,False,True,github,openai,text
713b604fb9cdd6631074cbd6bf36db029031992e,Large Language Models are Zero Shot Hypothesis Proposers,Large Language Models are Zero Shot Hypothesis Proposers,2023.0,2023.0,,Scientific Discovery,Dialogue Generation & Social Interaction; Functional & Professional Writing,high,"The paper's core focus is the generation and evaluation of biomedical hypotheses (Scientific Discovery), utilizing a multi-agent framework that simulates professional peer-review and collaborative ideation (Dialogue Generation and Professional Writing).","**Creative Artifacts Evaluated:**
The primary artifacts are **biomedical scientific hypotheses**. These are structured textual propositions that suggest novel biological relationships, mechanisms, or therapeutic interventions derived from specific background knowledge extracted from scientific literature.

**Creative Capabilities Assessed:**
The benchmark evaluates **scientific originality (novelty)**, **contextual relevance**, **clinical or theoretical significance**, and **empirical verifiability**. It measures the model's ability to perform divergent thinking within a constrained knowledge domain, moving beyond linguistic fluency to assess conceptual depth and the technical plausibility of proposed ideas.

**Evaluation Context:**
The evaluation is situated in the domain of **automated scientific discovery**. It utilizes a **hybrid scoring framework** that combines traditional NLP metrics (BLEU, ROUGE) with LLM-as-a-judge (ChatGPT/GPT-4) and expert human validation. The context shifts from individual generation to a collaborative environment in the multi-agent task, which simulates a professional scientific peer-review process.

**Task Characteristics:**
Tasks involve **knowledge-intensive open-ended generation** and **collaborative ideation**. The process requires synthesizing complex background data into testable claims. The multi-agent framework introduces **iterative refinement and critical judgment**, where models must not only generate ideas but also evaluate and improve them through specialized roles (e.g., Critic, Scientist), distinguishing this from general-purpose creative writing.",713b604fb9cdd6631074cbd6bf36db029031992e:Background and Hypothesis Pairs (BHP) dataset,Background and Hypothesis Pairs (BHP) dataset,https://github.com/TsinghuaC3I/LLM4BioHypoGen,1.0,API-Only,2.0,True,False,True,github,openai,text
6d6679bd5bf00db7ea3a8305c6db8c8e73a3997a,Jointly Reinforcing Diversity and Quality in Language Model Generations,Jointly Reinforcing Diversity and Quality in Language Model Generations,2025.0,2025.0,,Mathematical Reasoning,Narrative & Story Writing; Lateral Thinking & Creative Problem Solving,high,"The paper evaluates language model performance across elite-level mathematical proofs and creative prose, specifically focusing on the intersection of divergent thinking (diversity) and convergent thinking (technical correctness).","### Domain-Focused Summary

**Creative Artifacts Evaluated:**
The evaluation focuses on text-based outputs, specifically creative writing responses, semantically diverse prose, and formal mathematical proofs or solutions for elite-level competitions (e.g., AIME, Olympiad, HMMT, and BMO 2025).

**Creative Capabilities Assessed:**
The research assesses the intersection of divergent and convergent thinking. It measures originality, ""interestingness,"" and semantic novelty (via n-gram and semantic distinctness) alongside technical correctness and instruction-following. A key focus is the ability to generate multiple valid, distinct solution paths for complex problems, framing solution variety as a proxy for creative exploration.

**Evaluation Context:**
This is a text-centric study employing a hybrid evaluation framework. Qualitative aspects, such as style and creative merit, are judged by advanced LLMs (GPT-4o and Claude 3.7 Sonnet) using win rates and normalized ELO scores. Quantitative performance is measured through automatic verification (Math-Verify library) and statistical diversity metrics.

**Task Characteristics:**
Tasks range from open-ended generation, where semantic variety is prioritized, to highly constrained, competition-level problem-solving. The work bridges the gap between ideation (generating diverse responses) and refinement (ensuring technical accuracy), covering both general instruction-following and specialized mathematical domains.",6d6679bd5bf00db7ea3a8305c6db8c8e73a3997a:ArenaHard v2.0,ArenaHard v2.0,https://github.com/facebookresearch/darling,1.0,API-Only,1.0,True,True,True,huggingface,openai; google,text
7327aa95a364a34f1ab72914f81f8c702a4b7a65,Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification,Towards Multimodal Metaphor Understanding: A Chinese Dataset and Model for Metaphor Mapping Identification,2025.0,2025.0,,Figurative Language & Rhetoric,Functional & Professional Writing; Visual Arts & Stylized Imagery,high,"The paper's core focus is on decoding metaphors and cross-domain conceptual mapping within multimodal advertisements, which directly aligns with the Figurative Language & Rhetoric domain.","**Creative Artifacts Evaluated:**
The benchmark evaluates multimodal Chinese advertisements, specifically the semantic synergy between textual slogans and visual imagery. The generated outputs are conceptual domain labels that identify the ""source"" (the figurative vehicle) and ""target"" (the intended subject or product) of a metaphor.

**Creative Capabilities Assessed:**
The study assesses conceptual depth, figurative reasoning, and cross-domain mapping. It measures the model's ability to decode symbolic intent, distinguishing between literal depictions and the abstract qualities they represent—for example, identifying how a visual of a ""shield"" (source) maps to the concept of ""security"" (target) in a software ad.

**Evaluation Context:**
Testing is conducted in a multimodal, Chinese-specific cultural context. Performance is measured using a hybrid approach: automated metrics (Accuracy and BertScore) evaluate linguistic alignment, while human evaluation (H-E) validates the nuance of metaphorical interpretation. Cohen's Kappa is utilized to ensure the reliability of these subjective human judgments.

**Task Characteristics:**
The tasks involve constrained generation focused on metaphor identification. This requires synthesizing visual and textual cues to perform cross-domain transfer, effectively reverse-engineering the creative process used in advertising to link disparate concepts through shared attributes and symbolic associations.",7327aa95a364a34f1ab72914f81f8c702a4b7a65:CM3D (Chinese Multimodal Metaphor Mapping Dataset),CM3D (Chinese Multimodal Metaphor Mapping Dataset),https://github.com/DUTIR-YSQ/MultiMM,2.0,GPU/Local,2.0,False,False,True,github,openai,multimodal
72faa912691bc2d8dc342fc1c9b53cccc9ba56fc,LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context,LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation and evaluation of novel scientific hypotheses and research ideas, while explicitly utilizing divergent thinking metrics such as fluency and flexibility typically associated with lateral thinking.","**Creative Artifacts Evaluated:**
The benchmark evaluates textual scientific research ideas and hypotheses. These artifacts are generated as conceptual proposals derived from a broad vocabulary of 1,180 specific scientific keywords across diverse disciplines, such as Partial Differential Equations (PDEs).

**Creative Capabilities Assessed:**
The primary focus is on divergent thinking within a scientific context. Capabilities are measured across five dimensions: originality (novelty of the concept), feasibility (scientific viability), clarity (coherence of expression), fluency (the ability to produce a high volume of distinct ideas), and flexibility (the breadth of different conceptual categories explored).

**Evaluation Context:**
This is a large-scale text-based evaluation involving over 40 LLMs. The benchmark employs a ""minimal context"" paradigm, providing only a single keyword to trigger generation. Scoring is conducted via a panel of LLM judges using a structured rubric, which was validated through Pearson correlation against ratings from human experts in specialized scientific fields.

**Task Characteristics:**
The core task is open-ended, zero-shot scientific ideation. Unlike benchmarks that provide detailed prompts or constraints, this task requires the model to autonomously expand a single seed term into multiple research directions. It emphasizes the generative phase of the creative process rather than iterative refinement or convergent problem-solving, specifically testing the model's internal knowledge-base and associative breadth.",72faa912691bc2d8dc342fc1c9b53cccc9ba56fc:LiveIdeaBench,LiveIdeaBench,https://huggingface.co/datasets/6cf/liveideabench-v2,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic; google,text
7354b87a1b4c99ccd9cf25b7314927ced8b156f7,IGA: An Intent-Guided Authoring Assistant,IGA: An Intent-Guided Authoring Assistant,2021.0,2021.0,,Narrative & Story Writing,Figurative Language & Rhetoric; Functional & Professional Writing,high,"The paper focuses on an AI assistant for authoring short stories and prose, evaluating narrative coherence alongside specific rhetorical intents, idiomatic expressions, and functional stylistic constraints.","**Creative Artifacts Evaluated:**
Short stories and rhetorically specific prose segments, including causal links, descriptive passages, biographical snippets, and idiomatic expressions, alongside stylistically embellished paraphrases.

**Creative Capabilities Assessed:**
Assesses rhetorical adequacy (adherence to specific functional intents), stylistic quality, linguistic fluency, and narrative coherence. It further evaluates collaborative utility by measuring user interaction patterns, such as token retention/deletion and subjective satisfaction with AI suggestions.

**Evaluation Context:**
A text-based LLM framework utilizing a hybrid evaluation strategy. This includes automatic linguistic metrics (ROUGE, BLEURT, self-BLEU), human qualitative ratings on 3-point and Likert scales, and quantitative user-study logs capturing real-time editing behavior during the creative process.

**Task Characteristics:**
Combines constrained generation (rhetorical infilling) and text refinement (embellishing paraphrases) with interactive, open-ended story writing. The work distinguishes itself by focusing on intent-guided authoring, where the AI serves as a steerable assistant for specific rhetorical goals rather than an autonomous creator.",7354b87a1b4c99ccd9cf25b7314927ced8b156f7:IGA Intent-Guided Generation Evaluation,IGA Intent-Guided Generation Evaluation,https://github.com/SimengSun/IGA-An-Intent-Guided-Authoring-Assistant,2.0,GPU/Local,2.0,False,False,True,github,none,text
760527bc3b5d9d4015b2d7ee72f900ff7a4c6927,How do Humans and Language Models Reason About Creativity? A Comparative Analysis,How do Humans and Language Models Reason About Creativity? A Comparative Analysis,2025.0,2025.0,,Scientific Discovery,Engineering & Technical Design; Lateral Thinking & Creative Problem Solving,high,"The paper evaluates reasoning about creativity within STEM and engineering design challenges, focusing on technical feasibility and originality in scientific problem-solving.","**Creative Artifacts Evaluated:**
The benchmark focuses on written solutions to real-world STEM design challenges. These artifacts are technical, open-ended proposals that address specific engineering or scientific problem statements, distinguishing them from purely artistic or literary creative outputs.

**Creative Capabilities Assessed:**
The study evaluates creative judgment and reasoning rather than raw generative output. Specifically, it assesses the ability to rate solutions across four fine-grained facets, including originality, using 5-point Likert scales. It also measures the capacity to generate coherent justifications for these ratings and the presence of specific linguistic markers within those explanations.

**Evaluation Context:**
This is a text-based comparative study between LLMs and human experts. The evaluation is meta-analytical, using Pearson correlation and Cohen’s Kappa to measure alignment between LLM ratings and expert ground truth. It further employs statistical tests (Mann-Whitney U and Cramer’s V) to compare the linguistic distribution of reasoning patterns between humans and models.

**Task Characteristics:**
The tasks are primarily evaluative and meta-cognitive. They include fine-grained judgment (scoring), constrained explanation generation (justifying scores), and linguistic classification (analyzing the structure of reasoning). This framework shifts the focus from ""creating"" to ""reasoning about creativity,"" specifically within the constrained, objective-oriented domain of STEM problem-solving.",760527bc3b5d9d4015b2d7ee72f900ff7a4c6927:Design Problems Task (DPT),Design Problems Task (DPT),https://github.com/Beaty-Lab/CogSci-2025-Scientific-Creativity,1.0,API-Only,3.0,True,False,True,github,openai; anthropic; google,text
777683db4795ff691533c2c4be3244fabd826842,MiQA: A Benchmark for Inference on Metaphorical Questions,MiQA: A Benchmark for Inference on Metaphorical Questions,2022.0,2022.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper focuses explicitly on metaphorical inference and cross-domain mapping, which are central to figurative language, while also requiring the analogical reasoning characteristic of lateral thinking.","**Creative Artifacts Evaluated**
Conventional metaphorical sentences and their corresponding literal semantic paraphrases. The artifacts consist of natural language premises that utilize figurative language to describe abstract concepts, alongside generated textual explanations that bridge the gap between metaphorical and literal domains.

**Creative Capabilities Assessed**
Conceptual depth and figurative fluency. The benchmark measures the capacity for cross-domain mapping—the ability to transfer logic from a source domain to a target domain. It specifically assesses non-literal reasoning, semantic consistency, and the ability to resolve linguistic ambiguity in creative language that deviates from standard literal interpretations.

**Evaluation Context**
A text-based evaluation of Large Language Models (LLMs). The methodology employs a hybrid approach: automated discriminative scoring based on the log-likelihood of binary choices (implies/implied-by) and human-centric evaluation for generative tasks, where experts rate the accuracy and clarity of open-ended inferences using inter-rater reliability metrics.

**Task Characteristics**
The benchmark utilizes a mix of constrained discriminative tasks and open-ended generation. These tasks require complex inference-based reasoning and cross-domain transfer, forcing models to navigate the tension between literal syntax and figurative intent. This involves both evaluative judgment (selecting correct implications) and ideation (generating literal explanations for metaphorical premises).",777683db4795ff691533c2c4be3244fabd826842:MiQA (Metaphorical Inference Questions and Answers),MiQA (Metaphorical Inference Questions and Answers),https://github.com/google-research/language/tree/master/language/miqa,1.0,API-Only,3.0,True,False,True,github,none,text
6d4def9e60c21ccf6e7620f619cd621b50363c5b,VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness,VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness,2025.0,2025.0,,Performing Arts & Video Synthesis,Narrative & Story Writing; Engineering & Technical Design,high,"The paper presents a benchmark for video generation focusing on temporal consistency and motion (Performing Arts & Video Synthesis), while explicitly evaluating multi-scene narrative logic (Narrative & Story Writing) and the simulation of physical laws like mechanics and thermotics (Engineering & Technical Design).","**Creative Artifacts Evaluated:**
High-fidelity video sequences encompassing photorealistic human figures (anatomy, identity, and clothing), multi-scene narrative plots, and complex landscape transitions. The benchmark specifically evaluates simulated physical phenomena, including thermal state changes (vaporization, liquefaction) and material interactions such as combustion, hardness, and color mixing.

**Creative Capabilities Assessed:**
The suite measures technical correctness through physical realism (mechanics, thermotics) and anatomical structural integrity. It evaluates narrative fluency and conceptual depth via multi-stage plot construction and motion ordering. Furthermore, it assesses divergent thinking through inter-sample diversity and the ability to synthesize novel, uncommon compositions, such as unique species combinations.

**Evaluation Context:**
This is a multimodal benchmark designed for diffusion-based video models. It employs a sophisticated hybrid scoring pipeline: automatic specialist detectors (ViT, ArcFace, SIFT/RAFT) are used for structural and temporal consistency, while advanced VLM-based judges (LLaVA-Video, Qwen2.5-VL) provide qualitative assessments of high-level semantic alignment, physical rationality, and narrative logic.

**Task Characteristics:**
Tasks focus on constrained and open-ended generation. They range from fine-grained attribute manipulation and camera motion control to complex world-modeling challenges. Models must demonstrate ""intrinsic faithfulness"" by maintaining instance preservation, 3D geometric consistency, and logical real-world consequences across temporal dimensions.",6d4def9e60c21ccf6e7620f619cd621b50363c5b:VBench-2.0,VBench-2.0,https://github.com/Vchitect/VBench,2.0,GPU/Local,18.0,False,True,True,huggingface,openai,video
7455c15acdea4beafa9fc50f3f4225cf10b4f971,VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning,VideoCanvas: Unified Video Completion from Arbitrary Spatiotemporal Patches via In-Context Conditioning,2025.0,2025.0,,Performing Arts & Video Synthesis,Visual Arts & Stylized Imagery,high,"The paper focuses on generating temporally consistent video sequences and cinematic transitions, which aligns with the Performing Arts & Video Synthesis domain, while also addressing aesthetic imaging quality and spatial completion tasks relevant to Visual Arts.","**Creative Artifacts Evaluated**
The model generates temporally consistent video sequences, including photorealistic scenes, seamless transitions between non-homologous video clips, and spatially or temporally extended video frames (inpainting and outpainting). These artifacts are synthesized from sparse, arbitrary spatiotemporal ""anchors,"" such as isolated patches or full frames placed at specific timestamps.

**Creative Capabilities Assessed**
The benchmark evaluates aesthetic and imaging quality, temporal coherence, and the ""dynamic degree"" of motion. It specifically assesses the model’s capacity for divergent generation—creating a full video from a single patch—and its ability to maintain semantic and visual continuity. The ""AnyV2V"" tasks further measure logical storytelling through the creation of plausible transitions between disparate visual concepts.

**Evaluation Context**
Evaluation is situated within video diffusion research, utilizing a multimodal framework (text-to-video with pixel-level conditioning). It employs a mixed-methods scoring system that combines technical fidelity metrics (PSNR, FVD) with subjective human user studies focusing on visual quality, semantic alignment, and overall preference win rates.

**Task Characteristics**
Tasks represent highly constrained generation where the model must respect arbitrary spatiotemporal prompts while performing open-ended completion. This includes spatial extrapolation (outpainting), refinement (inpainting), and cross-domain synthesis (transitions), shifting the focus from simple prompt-following to complex, context-aware video manipulation and ""in-context"" conditioning.",7455c15acdea4beafa9fc50f3f4225cf10b4f971:VideoCanvasBench,VideoCanvasBench,https://github.com/KwaiVGI/VideoCanvas,2.0,GPU/Local,6.0,False,False,True,unknown,none,video
77a4a2791680d7651e11199b88e115361cb28c3f,VLMgineer: Vision Language Models as Robotic Toolsmiths,VLMgineer: Vision Language Models as Robotic Toolsmiths,2025.0,2025.0,,Engineering & Technical Design,"Lateral Thinking & Creative Problem Solving; 3D, Spatial & Architectural Design; Programming & Algorithmic Creativity",high,"The paper focuses on the generation of functional mechanical tools and robotic trajectories to solve physical tasks, emphasizing technical utility, spatial reasoning, and adherence to physical laws.","**Creative Artifacts Evaluated:**
Functional 3D tool geometries and corresponding robotic control trajectories. These artifacts comprise specialized mechanical designs—such as hooks, scoops, and extensions—represented as executable code or geometric parameters designed to augment a robotic end-effector's physical capabilities.

**Creative Capabilities Assessed:**
Functional creativity, physical reasoning, and mechanical utility. The benchmark evaluates the model’s capacity for spatial problem-solving and divergent thinking within the constraints of Newtonian physics. Success is measured by the tool's effectiveness in achieving specific goals (usefulness) and the efficiency of the resulting manipulation (technical correctness), specifically focusing on the model's ability to bridge the gap between visual perception and physical execution.

**Evaluation Context:**
A multimodal simulation framework (ROBOTOOLBENCH) where Vision Language Models (VLMs) interpret visual scenes to generate designs. Evaluation is entirely automatic, utilizing task-specific normalized rewards (0–1) derived from objective physical state changes—such as object displacement, elevation, or containment—and motion efficiency (total distance traversed).

**Task Characteristics:**
Constrained functional generation and physical problem-solving. Tasks require the model to navigate complex environmental constraints—such as narrow openings (DislodgeCube), fragile arrangements (OneBook), or out-of-reach targets (ScoreGoal)—necessitating the translation of visual-spatial understanding into novel, task-appropriate mechanical solutions.",77a4a2791680d7651e11199b88e115361cb28c3f:ROBOTOOLBENCH,ROBOTOOLBENCH,https://vlmgineer.github.io/,2.0,GPU/Local,12.0,False,False,True,unknown,openai,multimodal
78a4ecbb479be5e0ae6f5c081718506ce2f8ed98,"TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models","TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models",2025.0,2025.0,,Narrative & Story Writing,Functional & Professional Writing,high,"The paper centers on the generation of synthetic moral fables, which are narrative-driven prose, while also emphasizing pedagogical design through age-specific adaptation and didactic moral lessons.","**Creative Artifacts Evaluated:**
The primary artifacts are **short-form synthetic moral fables**. These are narrative-driven texts featuring anthropomorphic or human characters, structured plots, and explicit or implicit didactic lessons (morals).

**Creative Capabilities Assessed:**
The framework measures **originality (Creativity)**, **thematic coherence (Moral Clarity)**, and **stylistic fluency (Grammar & Style)**. It uniquely assesses **audience-specific adaptation**—the ability to modulate complexity for five distinct age brackets (0–18+ years)—and **instructional precision (Prompt Adherence)**. At the corpus level, it evaluates **lexical diversity** and **structural variety** through metrics like Distinct-1 and Self-BLEU.

**Evaluation Context:**
This is a **text-based** hybrid evaluation environment. It utilizes **LLM-as-judge** scoring based on a multi-dimensional rubric (1–10 scale) alongside **automatic reference-free metrics** (e.g., Flesch Reading Ease). The context is specifically designed to benchmark the creative output of **small, open language models (SLMs)** trained on massive synthetic datasets.

**Task Characteristics:**
Tasks center on **constrained open-ended generation**, requiring the synthesis of narrative elements with specific moral themes. The framework also includes **evaluative classification** (mapping fables to target age groups) and **diversity analysis**, distinguishing this work by its focus on the ""creative utility"" and moral alignment of large-scale synthetic corpora rather than simple factual accuracy.",78a4ecbb479be5e0ae6f5c081718506ce2f8ed98:Fable Generation Evaluation Framework,Fable Generation Evaluation Framework,https://huggingface.co/datasets/klusai/ds-tf1-en-3m,1.0,API-Only,3.0,True,True,True,huggingface,openai; huggingface,text
79914b8f022a2e4b522d890cbaf001842a116df0,Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest,Do Androids Laugh at Electric Sheep? Humor “Understanding” Benchmarks from The New Yorker Caption Contest,2022.0,2022.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving; Figurative Language & Rhetoric; Visual Arts & Stylized Imagery,high,"The paper evaluates the understanding and generation of humor, wit, and irony within the specific context of multimodal cartoon captions, requiring incongruity resolution and cross-domain semantic synthesis.","**Creative Artifacts Evaluated:**
Single-panel line-drawing cartoons and short-form humorous captions. The focus is on the specific visual-linguistic pairing where humor emerges from the interaction between an image and its text.

**Creative Capabilities Assessed:**
Humor recognition, incongruity resolution, and conceptual depth. The benchmarks measure a model’s ability to identify wit, distinguish high-quality humor from mediocre attempts (quality ranking), and provide logical justifications for why a specific pairing is funny. This requires social-cultural awareness and the ability to process irony and satire.

**Evaluation Context:**
A multimodal (image-text) framework comparing AI performance against expert-curated (New Yorker editors) and crowd-sourced benchmarks. Evaluation employs automatic accuracy for discriminative tasks and a mix of linguistic metrics (BLEU, ROUGE, Perplexity) and human preference for generative explanations.

**Task Characteristics:**
The suite combines discriminative evaluation (matching and ranking) with open-ended reasoning (explanation generation). These tasks necessitate high-level semantic synthesis to bridge the gap between visual anomalies and linguistic punchlines, moving beyond simple object recognition to complex, abstract interpretation of creative intent and cross-modal humor.",79914b8f022a2e4b522d890cbaf001842a116df0:New Yorker Cartoon Caption Contest Humor 'Understanding' Benchmarks,New Yorker Cartoon Caption Contest Humor 'Understanding' Benchmarks,https://huggingface.co/datasets/jmhessel/newyorker_caption_contest,1.0,API-Only,3.0,True,True,True,huggingface,openai; anthropic; google; huggingface,multimodal
78b1246d16ff17b3481a0057abf36ddad5e91c83,Word2Minecraft: Generating 3D Game Levels through Large Language Models,Word2Minecraft: Generating 3D Game Levels through Large Language Models,2025.0,2025.0,,"3D, Spatial & Architectural Design",Narrative & Story Writing,high,"The paper focuses on generating functional 3D game environments from narrative descriptions, emphasizing spatial reasoning and structural coherence alongside narrative alignment.","**Creative Artifacts Evaluated:** 
Playable 3D Minecraft levels, top-down map layouts, objective-based sub-maps, and gameplay videos demonstrating level navigation and environmental interaction.

**Creative Capabilities Assessed:** 
Narrative-spatial coherence (aligning story beats with physical environments), environmental diversity (tile variety and distribution), and playability (navigational functionality). The system measures the ability to translate abstract narrative concepts into functional 3D structures, assessing aesthetic quality alongside structural complexity, challenge, and the logical placement of objectives.

**Evaluation Context:** 
A complex multimodal framework utilizing LLM-based coherence scoring (text-embedding-3-large), algorithmic metrics (Shannon entropy, shortest-path analysis), and human-in-the-loop assessments (preference rankings and gameplay observation). It bridges text, 3D voxel data, and video modalities to evaluate the success of the creative generation process.

**Task Characteristics:** 
Cross-domain transfer (narrative-to-spatial), constrained generation (balancing creative expression with functional walkability), and multi-dimensional evaluation of open-ended creative outputs. The work distinguishes itself by integrating narrative alignment with rigorous functional playability constraints in a 3D environment, moving beyond simple asset generation to holistic world-building.",78b1246d16ff17b3481a0057abf36ddad5e91c83:Word2Minecraft,Word2Minecraft,https://github.com/JMZ-kk/Word2Minecraft,3.0,Special,7.0,False,False,True,github,openai,multimodal
7a4f042ce29809ef9c2ddde9726b48c5a84feb5e,Salsa as a Nonverbal Embodied Language - The CoMPAS3D Dataset and Benchmarks,Salsa as a Nonverbal Embodied Language - The CoMPAS3D Dataset and Benchmarks,2025.0,2025.0,,Performing Arts & Video Synthesis,"Dialogue Generation & Social Interaction; Music & Auditory Arts; 3D, Spatial & Architectural Design",high,"The paper focuses on generating 3D salsa dance sequences, emphasizing choreography and nonverbal expression (Performing Arts), while treating duets as a communicative social exchange (Dialogue) that must synchronize with musical rhythms (Music) using 3D kinematic data (3D/Spatial Design).","**Creative Artifacts Evaluated:**
The primary artifacts are 3D motion sequences representing improvised salsa dancing. These include solo performances (leader or follower) and interactive duets where motion is conditioned on a partner's movements and musical cues. The data captures expressive, nonverbal communication through kinematic and graphical representations.

**Creative Capabilities Assessed:**
The benchmarks measure rhythmic fluency (beat alignment), stylistic originality (diversity), and technical fidelity (kinematic and graphical FID). In duet tasks, the assessment extends to interactive intelligence—specifically the ability to maintain spatial and temporal coordination (cross-distance metrics) while responding to nonverbal cues, reflecting the ""dialogue"" inherent in social dance.

**Evaluation Context:**
This is a multimodal domain involving audio-to-motion and motion-to-motion generation. Evaluation is primarily automatic, utilizing kinematic and graphical Fréchet Inception Distance (FID) to assess realism, alongside specialized metrics like Beat Echo Degree (BED) to quantify interpersonal synchronization and rhythmic response.

**Task Characteristics:**
The work encompasses open-ended creative generation (solo), constrained interactive generation (duet), and the semantic understanding of embodied language through move classification. It distinguishes itself by treating dance not merely as aesthetic movement, but as a complex, communicative, and reactive nonverbal language.",7a4f042ce29809ef9c2ddde9726b48c5a84feb5e:CoMPAS3D,CoMPAS3D,https://huggingface.co/datasets/Rosie-Lab/compas3d,2.0,GPU/Local,3.0,False,True,True,huggingface,none,multimodal
79d455c55401734971aba42a76204f973980ed04,LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models,LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models,2025.0,2025.0,,Scientific Discovery,Mathematical Reasoning,high,"The paper focuses on discovering mathematical equations that represent scientific laws across various STEM fields, combining scientific hypothesis generation with symbolic mathematical reasoning.","**Creative Artifacts Evaluated:**
The benchmark evaluates symbolic mathematical equations representing scientific laws. These include transformed versions of classical physics formulas (e.g., Feynman equations) and novel synthetic equations spanning four distinct scientific domains: chemistry, biology, medicine, and physics.

**Creative Capabilities Assessed:**
The primary capability assessed is scientific hypothesis generation—specifically, the ability to discover underlying mathematical structures from numerical data. The benchmark measures technical correctness through symbolic accuracy and predictive utility via numeric precision (NMSE and Acc0.1). It evaluates the models' capacity for divergent thinking within a scientific framework, requiring them to identify parsimonious yet accurate representations of complex phenomena.

**Evaluation Context:**
Evaluation is conducted using text-based LLMs (e.g., GPT-4o, Claude 3.5 Sonnet) in a zero-shot or few-shot discovery context. It employs a hybrid scoring approach: automated numeric metrics for data fitting and an LLM-as-a-judge (GPT-4o) to determine symbolic equivalence between generated and ground-truth equations.

**Task Characteristics:**
Tasks involve constrained generation (fitting equations to specific data points) and open-ended discovery. The benchmark emphasizes symbolic regression as a creative problem-solving task, testing the models' ability to handle symbolic transformations and generalize across diverse scientific domains, moving beyond simple memorization of established physics benchmarks.",79d455c55401734971aba42a76204f973980ed04:LLM-SRBench,LLM-SRBench,https://github.com/deep-symbolic-mathematics/llm-srbench,2.0,GPU/Local,2.0,False,True,True,huggingface,openai; huggingface,text
7d4676c00282c04da5df5d5ad2273a42004021be,Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem Translation,Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem Translation,2024.0,2024.0,,Poetry & Verse,Figurative Language & Rhetoric,high,"The paper focuses on the generation and evaluation of Vietnamese poetry based on rigid metrical, tonal, and rhyme constraints such as the bang-trac system and specific syllable counts.","**Creative Artifacts Evaluated:**
The primary artifacts are Vietnamese poems across multiple traditional genres, specifically *luc bat* (6-8 syllable couplets), *song that luc bat*, and *duong luat*. These artifacts are defined by rigid linguistic structures and complex prosodic requirements.

**Creative Capabilities Assessed:**
The benchmarks measure technical proficiency in formal poetic structures, specifically syllable count precision, rhyme scheme adherence, and tonal harmony (the *bang-trac* system). It also assesses the model's ability to maintain thematic consistency and fluency when transforming non-poetic paraphrases into structured verse or generating original content from natural language prompts.

**Evaluation Context:**
This research operates in a text-based LLM generation domain. Evaluation is uniquely automated through a custom, rule-based scoring engine that quantifies prosodic accuracy. A BERT-based classifier is integrated to facilitate ""blind"" testing, identifying the genre of generated text before applying the corresponding structural scoring rules.

**Task Characteristics:**
The tasks involve highly constrained generation and cross-format refinement. These include text-to-poem generation (thematic prompts), poem-to-poem generation (re-versification of paraphrased text), and ""blind"" generation where the model must autonomously select and adhere to a specific poetic framework without explicit genre instructions.",7d4676c00282c04da5df5d5ad2273a42004021be:Vietnamese Poem Quality Scoring System,Vietnamese Poem Quality Scoring System,https://github.com/Anshler/poem_generator,2.0,GPU/Local,3.0,False,False,True,github,openai; huggingface,text
7cdebb73662387d9040da4f27a7dc04dbffa3c3e,ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models,ByGPT5: End-to-End Style-conditioned Poetry Generation with Token-free Language Models,2022.0,2022.0,,Poetry & Verse,,high,"The paper specifically addresses the generation of English quatrains governed by rigid formal constraints such as rhyme schemes, rhythmic meters, and phonetic alliteration.","**Creative Artifacts Evaluated:**
The primary artifacts are English quatrains (four-line stanzas) generated under specific stylistic and thematic constraints. These include poems conditioned on formal structures (rhyme schemes, rhythmic meters, and alliteration levels) and specific emotional prompts.

**Creative Capabilities Assessed:**
The framework measures technical adherence to formal poetic constraints (rhyme, meter, and phonetic alliteration) and the ability to convey specific emotional sentiments. It further assesses linguistic fluency, thematic coherence, and ""human-likeness."" Originality is explicitly tested through extractive memorization metrics to ensure the model is not merely reproducing training data.

**Evaluation Context:**
This text-based evaluation compares a token-free, byte-level model (ByGPT5) against subword-based models like ChatGPT. It employs a hybrid scoring methodology: automatic classifier-based metrics provide objective data on structural precision, while human annotators use Best-Worst Scaling (BWS) to judge subjective aesthetic quality and stylistic success.

**Task Characteristics:**
Tasks involve highly constrained, multi-objective generation. The model must simultaneously satisfy rigid formal parameters (e.g., AABB rhyme, iambic pentameter) and open-ended thematic or emotional requirements. This emphasizes character-level control and phonetic awareness, distinguishing it from standard semantic text generation by focusing on the ""sound"" and structural architecture of the poetry.",7cdebb73662387d9040da4f27a7dc04dbffa3c3e:Style-conditioned Quatrain Generation Evaluation,Style-conditioned Quatrain Generation Evaluation,https://github.com/potamides/uniformers,3.0,Special,5.0,False,False,True,github,openai; huggingface,text
7c6729bd4b2b3bdf013482b499c7401ab6de44ef,ProteinBench: A Holistic Evaluation of Protein Foundation Models,ProteinBench: A Holistic Evaluation of Protein Foundation Models,2024.0,2024.0,,Scientific Discovery,Engineering & Technical Design,high,"The paper evaluates the generation of novel, biophysically plausible protein sequences and 3D structures, which is a core task in biological scientific research and protein engineering.","**Creative Artifacts Evaluated:**
The benchmark evaluates amino acid sequences, 3D protein backbone folds, integrated sequence-structure pairs, functional protein scaffolds, and antibody CDR-H3 loops. It also assesses dynamic ensembles and conformational distributions of protein structures.

**Creative Capabilities Assessed:**
The primary focus is on technical correctness (biophysical plausibility, structural stability, and sequence naturalness) and functional utility (motif preservation, binding energy, and specificity). Creativity is specifically quantified through structural novelty (distance from known proteins in the Protein Data Bank) and diversity (the ability to generate a wide variety of unique folds without mode collapse). It measures the model's ability to navigate the ""fitness landscape"" to find novel yet functional biological solutions.

**Evaluation Context:**
Evaluation is entirely automatic, utilizing specialized biophysical metrics and ""in silico"" validation. It employs ""refoldability"" checks—using secondary models like AlphaFold2 to verify if a generated sequence actually adopts the intended structure—alongside geometric similarity measures (TM-score, RMSD). There is no human or LLM-based subjective assessment, as success is defined by physical realizability.

**Task Characteristics:**
Tasks range from constrained generation (designing sequences for fixed structures or scaffolding specific functional motifs) to open-ended de novo generation (backbone and sequence design). It also includes predictive problem-solving (folding) and distributional modeling, where the model must capture the inherent flexibility and multiple states of biological molecules.",,,,,,,,,,,,
7d9d41d90f13d75467ed1bd1c322d95449cd421e,Calligrapher: Freestyle Text Image Customization,Calligrapher: Freestyle Text Image Customization,2025.0,2025.0,,Graphic Design & Visual Layout,Visual Arts & Stylized Imagery,high,"The paper specifically addresses artistic typography and the integration of graphic design elements into letterforms, focusing on the synergy between stylistic motifs and textual legibility.","**Creative Artifacts Evaluated:**
The benchmark evaluates artistic typography and stylized text images. These artifacts consist of specific textual strings rendered with customized visual styles, textures, and artistic motifs, ranging from calligraphic effects to complex graphic design elements integrated into the letterforms.

**Creative Capabilities Assessed:**
The assessment focuses on the dual capability of stylistic fidelity and functional legibility. It measures ""Style Sync"" (the ability to replicate a target aesthetic) and ""Text Matching"" (content accuracy). Technical capabilities include OCR accuracy (legibility), aesthetic quality, and the model's ability to maintain structural character integrity while performing divergent visual transformations.

**Evaluation Context:**
This research operates within the image-generation domain, specifically focusing on diffusion models. The evaluation is a hybrid framework combining automated computer vision metrics—FID for distributional quality, and CLIP/DINO for semantic and stylistic similarity—with human user studies. Human graders provide subjective ratings on a 1-4 scale for aesthetics and style synchronization, alongside overall preference percentages.

**Task Characteristics:**
The tasks represent constrained generation, where the model must adhere to strict textual inputs while executing open-ended stylistic customization. This requires high-precision spatial control and the synthesis of linguistic symbols with diverse artistic textures, distinguishing it from general-purpose text-to-image synthesis by emphasizing the intersection of graphic design and character preservation.",7d9d41d90f13d75467ed1bd1c322d95449cd421e:style-centric text customization benchmark,style-centric text customization benchmark,https://huggingface.co/Calligrapher2025/Calligrapher,2.0,GPU/Local,1.0,False,True,True,huggingface,huggingface,image
7edc82ab56792e72ded0cd4204f9a72fdce5a92f,I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt Generation for Text-Guided Multi-Mask Inpainting,I Dream My Painting: Connecting MLLMs and Diffusion Models via Prompt Generation for Text-Guided Multi-Mask Inpainting,2024.0,2024.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on the synthesis and evaluation of artistic and photorealistic images through text-guided inpainting, emphasizing aesthetic quality, style, and visual coherence.","**Creative Artifacts Evaluated:**
The research evaluates artistic paintings across diverse historical styles (WikiArt/ArtGraph) and photorealistic images (DCI). Specifically, it focuses on localized text prompts describing masked regions and the resulting inpainted visual segments integrated into existing compositions.

**Creative Capabilities Assessed:**
The assessment targets contextual ideation and visual synthesis. It measures the model's ability to generate semantically relevant and linguistically diverse prompts (Distinct-N, Self-BLEU) and the technical capacity to produce visually coherent, high-quality image completions. Key metrics include aesthetic quality (CLIP-IQA), structural fidelity (PSNR, LPIPS), and cross-modal alignment (CLIPSim) between the generated text and the synthesized image.

**Evaluation Context:**
The study employs a multimodal pipeline connecting MLLMs (LLaVA) for prompt generation with Diffusion models (Stable Diffusion) for image synthesis. Evaluation is conducted through a hybrid of automatic computer vision metrics and NLP-based linguistic analysis, testing the system's performance on both abstract artistic and concrete photographic datasets to ensure generalizability across creative domains.

**Task Characteristics:**
The core task is constrained, multi-modal generation involving simultaneous multi-mask inpainting. This requires spatial reasoning and localized refinement, where the model performs ""creative filling"" by imagining and executing multiple distinct edits within a single visual frame, bridging the gap between descriptive ideation and generative execution.",7edc82ab56792e72ded0cd4204f9a72fdce5a92f:WikiArt (ArtGraph),WikiArt (ArtGraph),https://github.com/cilabuniba/i-dream-my-painting,2.0,GPU/Local,2.0,False,False,True,github,none,multimodal
7f711911c4cf3180d6a1fed0ced88a2749b0b7a5,Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems,Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems,2025.0,2025.0,,Mathematical Reasoning,Lateral Thinking & Creative Problem Solving,high,The paper focuses on generating novel mathematical proofs and derivations while explicitly measuring divergent thinking and the ability to bypass conventional solution paths.,"**Creative Artifacts Evaluated:**
Alternative mathematical proofs and step-by-step problem-solving derivations for established mathematical problems. These artifacts are structured as logical, text-based sequences of symbolic reasoning and natural language explanations that must reach a correct conclusion via a unique path.

**Creative Capabilities Assessed:**
The benchmark measures technical correctness (mathematical validity) and originality (divergence from existing methods). It specifically evaluates divergent thinking by requiring models to bypass conventional ""known"" solutions. A key metric is the ""Novel-Unknown Ratio,"" which identifies solutions that are both correct and distinct from any documented approaches provided in the prompt or likely encountered during training.

**Evaluation Context:**
This is a text-based symbolic reasoning domain. Evaluation is conducted via a multi-model LLM panel (GPT-4, Claude 3.5 Sonnet, and Gemini 1.5 Pro). To ensure high rigor in a technical field, correctness requires a unanimous consensus among the evaluators, while novelty is determined by majority vote, simulating a peer-review process for complex technical outputs.

**Task Characteristics:**
The benchmark utilizes constrained open-ended generation, where models must solve problems while explicitly avoiding $k$ provided ""known"" solutions. It also includes a meta-evaluation task—pairwise similarity analysis—to quantify the diversity of creative outputs across different model architectures, distinguishing this work from standard accuracy-focused mathematical benchmarks.",,,,,,,,,,,,
81aa8d0c94bbf8b928f9e946140b3af2bcddec17,Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning,Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning,2025.0,2025.0,,Graphic Design & Visual Layout,Functional & Professional Writing,high,"The paper focuses on evaluating and selecting advertising creative images based on their visual composition and marketing effectiveness, which aligns with graphic design principles and professional marketing goals.","**Creative Artifacts Evaluated:**
The research focuses on **advertising creative images** paired with **product titles** and **user-intent queries**. These artifacts are commercial visual designs specifically engineered to elicit consumer engagement and drive marketing conversions.

**Creative Capabilities Assessed:**
The benchmarks measure **marketing effectiveness** (utility) and **comparative reasoning**. Beyond simple selection, the model is assessed on its ability to provide **explainable judgments**, articulating why one visual composition is superior to another for a specific audience. It evaluates the model’s **aesthetic and functional alignment** with user needs, prioritizing high-performing assets that influence consumer behavior.

**Evaluation Context:**
This is a **multimodal** evaluation framework utilizing **Vision-Language Models (VLMs)**. It employs a hybrid scoring system: **automatic metrics** derived from real-world A/B testing (Click-Through Rate, Conversion Rate, and Revenue Per Mille), **selection accuracy** against historical performance data, and **LLM-based evaluation** (Qwen-Score) to judge the logical quality of natural language explanations.

**Task Characteristics:**
The work shifts from creative generation to **evaluative judgment** and **preference modeling**. Tasks include **constrained selection** (identifying the top-10 most effective images from a candidate pool) and **explainable creative selection**, where the model must perform comparative analysis to justify its choices in a commercial context.",81aa8d0c94bbf8b928f9e946140b3af2bcddec17:CreativePair,CreativePair,https://pan.quark.cn/s/8fc8ec3e74f3,2.0,GPU/Local,1.0,False,False,True,url,openai; none,multimodal
7ec9a50b9a91b2d0ec60a335396d0d2daedb5c6b,A Causality-Aware Paradigm for Evaluating Creativity of Multimodal Large Language Models,A Causality-Aware Paradigm for Evaluating Creativity of Multimodal Large Language Models,2025.0,2025.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on evaluating 'Oogiri-style' humorous captions and witty punchlines, specifically measuring divergent thinking and the resolution of visual-textual incongruities through an iterative creative process.","**Creative Artifacts Evaluated:**
The primary artifacts are multimodal humorous responses, specifically ""Oogiri-style"" captions and witty textual punchlines generated in response to incongruous image-text prompts. These include creative dialogue completions and short-form humorous interpretations of visual scenes.

**Creative Capabilities Assessed:**
The paradigm measures humorous originality, divergent thinking, and the ability to resolve visual-textual semiotic gaps. It specifically assesses ""creative effort"" through the number of iterations required to reach a quality threshold, as well as the model’s evaluative judgment (the ability to discriminate and rank creative quality in alignment with human aesthetics).

**Evaluation Context:**
Evaluations are conducted in a multimodal (image-text) setting focusing on Multimodal Large Language Models (MLLMs). The context utilizes a ""causality-aware"" framework to mitigate information leakage, employing a hybrid scoring model: automatic metrics (Accuracy, NDCG) for discriminative tasks, causal LLM-as-a-judge for interactive generation, and human preference voting as a gold standard.

**Task Characteristics:**
Tasks include discriminative selection (m-choose-n creative options), ranking of candidate responses, and constrained interactive generation (filling masked key text). The framework emphasizes the process of refinement and iterative ideation over single-turn output, distinguishing it from static benchmarks by measuring the ""Length of Thought"" (LoT) required for creative success.",7ec9a50b9a91b2d0ec60a335396d0d2daedb5c6b:Oogiri-GO Standard Evaluation,Oogiri-GO Standard Evaluation,https://huggingface.co/datasets/zhongshsh/CLoT-Oogiri-GO,2.0,GPU/Local,2.0,False,True,True,huggingface,openai; google; anthropic,multimodal
83f969aea328a954f8714e273b7d1a72bf7656c0,The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models,The Geometry of Creative Variability: How Credal Sets Expose Calibration Gaps in Language Models,2025.0,2025.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,"The paper evaluates the generation of short-form narrative prose (WRITINGPROMPTS) while specifically focusing on 'divergent thinking' and the variability of the model's creative 'possibility space,' which aligns with lateral thinking principles.","**Creative Artifacts Evaluated:**
The primary artifacts are short-form narrative prose continuations. These consist of multiple distinct story completions generated in response to creative prompts from a curated subset of the WRITINGPROMPTS dataset.

**Creative Capabilities Assessed:**
The framework evaluates **creative variability** and **distributional calibration**. Rather than assessing individual output quality, it measures the model’s capacity for divergent thinking by analyzing the breadth and density of its ""possibility space."" Specific dimensions include semantic diversity (via S-BERT embeddings), lexical variety (unigram Jaccard distance), and syntactic complexity (Part-of-Speech bigram Jaccard distance). It assesses how closely a model’s range of creative expression mirrors the statistical distribution of human-authored responses.

**Evaluation Context:**
This is a text-based, fully automatic evaluation using a novel geometric framework. It utilizes **credal sets**—convex sets of probability distributions—to quantify the uncertainty and variety inherent in open-ended generation. The methodology compares LLM output manifolds against human baselines using geometric metrics such as overlap coefficients, credal set volume, centroid distance, and Wasserstein distance.

**Task Characteristics:**
The task involves open-ended, unconstrained narrative generation. It shifts the focus from ""correctness"" to the **stochastic nature of creativity**, treating creative output as a probability manifold. It is characterized by its emphasis on the ""calibration gap,"" identifying where models under- or over-represent certain creative directions compared to the human baseline.",83f969aea328a954f8714e273b7d1a72bf7656c0:Credal Set Analysis for Creative Variability,Credal Set Analysis for Creative Variability,https://github.com/EstebanGarces/uncertainHuman,2.0,GPU/Local,1.0,False,False,True,github,huggingface,text
80a0b76dedc4c3e3d365bbaececcd44a996eb38b,AI-Researcher: Autonomous Scientific Innovation,AI-Researcher: Autonomous Scientific Innovation,2025.0,2025.0,,Scientific Discovery,Programming & Algorithmic Creativity,high,"The paper focuses on autonomous scientific innovation, including hypothesis generation and research paper writing, while also requiring the creation of executable machine learning code implementations.","**Creative Artifacts Evaluated:**
Scientific research papers, executable machine learning code implementations, and novel research hypotheses or directions.

**Creative Capabilities Assessed:**
Scientific originality and novelty, technical correctness and completeness of implementations, and conceptual depth in academic writing. It specifically measures evaluative judgment—the capacity to distinguish between high-quality (accepted) and low-quality (rejected) scientific contributions.

**Evaluation Context:**
A text- and code-based framework addressing high-complexity, end-to-end scientific discovery. Evaluation utilizes LLM-based agents as peer reviewers, employing relative scoring against human-authored ground truths and alignment with historical ICLR expert decisions.

**Task Characteristics:**
Tasks range from constrained generation based on explicit instructions to fully open-ended autonomous exploration and ideation. The framework uniquely combines generative problem-solving with meta-evaluative judgment, testing the agent's ability to both produce and critique innovative scientific work. This dual focus on generation and discrimination differentiates it from standard creative writing or coding benchmarks.",80a0b76dedc4c3e3d365bbaececcd44a996eb38b:Scientist-Bench,Scientist-Bench,https://github.com/HKUDS/AI-Researcher,2.0,GPU/Local,2.0,False,False,True,github,openai; anthropic; google,text
835ee2b8b2d4b148ce27b9a21dfe807bd20f4e9e,Digital Avatars: Framework Development and Their Evaluation,Digital Avatars: Framework Development and Their Evaluation,2024.0,2024.0,,Dialogue Generation & Social Interaction,Humor & Satire; Figurative Language & Rhetoric,high,"The paper's core focus is on persona-driven dialogue and creative role-play, while explicitly evaluating humor, wit, and rhetorical appeal as key metrics of digital avatar authenticity.","**Creative Artifacts Evaluated:**
Persona-constrained dialogue, character-specific humorous responses, and simulated political interview transcripts. These artifacts represent ""digital avatars""—synthetic recreations of specific human identities (e.g., public figures like Joe Biden) responding to open-ended prompts or historical interview questions.

**Creative Capabilities Assessed:**
The framework assesses anthropomorphic and creative-adjacent qualities: humor (wit and comedic timing), authenticity (stylistic verisimilitude and persona consistency), and favorability (rhetorical appeal and likability). It specifically measures the model's ability to perform nuanced ""creative role-play"" and maintain a consistent voice, prioritizing social and emotional resonance over technical or factual correctness.

**Evaluation Context:**
A text-based multi-agent environment utilizing an ""LLM-as-a-judge"" ensemble. Candidate outputs are evaluated by a panel of LLM judges, often in direct comparison with real-world human transcripts to gauge the gap between synthetic mimicry and organic persona expression. This approach moves beyond traditional NLP metrics to capture subjective, human-centric qualities.

**Task Characteristics:**
Constrained open-ended generation and comparative evaluation. The tasks involve role-playing within specific identity parameters, requiring the synthesis of a persona's voice across diverse scenarios—ranging from lighthearted comedic prompts to formal political discourse—followed by a discriminative judgment task to rank the quality of the creative simulation.",835ee2b8b2d4b148ce27b9a21dfe807bd20f4e9e:Crowd Vote,Crowd Vote,,1.0,API-Only,3.0,True,True,True,huggingface,openai,text
7fd30cd5552ae6e783d48e3cbeae6a63147b8a5f,LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context,LiveIdeaBench: Evaluating LLMs' Scientific Creativity and Idea Generation with Minimal Context,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on evaluating the generation of novel scientific hypotheses and ideas (Scientific Discovery) while specifically measuring divergent thinking and originality through unconstrained, minimalist prompting (Lateral Thinking).","**Creative Artifacts Evaluated**
The primary artifacts are **scientific ideas and hypotheses** formatted as text. These outputs are generated in response to minimalist, single-keyword prompts (e.g., ""Graphene"" or ""CRISPR"") across diverse scientific disciplines, representing raw conceptual proposals rather than fully developed research papers.

**Creative Capabilities Assessed**
The benchmark evaluates **divergent thinking** through five distinct dimensions: **originality** (novelty of the concept), **feasibility** (scientific viability and practical potential), **clarity** (structural and linguistic coherence), **fluency** (the ability to generate multiple distinct, non-redundant ideas), and **flexibility** (the breadth of conceptual categories covered). It specifically aims to decouple creative ""spark"" from general linguistic or reasoning proficiency.

**Evaluation Context**
This is a **text-based** evaluation utilizing a **dynamic LLM-as-a-judge panel** composed of multiple state-of-the-art models to reduce individual bias. This automated scoring is validated against **human expert assessments**. The ""live"" nature of the benchmark ensures that prompts are frequently updated to prevent data contamination and memorization.

**Task Characteristics**
The core task is **open-ended scientific ideation** characterized by **minimalist prompting**. Unlike benchmarks that provide detailed problem statements, this requires the model to perform **unconstrained generation**, retrieving and synthesizing latent scientific knowledge into novel applications without external scaffolding or multi-step instructions.",,,,,,,,,,,,
83d721abd07298ef0f5d05575410ae386b77bb4a,LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play,LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Dialogue Generation & Social Interaction; Scientific Discovery,high,"The paper focuses on enhancing divergent thinking and brainstorming through tasks like object-based ideation and conceptual blending, utilizing a multi-agent role-play framework to address both general and scientific problem-solving.","**Creative Artifacts Evaluated:**
The research evaluates text-based ideation outputs, specifically lists of innovative use cases for common objects (e.g., a fork), sets of items satisfying specific criteria, conceptual links between disparate concepts, and practical solutions to scientific problems.

**Creative Capabilities Assessed:**
The focus is on divergent thinking across four primary dimensions: originality (novelty of ideas), elaboration (depth of detail), fluency (total volume of ideas), and flexibility (diversity of conceptual categories). It also measures domain-specific scientific problem-solving and the ability to synthesize connections between unrelated concepts.

**Evaluation Context:**
The study employs a text-only modality using LLMs (e.g., GPT-4) within a multi-agent ""discussion"" framework. Performance is measured via a mixed-methods approach, combining human expert evaluation with LLM-based scoring (ChatGPT) using 5-point Likert scales and quantitative counts to validate the effectiveness of role-play-driven refinement.

**Task Characteristics:**
Tasks center on open-ended generation and brainstorming. They range from simple object-based ideation and conceptual blending to complex, constrained problem-solving in scientific contexts. The methodology emphasizes iterative refinement and role-play to simulate collaborative creativity, distinguishing it from single-prompt generation approaches.",83d721abd07298ef0f5d05575410ae386b77bb4a:Alternative Uses Test (AUT),Alternative Uses Test (AUT),https://github.com/lawraa/LLM-Discussion,1.0,API-Only,1.0,True,False,True,github,openai,text
85f22e830bedbae809c6e57d374f4ad3cd2f76e5,"Large Language Models for Classical Chinese Poetry Translation: Benchmarking, Evaluating, and Improving","Large Language Models for Classical Chinese Poetry Translation: Benchmarking, Evaluating, and Improving",2024.0,2024.0,,Poetry & Verse,Figurative Language & Rhetoric,high,"The paper specifically benchmarks and evaluates the translation of classical Chinese poetry, focusing on poetic constraints like meter, rhyme, and the creative adaptation of metaphors.","**Creative Artifacts Evaluated:**
English translations of classical Chinese poetry, ranging from individual couplets and sentences to full-length discourse-level poems.

**Creative Capabilities Assessed:**
The benchmark measures ""poetic elegance"" through the ""Three Beauties"" framework: Beauty of Sound (meter and rhyme), Beauty of Form (structural symmetry), and Beauty of Meaning (imagery and evocative depth). It assesses the model's ability to balance semantic adequacy with stylistic preservation, cultural resonance, and the creative adaptation of ancient metaphors into modern English.

**Evaluation Context:**
A text-centric framework comparing LLM outputs against human-authored reference translations. It employs a hybrid scoring system: automated metrics (BLEU, COMET, BLEURT) for literal accuracy, and LLM-based rubric scoring (1–5 scale) for creative attributes. This is further validated by human expert correlation and detailed error analysis to ensure the LLM-as-a-judge aligns with literary standards.

**Task Characteristics:**
Combines open-ended generation (translating complex, metaphor-heavy verse) with discriminative evaluation (judging translation adequacy). The tasks require high-level cross-lingual and cross-cultural transfer, mapping ancient linguistic structures into modern poetic forms while adhering to the rigid stylistic constraints inherent in classical Chinese prosody.",85f22e830bedbae809c6e57d374f4ad3cd2f76e5:PoetMT,PoetMT,https://github.com/andongBlue/PoetMT,1.0,API-Only,2.0,True,False,True,github,openai,text
87e044b0cb5e43d630972afaf925af032efa61bd,The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas,The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas,2025.0,2025.0,,Scientific Discovery,Programming & Algorithmic Creativity; Functional & Professional Writing,high,"The paper evaluates the generation and execution of scientific hypotheses and research papers in the NLP domain, while also incorporating the development of functional codebases and technical documentation.","**Creative Artifacts Evaluated:**
The study focuses on high-level scientific outputs, specifically **NLP research ideas** (textual hypotheses), **4-page scientific papers** (including methodology, results, and analysis), and **functional codebases** required to implement the proposed research.

**Creative Capabilities Assessed:**
The benchmark measures **scientific ideation** through dimensions of novelty and excitement. It further assesses **conceptual depth** and **practical utility** by evaluating technical soundness, expected versus actual effectiveness, and the ""faithfulness"" of a final executed project to its initial creative seed. It distinguishes between the brilliance of an abstract idea and its viability for rigorous implementation.

**Evaluation Context:**
The context is a specialized technical domain (Natural Language Processing) using a **human-in-the-loop** execution model. Evaluation is conducted via **blinded expert human peer review**, where professional researchers score both the initial AI/human ideas and the final research papers. This provides a high-stakes, real-world measure of creativity beyond simple linguistic fluency.

**Task Characteristics:**
The study involves **open-ended scientific generation** followed by a **multi-stage execution process**. It is uniquely characterized by the ""ideation-execution gap,"" comparing how initial creative prompts translate into complex, multi-modal (text and code) artifacts through human labor. This distinguishes it from standard benchmarks by measuring the long-term value and ""executability"" of creative concepts.",87e044b0cb5e43d630972afaf925af032efa61bd:Research Idea Execution Study,Research Idea Execution Study,https://github.com/NoviScl/AI-Researcher,1.0,API-Only,2.0,True,False,True,github,openai; anthropic; google; huggingface,text
8c99873106d3f2bf649c85bb5f72aa7fe1c9e19c,Spark: A System for Scientifically Creative Idea Generation,Spark: A System for Scientifically Creative Idea Generation,2025.0,2025.0,,Scientific Discovery,Functional & Professional Writing,high,"The paper focuses on the generation and critical evaluation of scientific ideas and peer reviews, which directly aligns with the Scientific Discovery domain's emphasis on research hypotheses and technical feasibility.","**Creative Artifacts Evaluated:**
The system evaluates scientific abstracts and generates structured textual peer reviews and numerical review scores. These artifacts represent the critical-evaluative component of the scientific process, focusing on the formal assessment of research proposals and findings.

**Creative Capabilities Assessed:**
The benchmarks measure critical evaluation, scientific judgment, and conceptual depth. By assessing the quality of generated reviews and the accuracy of score predictions (RMSE), the framework evaluates the model’s capacity for convergent thinking—specifically its ability to discern the novelty, utility, and technical soundness of complex scientific ideas.

**Evaluation Context:**
This is a text-based evaluation utilizing the JUDGE framework applied to a temporal split of OpenReview data. The context is highly specialized, employing a hybrid scoring approach that combines automatic metrics (Accuracy, RMSE) for quantitative predictions with qualitative assessments of the generated review text to ensure alignment with human expert standards.

**Task Characteristics:**
The tasks are centered on evaluation and judgment rather than open-ended ideation. They involve constrained generation (producing reviews based on specific abstracts) and predictive modeling. This distinguishes the work by framing scientific creativity not just as the generation of new ideas, but as the rigorous critical appraisal required to validate them.",8c99873106d3f2bf649c85bb5f72aa7fe1c9e19c:JUDGE Model Evaluation on OpenReview,JUDGE Model Evaluation on OpenReview,https://huggingface.co/datasets/spiralworks/openreview_wildcard_2025_v2,2.0,GPU/Local,2.0,False,True,True,huggingface,openai,text
8e3a1df364aa8429d8e04015dfb95145a860ebcb,Prose for a Painting,Prose for a Painting,2019.0,2019.0,,Narrative & Story Writing,Figurative Language & Rhetoric,high,"The paper focuses on generating creative prose descriptions of paintings, emphasizing narrative coherence and stylistic transformation into a specific historical register (Shakespearean English) which relies heavily on rhetorical devices.","**Creative Artifacts Evaluated:**
The primary artifacts are Shakespearean-style prose descriptions generated in response to visual paintings. Additionally, the research evaluates intermediate text-based artifacts consisting of modern English sentences translated into Early Modern English (Shakespearean) prose.

**Creative Capabilities Assessed:**
The benchmarks measure stylistic fidelity (the ability to mimic a specific historical literary persona), content relevance (the degree to which the text accurately reflects the visual elements of a painting), and subjective creativity. The evaluation specifically targets the model's capacity for aesthetic expression and its ability to maintain linguistic consistency within a constrained historical register while grounding the output in multimodal stimuli.

**Evaluation Context:**
This research operates in a multimodal (image-to-text) and unimodal (text-to-text) environment. It employs a hybrid evaluation strategy: human judges provide Likert-scale ratings for qualitative dimensions like creativity and style, while automatic metrics (BLEU) are used to assess the technical accuracy of the style-transfer sub-task. This distinguishes the work by combining subjective artistic judgment with objective linguistic benchmarks.

**Task Characteristics:**
The tasks involve constrained generation and cross-domain mapping. The model must perform a complex pipeline of visual-to-linguistic translation followed by stylistic transformation. This requires the integration of descriptive accuracy (mapping visual features to modern text) with creative refinement (transforming modern text into a specific, archaic literary style), representing a high-complexity ""vision-to-stylized-prose"" challenge.",8e3a1df364aa8429d8e04015dfb95145a860ebcb:Human Evaluation of Generated Prose for Paintings,Human Evaluation of Generated Prose for Paintings,https://github.com/prerna135/prose-for-a-painting,3.0,Special,1.0,False,False,True,github,none,multimodal
904e6b7e15eeb3b408da3a278053750b9d3e327b,"Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation","Automatic Extraction of Metaphoric Analogies from Literary Texts: Task Formulation, Dataset Construction, and Evaluation",2024.0,2024.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper specifically addresses the extraction and interpretation of metaphoric analogies and cross-domain mapping, which are core components of figurative language and lateral thinking through structural isomorphisms.","**Creative Artifacts Evaluated**
The benchmark evaluates short literary excerpts containing complex metaphoric analogies. The primary outputs are structured four-term analogical components (Target 1, Target 2, Source 1, and Source 2), consisting of extracted textual phrases and generated conceptual labels for implicit (unstated) terms.

**Creative Capabilities Assessed**
The tasks measure high-level linguistic reasoning and conceptual depth, specifically the ability to decode non-literal language. It assesses cross-domain mapping—the capacity to identify structural parallels between disparate semantic domains—and the ability to bridge literal descriptions with abstract figurative meanings.

**Evaluation Context**
This text-based evaluation focuses on Large Language Models (LLMs) using literary corpora. It employs a hybrid scoring model: automatic metrics (exact match, lemmatized overlap, and head-noun matching) evaluate the precision of explicit term extraction, while human experts provide qualitative ratings (0-2 scale) to judge the relevance and semantic accuracy of generated implicit concepts.

**Task Characteristics**
The benchmark combines constrained extraction with open-ended generation. It requires models to perform structural decomposition of creative prose and inferential reasoning to identify missing conceptual links. This involves both convergent thinking (locating specific textual evidence) and divergent interpretation (generating plausible conceptual bridges for abstract metaphors).",904e6b7e15eeb3b408da3a278053750b9d3e327b:Metaphoric Analogies from Literary Texts Dataset,Metaphoric Analogies from Literary Texts Dataset,https://github.com/Mionies/metaphoric-analogies-extraction,3.0,Special,2.0,False,False,True,github,openai,text
8dc18a17873a7c43f3f10d5b9acbfedffddf3675,In BLOOM: Creativity and Affinity in Artificial Lyrics and Art,In BLOOM: Creativity and Affinity in Artificial Lyrics and Art,2023.0,2023.0,,Music & Auditory Arts,Visual Arts & Stylized Imagery; Figurative Language & Rhetoric,high,"The paper specifically evaluates song lyrics (a core component of Music & Auditory Arts) and their multimodal translation into digital album cover art, while focusing on emotional affinity and poetic metaphors.","**Creative Artifacts Evaluated:**
The research evaluates Chinese song lyrics and lyric-guided digital album cover art.

**Creative Capabilities Assessed:**
The study measures linguistic originality, emotional affinity (the capacity to evoke a personal connection), semantic coherence, and human-likeness (recognition). It further assesses lexical diversity, distributional similarity to human-authored corpora, and the visual aesthetic quality and fidelity of generated imagery.

**Evaluation Context:**
This is a multimodal evaluation involving text (Chinese) and image modalities. It utilizes a large-scale LLM (BLOOM-176B) for creative writing and a diffusion model (Taiyi Stable Diffusion) for visual synthesis. The framework employs a hybrid scoring approach, combining crowdsourced human judgments via Amazon Mechanical Turk for subjective qualities with automated metrics like MAUVE for text distribution and Fréchet Inception Distance (FID) for image quality.

**Task Characteristics:**
The tasks involve open-ended creative text generation followed by constrained cross-domain transfer. A key discriminative feature is the focus on ""affinity,"" testing whether AI can produce emotionally resonant content. The workflow emphasizes multimodal ideation, where generated poetic metaphors serve as the foundational prompts for visual creative output, testing thematic consistency across different media.",8dc18a17873a7c43f3f10d5b9acbfedffddf3675:Chinese Lyric Generation Evaluation,Chinese Lyric Generation Evaluation,https://github.com/ecrows/in-bloom,3.0,Special,2.0,False,False,True,github,huggingface,text
8c5aab75826620559d33e99652f4cac9f6efd2fc,CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation,CreativEval: Evaluating Creativity of LLM-Based Hardware Code Generation,2024.0,2024.0,,Engineering & Technical Design,Programming & Algorithmic Creativity; Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates hardware description code (Verilog) and circuit topologies, which are explicitly listed under Engineering & Technical Design, while utilizing divergent thinking metrics and code generation frameworks.","**Creative Artifacts Evaluated:**
Functional Verilog hardware description code, specifically Register-Transfer Level (RTL) digital circuit modules and hierarchical hardware designs ranging from basic logic gates to complex state machines.

**Creative Capabilities Assessed:**
The framework evaluates four cognitive dimensions of divergent thinking: **fluency** (the volume of unique, functional solutions generated), **flexibility** (the ability to produce implementations structurally distinct from a provided reference), **originality** (statistical distance and structural uniqueness compared to standard ""golden"" designs), and **elaboration** (the capacity to synthesize complex systems by composing simpler sub-modules). Technical correctness (functionality) is treated as a foundational prerequisite for all creativity metrics.

**Evaluation Context:**
A text-to-code generation environment specialized for hardware engineering. Evaluation is entirely automated, uniquely combining logic simulation (Icarus Verilog) for functional verification with Graph Neural Network-based similarity scoring (GNN4IP) to quantify structural diversity and uniqueness at the circuit level rather than mere textual overlap.

**Task Characteristics:**
Tasks involve both constrained and open-ended generation within the rigid syntax of hardware description languages. They range from producing diverse implementations of a single logic specification to hierarchical composition and refinement, requiring models to navigate strict hardware constraints while exploring a broad architectural design space.",8c5aab75826620559d33e99652f4cac9f6efd2fc:CreativEval,CreativEval,https://github.com/matthewdelorenzo/CreativEval/,3.0,Special,4.0,False,False,True,github,openai,text
8f047a824ed8b5afc28715a552bcda9126fa7ef9,Time-R1: Towards Comprehensive Temporal Reasoning in LLMs,Time-R1: Towards Comprehensive Temporal Reasoning in LLMs,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Functional & Professional Writing; Narrative & Story Writing,high,"The paper evaluates an LLM's ability to perform complex temporal reasoning and speculative future-casting, specifically utilizing divergent thinking and analogical mapping to extrapolate trends into plausible future scenarios.","**Creative Artifacts Evaluated**
The benchmark evaluates news headlines and abstracts for hypothetical future events, specific temporal metadata (YYYY-MM formats), chronological sequences of historical events, and completed temporal expressions (masked years/months) within news narratives.

**Creative Capabilities Assessed**
The research measures speculative reasoning, plausibility, and semantic diversity in future scenario generation. It assesses the model's ability to maintain logical consistency across temporal gaps and its capacity for both convergent thinking (pinpointing exact dates) and divergent thinking (extrapolating current trends into diverse future news contexts). Technical correctness is gauged through temporal precision and chronological integrity.

**Evaluation Context**
This is a text-based LLM evaluation using the Time-Bench benchmark. Scoring is entirely automated, employing rule-based metrics—such as exponential decay functions for temporal distance and consistency penalties—alongside semantic embedding similarity (AvgMaxSim) to compare generated speculative scenarios against real-world outcomes.

**Task Characteristics**
The tasks span constrained generation (timestamping, entity completion), logical problem-solving (event ordering), and open-ended ideation (future scenario generation). The framework uniquely bridges factual historical reasoning with speculative future-casting, requiring models to synthesize historical patterns to predict and invent plausible future timelines.",8f047a824ed8b5afc28715a552bcda9126fa7ef9:Time-Bench,Time-Bench,https://github.com/ulab-uiuc/Time-R1,2.0,GPU/Local,6.0,False,True,True,huggingface,none,text
90777a2cfa8a6e860244dd8c1f8027961bc561f5,"Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting","Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting",2025.0,2025.0,,Scientific Discovery,Mathematical Reasoning; Programming & Algorithmic Creativity,high,"The paper explicitly evaluates LLM performance across three distinct benchmarks: scientific research ideation (Scientific Discovery), multi-step mathematical proofs (Mathematical Reasoning), and Python code generation (Programming & Algorithmic Creativity).","**Creative Artifacts Evaluated:**
The paper evaluates scientific hypotheses and research ideas (LiveIdeaBench), functional Python code snippets for data science (DS-1000), and multi-step mathematical proofs and solutions (Omni-MATH).

**Creative Capabilities Assessed:**
The study measures divergent thinking through novelty, originality, and feasibility in scientific ideation. It also assesses technical correctness, logical soundness, and pragmatism. A core focus is the model’s capacity for iterative refinement, quantified through ""Drift from Origin"" (semantic shift), ""Turn-to-Turn Volatility"" (stability of changes), and ""Lexical Novelty"" (introduction of new n-grams/concepts over time).

**Evaluation Context:**
Evaluations are conducted in a text-only modality using high-complexity reasoning tasks. The methodology employs a hybrid ""mixed"" scoring approach: qualitative dimensions (e.g., clarity, readability) are rated on 1–10 scales by an LLM-as-judge (Gemini 1.5 Pro), while structural evolution is tracked via automated metrics like cosine similarity and growth factors (word/line counts).

**Task Characteristics:**
The research centers on iterative, multi-turn generation across a spectrum of constraints. It contrasts open-ended ideation (scientific discovery) with constrained, closed-ended problem-solving (mathematics and coding). This allows for a comparative analysis of how successive prompting turns facilitate either creative expansion or functional convergence.",,,,,,,,,,,,
926f42f9f23ca925d9ac234e9db0e48a011dcc9f,Enhancing multimodal analogical reasoning with Logic Augmented Generation,Enhancing multimodal analogical reasoning with Logic Augmented Generation,2025.0,2025.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving; Scientific Discovery,high,"The paper's core focus is on interpreting metaphors and non-literal reasoning through cross-domain mapping, while utilizing analogical reasoning and applying these techniques to scientific contexts.","**Creative Artifacts Evaluated:**
The research evaluates metaphorical sentences (both general and domain-specific scientific text), visual metaphors (images containing non-literal communication), and structured logic representations in the form of Extended Knowledge Graphs (XKG) that map analogical relationships.

**Creative Capabilities Assessed:**
Assessment focuses on conceptual depth and non-literal reasoning, specifically the ability to perform cross-domain mapping between source and target domains. It measures abstract property identification—linking visual or textual cues to higher-level concepts—and the ability to distinguish between literal and figurative intent in complex scientific contexts.

**Evaluation Context:**
This multimodal study utilizes LLMs and VLMs across text and image modalities. Evaluation is highly rigorous, combining automatic metrics (Accuracy, F1, BLEURT) with human-centric validation. This includes majority-vote manual annotation (measured via Fleiss' Kappa), comparison against human participant performance, and expert qualitative review of generated logical graphs by knowledge engineers to ensure structural validity.

**Task Characteristics:**
Tasks are primarily interpretative and analytical rather than open-ended. They include discriminative classification (metaphor detection), constrained generation (identifying source/target domains), and cross-domain transfer (scientific metaphor understanding). The focus is on ""Logic Augmented Generation,"" where the model must not only identify a metaphor but also explicitly model the underlying analogical reasoning structure.",926f42f9f23ca925d9ac234e9db0e48a011dcc9f:MOH-X,MOH-X,https://github.com/dersuchendee/metaphorical-logic-augmented-generation,1.0,API-Only,1.0,True,True,True,huggingface,anthropic,text
9ab0e3fcf4ac34fbaf5e1d8c868c661da658305a,Summary of the Visually Grounded Story Generation Challenge,Summary of the Visually Grounded Story Generation Challenge,2024.0,2024.0,,Narrative & Story Writing,,high,"The paper describes a challenge specifically focused on generating short-form creative narratives from visual sequences, evaluating core storytelling elements such as plot progression, character consistency, and narrative coherence.","**Creative Artifacts Evaluated:**
The primary artifacts are short-form creative narratives (stories) generated from sequential image sets. These artifacts require maintaining character consistency and plot progression across multiple visual frames, moving beyond simple image captioning into creative prose.

**Creative Capabilities Assessed:**
The benchmark measures narrative coherence (logical continuity), visual grounding (mapping text to specific image elements), and linguistic diversity (lexical variety and unique verb usage). It specifically assesses the model's ability to synthesize discrete visual cues into a unified creative arc, balancing factual grounding with imaginative storytelling.

**Evaluation Context:**
This is a multimodal vision-and-language task involving large-scale AI models. Evaluation is ""mixed,"" utilizing standard automatic linguistic metrics (BLEU, ROUGE, CIDEr), specialized structural models for coherence (Entity Grid), and human expert ratings for subjective qualities like ""Overall"" quality, diversity, and grammaticality.

**Task Characteristics:**
This is a constrained, open-ended generation task. It requires cross-modal translation where the model must perform ""visually grounded"" storytelling—a form of creative synthesis that bridges discrete visual data points into a continuous, imaginative text. It focuses on the transition from descriptive vision tasks to creative narrative construction.",,,,,,,,,,,,
9d84da449502cca33edc2b3fa59fb7b5ae44a0f5,EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation,EmoArt: A Multidimensional Dataset for Emotion-Aware Artistic Generation,2025.0,2025.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on evaluating the generation of emotionally expressive artistic images, specifically assessing stylistic elements like brushwork, color, and composition.","**Creative Artifacts Evaluated:**
The benchmark evaluates emotionally expressive artistic images generated from textual prompts. These artifacts are characterized by specific stylistic elements including varied brushwork, color palettes, compositions, lighting conditions, and line work, distinguishing them from generic photorealistic imagery or standard digital illustrations.

**Creative Capabilities Assessed:**
The dataset assesses the model's ability to achieve emotional resonance and stylistic coherence. Specifically, it measures ""Attributes Alignment,"" evaluating how effectively generated images reflect five core artistic dimensions: brushwork, color, composition, light, and line. This evaluates the model's technical artistic execution and its capacity to translate abstract affective states into specific visual motifs.

**Evaluation Context:**
Evaluation is multimodal, focusing on text-to-image diffusion models. The framework employs a hybrid scoring approach: standard computer vision metrics (FID, SSIM, PSNR, LPIPS) are combined with a custom alignment score derived from a fine-tuned Vision-Language Model (MiniCPM-V-2.6). This provides a more nuanced, domain-specific critique of artistic features than general-purpose CLIP embeddings.

**Task Characteristics:**
The primary task is constrained generation, requiring models to synthesize images that satisfy both emotional intent and complex artistic constraints. It represents a high-complexity mapping task where abstract affective concepts must be translated into concrete visual-stylistic choices, testing cross-domain transfer between linguistic sentiment and artistic expression.",9d84da449502cca33edc2b3fa59fb7b5ae44a0f5:EmoArt,EmoArt,https://huggingface.co/datasets/printblue/EmoArt-130k,2.0,GPU/Local,1.0,False,True,True,huggingface,none,multimodal
93bd60ce85fed4ccc7c7a6f05b90a1cb1e990eac,Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events,Outline to Story: Fine-grained Controllable Story Generation from Cascaded Events,2021.0,2021.0,,Narrative & Story Writing,,high,"The paper focuses exclusively on the generation of multi-paragraph fictional stories and hierarchical plot outlines, emphasizing narrative coherence and thematic consistency.","**Creative Artifacts Evaluated:**
The primary artifacts are multi-paragraph narrative stories and hierarchical event outlines consisting of keyword-based event sequences. These outlines serve as the structural backbone for the generated prose, representing distinct plot points for each paragraph.

**Creative Capabilities Assessed:**
The benchmarks measure narrative fluency, structural coherence, and the ability to maintain thematic consistency across long-form text. Specifically, it evaluates ""fine-grained controllability""—the model's capacity to adhere to specific content constraints (faithfulness to the outline). It also assesses predictive planning and divergent thinking through the generation of subsequent plot events, measuring how well the model can forecast a logical narrative progression.

**Evaluation Context:**
This research operates within the textual domain, utilizing Large Language Models (LLMs) evaluated on the WritingPrompts (creative fiction) and WikiPlots (plot summaries) datasets. Performance is quantified through automatic linguistic metrics, including word-level and BPE-based Perplexity, BLEU-4, and ROUGE (1, 2, and L) scores, which compare machine-generated stories and event sequences against human-authored references.

**Task Characteristics:**
The work centers on constrained generation and hierarchical ideation. Unlike simple prompt-to-text tasks, this involves a cascaded approach where high-level event sequences are transformed into detailed prose. The tasks encompass both creative drafting (story generation) and structural planning (event generation), emphasizing the transition from abstract, keyword-based plot points to concrete, multi-paragraph creative expression.",93bd60ce85fed4ccc7c7a6f05b90a1cb1e990eac:Outline to Story (O2S),Outline to Story (O2S),https://github.com/fangleai/Outline22Story,2.0,GPU/Local,3.0,False,False,True,github,,text
9e78475b5a99c7ce7e106e7a9fecf0c0db56e733,***YesBut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models,***YesBut***: A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models,2024.0,2024.0,,Humor & Satire,Visual Arts & Stylized Imagery; Figurative Language & Rhetoric; Graphic Design & Visual Layout,high,The paper explicitly focuses on evaluating the comprehension of visual satire and irony using a dataset of juxtaposed images and satirical photographs.,"**Creative Artifacts Evaluated:**
The benchmark evaluates satirical visual compositions, specifically two-part juxtaposed images (the ""YesBut"" format) and real-world satirical photographs. These artifacts represent a specialized form of visual storytelling where meaning is derived from the contrast between two related scenes.

**Creative Capabilities Assessed:**
The dataset measures high-level conceptual depth, irony detection, and visual-semantic reasoning. It specifically assesses a model’s ability to decode humorous intent, identify social contradictions, and articulate the underlying logic of satirical narratives. Unlike standard image captioning, this requires bridging the gap between literal visual content and figurative meaning.

**Evaluation Context:**
This is a multimodal evaluation of Vision-Language Models (VLMs) using both synthetic/structured and ""in-the-wild"" datasets. Scoring is a hybrid of automatic metrics (Accuracy, F1, BLEU, ROUGE-L, BERTScore) for classification and generation, and human evaluation focused on correctness, faithfulness, and visual completeness of the explanations.

**Task Characteristics:**
The framework includes discriminative judgment (satire detection), open-ended generation (satire understanding/explanation), and constrained logical reasoning (image completion). This multi-task approach requires models to not only recognize creative intent but also predict the specific visual components necessary to fulfill a satirical premise.",9e78475b5a99c7ce7e106e7a9fecf0c0db56e733:Real Satirical Photographs Dataset,Real Satirical Photographs Dataset,https://huggingface.co/datasets/bansalaman18/yesbut,1.0,API-Only,2.0,True,True,True,huggingface,openai; google,multimodal
9bd723ac16fab44267211f93442153626aac8ccd,"Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models","Diverse, not Short: A Length-Controlled Data Selection Strategy for Improving Response Diversity of Language Models",2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing,high,"The paper's core focus is on divergent thinking and ideational fluency, specifically utilizing tasks like the Alternative Uses Task and semantic dissimilarity measures, while also evaluating constrained short story generation.","**Creative Artifacts Evaluated:**
The benchmarks evaluate lists of semantically dissimilar words, structured JSON persona profiles (comprising names, birth cities, and occupations), unconventional functional uses for common household objects (e.g., bricks, brooms), and short stories constrained by the inclusion of specific keyword sets.

**Creative Capabilities Assessed:**
Assessment focuses on divergent thinking, specifically semantic dissimilarity and ideational fluency. The tasks measure the model's ability to maximize conceptual distance (originality), avoid repetitive patterns in structured data (diversity), and generate unconventional associations (flexibility). It also evaluates the trade-off between creative diversity and narrative quality.

**Evaluation Context:**
This is a text-based LLM evaluation utilizing a hybrid scoring framework. It combines psychometric metrics, such as Divergent Semantic Integration (DSI), with automated linguistic measures (4-gram diversity, unique value proportions) and reward models (ArmoRM). It further incorporates pairwise preferences via human and LLM-as-a-Judge, specifically utilizing the ""Diversity Decile"" (∆DD) to isolate diversity improvements from length-biased artifacts.

**Task Characteristics:**
The research employs a mix of open-ended ideation (word/use generation) and constrained generation (JSON formatting and keyword-integrated storytelling). It adapts validated psychological assessments to probe the model's ability to traverse broad conceptual spaces while maintaining structural and narrative constraints.",9bd723ac16fab44267211f93442153626aac8ccd:Persona Generation Task,Persona Generation Task,https://github.com/vijetadeshpande/Diverse-NS,1.0,API-Only,1.0,True,False,True,github,openai; anthropic; google; together,text
91044b93c470eb7619d6443cf89a27a77919c740,ALF at SemEval-2024 Task 9: Exploring Lateral Thinking Capabilities of LMs through Multi-task Fine-tuning,ALF at SemEval-2024 Task 9: Exploring Lateral Thinking Capabilities of LMs through Multi-task Fine-tuning,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Humor & Satire; Figurative Language & Rhetoric,high,"The paper explicitly focuses on evaluating lateral thinking through riddles and brain teasers, which are core components of the Lateral Thinking domain, while also involving wordplay and non-literal linguistic devices.","**Creative Artifacts Evaluated:**
The evaluation focuses on linguistic puzzles, specifically **sentence-based brain teasers** and **riddles**. These artifacts are characterized by their reliance on wordplay, unconventional logic, and the deliberate subversion of standard commonsense expectations.

**Creative Capabilities Assessed:**
The primary capability measured is **lateral thinking**, specifically the ability to bypass intuitive but incorrect ""common sense"" answers in favor of creative, non-obvious solutions. The research assesses **divergent reasoning** and the model's robustness against **semantic and contextual perturbations**, requiring the model to maintain logical consistency across original and modified versions of a puzzle.

**Evaluation Context:**
This is a **text-based** evaluation of **Large Language Models** (LLMs). Performance is measured through **automatic scoring** of multiple-choice questions. A rigorous **group-based accuracy** metric is employed, where a model must correctly solve all variants of a puzzle (original, semantic, and contextual) to demonstrate true reasoning rather than pattern matching.

**Task Characteristics:**
The tasks involve **constrained problem-solving** and **creative reasoning**. A key feature is the use of **multi-task fine-tuning** to explore **cross-domain transfer**, specifically testing how auxiliary training on riddles and standard commonsense datasets (e.g., PIQA, HellaSWAG) impacts a model's ability to perform in the specialized domain of lateral thinking.",91044b93c470eb7619d6443cf89a27a77919c740:RiddleSense,RiddleSense,https://github.com/alifarrokh/SemEval2024-Task9,2.0,GPU/Local,1.0,False,True,True,huggingface,none,text
999d9248eaf9d077044c943c77ae2d1de1041fbf,MELFuSION: Synthesizing Music from Image and Language Cues Using Diffusion Models,MELFuSION: Synthesizing Music from Image and Language Cues Using Diffusion Models,2024.0,2024.0,,Music & Auditory Arts,Visual Arts & Stylized Imagery,high,"The paper focuses on synthesizing music (audio synthesis) and specifically evaluates the alignment of that music with visual aesthetics and imagery, which are core components of the Music & Auditory Arts and Visual Arts domains respectively.","**Creative Artifacts Evaluated:**
The primary artifacts are 10-second synthesized music clips generated from synergistic image-text prompts. These artifacts represent complex auditory compositions intended to reflect the specific mood, style, and thematic content of paired visual and linguistic inputs.

**Creative Capabilities Assessed:**
The research assesses technical fidelity (audio quality), stylistic relevance, and cross-modal alignment. It specifically measures the model’s ability to translate visual aesthetics and textual descriptions into coherent auditory themes, evaluating the ""creative synergy"" between modalities. Assessed capabilities include divergent synthesis (mapping visual cues to musical textures) and constrained generation (adhering to dual-modality prompts).

**Evaluation Context:**
The study operates within a multimodal framework (image/text/audio) utilizing diffusion models. Evaluation is a hybrid of objective signal-based metrics (FAD, KL Divergence, Fréchet Distance), a novel Image-Music Similarity Metric (IMSM) leveraging cross-modal embeddings, and human subjective ratings for Overall Quality (OVL) and Relevance (REL).

**Task Characteristics:**
Tasks involve constrained multimodal generation requiring the integration of disparate sensory inputs into a single creative output. The work emphasizes cross-domain transfer (visual-to-audio) and includes a meta-task of metric validation, where automated similarity scores are correlated against human perceptual judgments to quantify the ""perceptual gap"" in creative alignment.",999d9248eaf9d077044c943c77ae2d1de1041fbf:MeLBench,MeLBench,https://github.com/schowdhury671/melfusion,2.0,GPU/Local,1.0,False,False,True,github,,multimodal
9bf4e04916d7b5d9185472cb98bb0db24f9d2864,Learning to Generate Research Idea with Dynamic Control,Learning to Generate Research Idea with Dynamic Control,2024.0,2024.0,,Scientific Discovery,Functional & Professional Writing; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating novel scientific research proposals and methodologies based on existing literature, which directly aligns with the Scientific Discovery domain's emphasis on research hypotheses and technical feasibility.","**Creative Artifacts Evaluated:**
Structured scientific research proposals, specifically comprising novel methodologies and conceptual frameworks designed as follow-up developments to existing academic papers.

**Creative Capabilities Assessed:**
Scientific originality (novelty), technical viability (feasibility), and potential research impact (effectiveness). The benchmark measures the capacity for complex scientific reasoning, the synthesis of existing knowledge, and the ability to project logical yet innovative future directions within a specialized domain.

**Evaluation Context:**
A high-complexity text-based domain where Large Language Models (LLMs) act as generative agents. Evaluation is conducted through a hybrid approach: an automated ""reviewing agent"" (GPT-4) and manual assessment by 15 domain experts, simulating the rigorous standards of the academic peer-review process.

**Task Characteristics:**
This is a constrained ideation and problem-solving task. Unlike open-ended storytelling, it requires grounded generation based on specific scientific inputs. It emphasizes ""dynamic control,"" allowing for the steering of the creative process toward specific research goals. This distinguishes the work from unguided brainstorming, focusing instead on the intersection of creative synthesis and technical constraint.",9bf4e04916d7b5d9185472cb98bb0db24f9d2864:Learn2Gen,Learn2Gen,https://github.com/du-nlp-lab/Learn2Gen,2.0,GPU/Local,1.0,False,False,True,github,openai,text
975b23dd7f82ea53ccf9a0c6a5d65368fb19c3d8,Hypothesis Generation with Large Language Models,Hypothesis Generation with Large Language Models,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on the generation of research hypotheses and social science theories, which is the core definition of the Scientific Discovery domain, while also evaluating divergent thinking and pattern discovery.","**Creative Artifacts Evaluated**
The primary artifacts are linguistic hypotheses and causal explanations. These take the form of structured rules explaining consumer behavior (shoe color choices), markers of deception in hotel reviews, and engagement drivers for news headlines and social media posts (tweets).

**Creative Capabilities Assessed**
The research measures inductive reasoning, pattern discovery, and conceptual depth. Specifically, it evaluates the LLM's capacity for both convergent thinking (recovering a single, hidden ""ground truth"" rule in synthetic data) and divergent thinking (generating multiple plausible social science theories for real-world data). The usefulness of these artifacts is quantified by their predictive power in downstream classification tasks.

**Evaluation Context**
The evaluation is text-centric, utilizing LLMs to analyze diverse datasets. It employs a hybrid scoring model: automatic classification accuracy (testing if the hypothesis correctly predicts new data) and qualitative analysis, where LLM-generated insights are compared against established findings in peer-reviewed scientific literature to validate their depth and accuracy.

**Task Characteristics**
Tasks focus on hypothesis-driven problem-solving and ideation. They range from constrained generation in synthetic environments to open-ended reasoning in social science domains. A distinguishing feature is the assessment of cross-domain transfer, where hypotheses generated on one dataset are tested on out-of-distribution (OOD) data to ensure they capture generalizable principles rather than dataset-specific noise.",975b23dd7f82ea53ccf9a0c6a5d65368fb19c3d8:SHOE SALES,SHOE SALES,https://github.com/ChicagoHAI/hypothesis-generation,1.0,API-Only,1.0,True,True,True,huggingface,openai; anthropic; google,text
9ed9ee4b06249147adc5bee1aabaf86cb309bdc4,The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation,The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation,2024.0,2024.0,,Visual Arts & Stylized Imagery,Lateral Thinking & Creative Problem Solving,high,The paper focuses on the generation and evaluation of high-fidelity synthetic images (realistic and fantasy) and specifically assesses the model's capacity for divergent thinking and conceptual blending when interpreting complex prompts.,"**Creative Artifacts Evaluated:**
The primary artifacts are high-fidelity synthetic images categorized into two distinct domains: realistic scenes (complex, multi-object real-world scenarios) and fantasy scenes (imaginative environments, mythical creatures, and non-existent conceptual entities).

**Creative Capabilities Assessed:**
The benchmarks measure the model's capacity for divergent thinking and conceptual blending, specifically focusing on the ability to interpret and visualize intricate, multi-layered text prompts. Key capabilities include semantic reasoning (handling complex spatial and conceptual relationships), text-to-image fidelity (alignment with nuanced descriptions), and visual aesthetic quality. The evaluation distinguishes between the model's ability to replicate reality and its capacity for ""fantasy fabrication.""

**Evaluation Context:**
This research operates within a multimodal framework where Diffusion models are augmented by LLM-assisted prompt interpretation. Evaluation is conducted through a hybrid scoring system: automated metrics (GPT4-CLIP cosine similarity for text-text alignment), LLM-as-a-judge (GPT4Score for semantic adherence), and human preference ratings (1-5 scale) focusing on visual quality and fidelity.

**Task Characteristics:**
Tasks involve open-ended generation and interpretive refinement. Unlike standard text-to-image tasks, these require the model to bridge the gap between abstract, creative linguistic descriptions and concrete visual representations. This involves complex scene synthesis and the translation of imaginative, often paradoxical, prompts into coherent visual structures, emphasizing the transition from linguistic creativity to visual realization.",9ed9ee4b06249147adc5bee1aabaf86cb309bdc4:Realistic-Fantasy Benchmark (RFBench),Realistic-Fantasy Benchmark (RFBench),https://github.com/leo81005/Reality-and-Fantasy,2.0,GPU/Local,1.0,False,False,True,github,openai,image
9fc5a55e96b63c4e981363c7a03c54cc6ae8976d,CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays,CERD: A Comprehensive Chinese Rhetoric Dataset for Rhetorical Understanding and Generation in Essays,2024.0,2024.0,,Figurative Language & Rhetoric,Narrative & Story Writing,high,"The paper explicitly focuses on the identification, extraction, and generation of rhetorical devices such as metaphors and personification within the context of Chinese essays.","**Creative Artifacts Evaluated:**
The dataset focuses on Chinese rhetorical sentences extracted from high-quality essays. These artifacts include specific figures of speech such as metaphors, personification, and hyperbole, as well as their structural sub-components, including connectors, subjects, and comparative objects.

**Creative Capabilities Assessed:**
The benchmark measures linguistic sophistication and stylistic mastery through both understanding and generation. It assesses the ability to categorize rhetorical devices at coarse and fine-grained (form-level and content-level) scales. Furthermore, it evaluates structural precision via component extraction and creative fluency through the generation of contextually appropriate, stylistically rich prose that adheres to specific rhetorical constraints.

**Evaluation Context:**
This is a text-based Chinese language benchmark evaluating Large Language Models (LLMs). It employs a hybrid scoring framework: automatic metrics (Exact Match, F1, BLEU, ROUGE, BERTScore) for classification and extraction tasks, and a combination of human judgment and LLM-based (GPT-4) single-answer rating and pairwise ranking for generation quality.

**Task Characteristics:**
The tasks span a spectrum from discriminative analysis to constrained generation. They include multi-label classification (identifying rhetoric types), named entity recognition (extracting structural components), and controllable text generation where models must synthesize rhetorical sentences that fit a provided essay context.",9fc5a55e96b63c4e981363c7a03c54cc6ae8976d:Chinese Essay Rhetoric Dataset (CERD),Chinese Essay Rhetoric Dataset (CERD),https://github.com/cubenlp/cerd,2.0,GPU/Local,5.0,False,False,True,request_access,openai,text
9ee4e183e5d710a561b17ff1c157fbf34c97c1a5,Predicting Empirical AI Research Outcomes with Language Models,Predicting Empirical AI Research Outcomes with Language Models,2025.0,2025.0,,Scientific Discovery,,high,"The paper explicitly focuses on evaluating scientific hypotheses, experimental designs, and research ideation within the STEM field of AI/NLP.","**Creative Artifacts Evaluated:**
The artifacts consist of scientific hypotheses, experimental designs, and Natural Language Processing (NLP) research ideas. These are structured as comparative pairs—typically a novel methodological intervention versus a standard baseline—formulated to achieve specific empirical research goals.

**Creative Capabilities Assessed:**
The benchmark measures evaluative judgment and predictive foresight regarding scientific innovation. It assesses the model’s ability to determine the utility and empirical viability of a research idea. This requires technical correctness, conceptual depth, and convergent thinking to identify which of two competing creative paths will yield superior performance in a real-world experimental setting.

**Evaluation Context:**
This is a text-based evaluation comparing LLM performance against a ground truth of empirical results from both published and unpublished studies. It features a high-complexity comparative component involving 25 human NLP experts. Scoring is primarily automatic, based on prediction accuracy and robustness across various data subsets to ensure the model is not relying on simple heuristics.

**Task Characteristics:**
Tasks focus on discriminative evaluation and forecasting rather than open-ended generation. Key characteristics include scientific ideation assessment, judgment of novelty versus utility, and generalization to unpublished research. The framework emphasizes the model's capacity for ""scientific intuition"" and its ability to function as a critic or peer reviewer within the creative research process.",9ee4e183e5d710a561b17ff1c157fbf34c97c1a5:Research Outcome Prediction Benchmark,Research Outcome Prediction Benchmark,https://github.com/NoviScl/AI-Researcher,1.0,API-Only,3.0,True,False,True,github,openai; anthropic,text
a33651ced4bb85338daec7eebf7a06fb4c29a011,The Budget AI Researcher and the Power of RAG Chains,The Budget AI Researcher and the Power of RAG Chains,2025.0,2025.0,,Scientific Discovery,Functional & Professional Writing,high,"The paper focuses on the generation and evaluation of novel scientific research hypotheses and abstracts, while also simulating the professional academic peer-review process.","**Creative Artifacts Evaluated:**
The primary artifacts are **academic research abstracts** representing hypothetical scientific studies. These text-based outputs encapsulate novel research questions, methodologies, and expected outcomes within the domain of scientific discovery.

**Creative Capabilities Assessed:**
The framework measures **novelty** (uniqueness of the idea), **interestingness** (intellectual engagement), and **feasibility** (practicality of execution). It specifically evaluates **scientific originality**, **technical soundness**, and the **potential contribution** to the field. A distinguishing capability assessed is **predictive alignment**, which measures how closely AI-generated ideas mirror actual future research trends and conference publications.

**Evaluation Context:**
This is a text-centric evaluation focused on the **scientific ideation** phase. It employs a hybrid scoring model that combines **LLM-as-a-judge** (leveraging ChatGPT-4o and Llama 3.1 8B) with **expert human evaluation** from graduate-level researchers. The context is unique for its use of ""future"" conference data as a benchmark to quantify the foresight and relevance of AI-generated hypotheses.

**Task Characteristics:**
Tasks involve **open-ended ideation** (generating research abstracts) and **structured judgment** (detailed rubric-based peer review). The framework includes **constrained evaluation**, where models must provide binary ""Accept/Reject"" decisions and confidence scores, effectively simulating the academic peer-review process to distinguish high-potential research from incremental or non-viable work.",a33651ced4bb85338daec7eebf7a06fb4c29a011:Research Ideation Evaluation Framework,Research Ideation Evaluation Framework,https://github.com/hellojoeAoPS11235/ai-research-agent,3.0,Special,4.0,False,False,True,github,openai; anthropic; google,text
a1073c76108408dedac030ddd8f89c0b970e22d2,Characterising the Creative Process in Humans and Large Language Models,Characterising the Creative Process in Humans and Large Language Models,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Scientific Discovery,high,"The paper focuses on divergent thinking and the Alternate Uses Task (AUT), which are core components of lateral thinking, while employing a scientific framework to characterize the cognitive processes of LLMs.","**Creative Artifacts Evaluated**
The artifacts consist of lists of novel functional applications for common physical objects (e.g., bricks and paperclips) and sequences of semantic category members (e.g., animal names). These are represented as short-form text strings generated in a sequential list format.

**Creative Capabilities Assessed**
The primary focus is on divergent thinking, specifically measuring originality and the underlying semantic search process. The research assesses ""flexibility"" (the capacity to jump between disparate conceptual categories) versus ""persistence"" (the ability to exhaustively explore ideas within a single conceptual cluster). It also evaluates semantic fluency and the ability to navigate conceptual spaces.

**Evaluation Context**
This work utilizes text-based Large Language Models (LLMs) to simulate human-like ideation. Evaluation is entirely automated, combining LLM-based originality scoring (via Open Creativity Scoring/OCSAI) with algorithmic sequence analysis. The latter uses sentence embeddings and clustering to generate a ""jump profile,"" quantifying the cognitive trajectory of the model’s creative process.

**Task Characteristics**
The tasks involve open-ended ideation and constrained semantic retrieval. By comparing a creative generation task (Alternate Uses) with a non-creative baseline (Verbal Fluency), the research isolates the specific mechanisms of creative search. The tasks are inherently sequential, focusing on the evolution of ideas over time rather than isolated outputs.",a1073c76108408dedac030ddd8f89c0b970e22d2:Verbal Fluency Task,Verbal Fluency Task,https://github.com/surabhisnath/Creative_Process,2.0,GPU/Local,1.0,False,False,True,github,openai; anthropic; google; together,text
a0035379f93e0e95bdadd77a1d8eb27ba89dcf60,STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,2020.0,2020.0,,Narrative & Story Writing,Dialogue Generation & Social Interaction,high,"The paper introduces a dataset and platform specifically for long-form narrative generation and collaborative storytelling, while the machine-in-the-loop aspect involves interactive, persona-driven exchanges characteristic of social interaction domains.","**Creative Artifacts Evaluated:**
The primary artifacts are long-form, collaborative narrative ""scene entries."" These are text-based continuations of ongoing stories, deeply integrated with structural metadata such as character cards, world descriptions, and specific scene goals.

**Creative Capabilities Assessed:**
Evaluation focuses on narrative fluency, global coherence, and contextual relevance. A unique capability assessed is ""collaborative utility"" or usefulness, determined by the extent to which human authors incorporate AI suggestions into their final prose. It also measures aesthetic qualities like likability and the model's ability to adhere to complex, multi-modal constraints (text plus structural data).

**Evaluation Context:**
This is a machine-in-the-loop text generation environment. It employs a multi-tiered evaluation strategy: expert human ratings from active authors on the STORIUM platform, crowdsourced Likert-scale assessments, and automatic metrics including ROUGE, perplexity, and the domain-specific USER (User Story Edit Ratings) metric, which quantifies the efficiency of human-AI collaboration.

**Task Characteristics:**
The core task is constrained narrative generation within a collaborative framework. It involves open-ended storytelling that must align with pre-defined character traits and plot trajectories. The work emphasizes the transition from solo generation to interactive refinement and expert-level judgment.",a0035379f93e0e95bdadd77a1d8eb27ba89dcf60:STORIUM Dataset,STORIUM Dataset,https://huggingface.co/datasets/NewEden/Storium-5K,2.0,GPU/Local,1.0,False,True,True,huggingface,,text
a54e0011f478d476f4582ce1234d56d1974729fe,Understanding Large Language Models' Ability on Interdisciplinary Research,Understanding Large Language Models' Ability on Interdisciplinary Research,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper specifically evaluates the generation and evaluation of interdisciplinary research hypotheses and cross-domain scientific synthesis, which are core components of scientific discovery and lateral thinking.","**Creative Artifacts Evaluated:**
Scientific research abstracts, interdisciplinary research (IDR) hypotheses, and curated lists of cross-disciplinary paper pairings.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and divergent thinking by testing the ability to bridge disparate scientific fields. It evaluates the perceived ""promise"" or usefulness of proposed integrations, as well as convergent thinking through the prioritization of high-potential research directions from a pool of candidates.

**Evaluation Context:**
This text-based evaluation focuses on LLMs within the scientific domain. It employs a hybrid scoring methodology, utilizing automatic classification (F1), semantic similarity (SciBERT), and information retrieval metrics (MRR, NDCG@10, Kendall's τ) to assess both the discriminative accuracy and the generative quality of interdisciplinary insights.

**Task Characteristics:**
The tasks encompass a range of cognitive functions: discriminative judgment (identifying IDR), generative ideation (predicting integration outcomes), and evaluative refinement (re-ranking candidate papers). A defining feature is the focus on cross-domain transfer, requiring models to synthesize knowledge from distinct disciplines into novel, coherent research frameworks. This differentiates the work by focusing on ""scientific creativity"" rather than artistic or linguistic expression.",a54e0011f478d476f4582ce1234d56d1974729fe:IDRBench,IDRBench,,1.0,API-Only,3.0,True,True,True,huggingface,openai; anthropic; google; huggingface,text
a33ba08c5f7f4c11205ea19d4a8718cc5bb5de98,Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models,Automatic Generation of Fashion Images using Prompting in Generative Machine Learning Models,2024.0,2024.0,,Visual Arts & Stylized Imagery,Functional & Professional Writing; Engineering & Technical Design,high,"The paper focuses on the generation and aesthetic evaluation of fashion imagery and professional outfit descriptions, bridging visual artistic expression with functional product design for the fashion industry.","**Creative Artifacts Evaluated:**
The study evaluates AI-generated fashion ensemble images and their corresponding textual outfit descriptions, which detail specific garment types, color palettes, and fabric textures intended for human wear.

**Creative Capabilities Assessed:**
Assessment centers on aesthetic appeal, stylistic cohesion, and ""inspiration potential""—a metric capturing the novelty and utility of a design for professional use. It measures functional creativity through occasion and wearer suitability, alongside the linguistic comprehensibility, coherence, and descriptive accuracy of the generated fashion narratives.

**Evaluation Context:**
This multimodal research utilizes a new human-centric framework to evaluate the performance of Large Language Models (LLMs) and Diffusion models. It compares advanced prompting strategies (e.g., Chain-of-Thought, RAG) using a mix of 5-point Likert scales, comparative human ranking, and automated CLIPscore to bridge the gap between technical text-image alignment and subjective design quality.

**Task Characteristics:**
Tasks involve open-ended fashion generation constrained by specific user personas, styles, or social contexts. The methodology focuses on the evaluation and judgment of creative artifacts, specifically analyzing how different prompting architectures impact the synthesis of complex visual-textual designs and their potential to serve as practical ideation tools in the fashion industry.",a33ba08c5f7f4c11205ea19d4a8718cc5bb5de98:Human Evaluation of Fashion Generation,Human Evaluation of Fashion Generation,https://github.com/georgiarg/AutoFashion,,Unknown,3.0,False,False,False,unknown,,unknown
a15e0f13aecab1f376371cfa46f96a4c8d58434e,Mining Contextualized Visual Associations from Images for Creativity Understanding,Mining Contextualized Visual Associations from Images for Creativity Understanding,2025.0,2025.0,,Figurative Language & Rhetoric,Visual Arts & Stylized Imagery; Lateral Thinking & Creative Problem Solving,high,"The paper's core focus is on metaphorical reasoning and the visual grounding of non-literal language, which directly aligns with figurative language, while utilizing artistic imagery and cross-domain conceptual mapping.","**Creative Artifacts Evaluated:**
The research evaluates multimodal artifacts including **photorealistic images** (MSCOCO), **abstracted creative captions** generated at varying levels of conceptual distance, **short poems**, and both **visual and linguistic metaphors**. 

**Creative Capabilities Assessed:**
Key capabilities include **conceptual abstraction** (hierarchical ranking of association levels), **visual grounding** of non-literal language, and **metaphorical reasoning**. The work specifically assesses a model’s ability to distinguish **creative license** from factual hallucination, alongside its capacity for **cross-modal retrieval** within artistic and figurative contexts.

**Evaluation Context:**
The evaluation is **multimodal**, testing **Vision-Language Models (VLMs)** through a hybrid of **human judgment** (4-point Likert scales for grounding, Fleiss’ kappa for agreement) and **automatic metrics** (Recall@k, Average Rank). It focuses on the model's ability to bridge concrete visual pixels with high-level, figurative linguistic expressions.

**Task Characteristics:**
Tasks center on **evaluation/judgment** of generated captions and **cross-domain retrieval** (e.g., matching poems to images or visual metaphors to linguistic ones). The framework emphasizes **understanding and discriminative matching**—such as choosing metaphorical visualizations over literal ones—rather than open-ended generation, focusing on the **refinement** of visual-linguistic associations.",a15e0f13aecab1f376371cfa46f96a4c8d58434e:MSCOCO with Abstract Visual Associations and Creative Captions,MSCOCO with Abstract Visual Associations and Creative Captions,https://github.com/ananya-sahu/mining_visual_associations,2.0,GPU/Local,2.0,False,False,True,github,openai; google,multimodal
a85b39c1cb0ac7baad733aa6aab2c281a52485bf,Generation of Punning Riddles in Portuguese with Prompt Chaining,Generation of Punning Riddles in Portuguese with Prompt Chaining,2024.0,2024.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving; Figurative Language & Rhetoric,high,"The paper focuses on punning riddles and the detection of wordplay and incongruity, which are central to humor, while also involving the structural logic of riddles and the use of non-literal linguistic devices.","**Creative Artifacts Evaluated:**
Portuguese-language punning riddles and their corresponding non-humorous ""micro-edits""—sentences where minimal lexical changes remove the humorous element while maintaining structural and syntactic similarity to the original pun.

**Creative Capabilities Assessed:**
Linguistic humor recognition and semantic sensitivity. The benchmark measures a model’s ability to detect wordplay, phonetic ambiguity, and intentional incongruity. It specifically assesses the capacity to distinguish between creative linguistic subversion (the ""punchline"" mechanism) and literal, non-humorous communication, testing the model's grasp of the nuances required for computational creativity.

**Evaluation Context:**
A text-based assessment within a low-resource linguistic framework (Portuguese). The evaluation utilizes the Puntuguese benchmark, a manually curated dataset designed for high-difficulty discrimination. Performance is quantified using automatic F1-scores to evaluate Large Language Models (LLMs) on their ability to classify humor versus non-humor in a binary format.

**Task Characteristics:**
A constrained evaluation and judgment task. It focuses on binary classification rather than open-ended generation, requiring the model to act as a critic of creative output. The micro-editing methodology shifts the task from general sentiment analysis to a precise structural analysis of how subtle changes impact the creative and humorous value of a text.",a85b39c1cb0ac7baad733aa6aab2c281a52485bf:Puntuguese,Puntuguese,https://huggingface.co/datasets/Superar/Puntuguese,1.0,API-Only,1.0,True,True,True,huggingface,openai; google; anthropic,text
a8114398ad583ec38da97c1652bccc88168080d7,CrowdCounter: A benchmark type-specific multi-target counterspeech dataset,CrowdCounter: A benchmark type-specific multi-target counterspeech dataset,2024.0,2024.0,,Dialogue Generation & Social Interaction,Functional & Professional Writing; Figurative Language & Rhetoric; Humor & Satire,high,"The paper focuses on generating strategy-driven, nuanced responses to hate speech, emphasizing social nuance, rhetorical effectiveness, and specific creative tones like humor and empathy within interactive social media contexts.","**Creative Artifacts Evaluated:**
Nuanced, type-specific counterspeech responses designed to mitigate hate speech. These include creative rhetorical outputs such as humorous rebuttals, shaming-based corrections, empathetic interventions, and structured logical counter-arguments tailored to specific targets.

**Creative Capabilities Assessed:**
The benchmark measures stylistic flexibility and divergent thinking through semantic diversity and novelty metrics. It assesses rhetorical effectiveness via argument quality and counterspeech-specific quality scores. Crucially, it evaluates the ability to perform constrained creative synthesis—the capacity to adhere to specific emotional or strategic tones (e.g., humor vs. empathy) while maintaining linguistic fluency, readability, and non-toxic content.

**Evaluation Context:**
This text-based domain utilizes a hybrid evaluation framework. It combines traditional automatic NLP metrics (BLEURT, GRUEN, GLEU) with LLM-as-a-judge architectures (GPT-4 and Flan-T5) to assess strategy adherence and precision. Human ratings provide a ground-truth for subjective qualities like creativity and impact within the high-stakes context of social media moderation.

**Task Characteristics:**
The work spans open-ended generation (vanilla prompts) and highly constrained generation (type-specific prompts). It also includes discriminative tasks, such as multi-label classification of creative strategies, and comparative dataset analysis focusing on readability and linguistic complexity across different social media platforms (Gab vs. Reddit).",a8114398ad583ec38da97c1652bccc88168080d7:CROWDCOUNTER,CROWDCOUNTER,https://github.com/hate-alert/CrowdCounter,2.0,GPU/Local,3.0,False,False,True,github,none,text
a972b30e9c8e3cc4eedbd6f49df5c774cced0662,DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding,DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding,2024.0,2024.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper focuses on the comprehension of Chinese jokes (Duanzi) and puns, which directly aligns with Humor & Satire, while the analysis of slang and phonetic ambiguity involves Figurative Language & Rhetoric.","**Creative Artifacts Evaluated:**
The research focuses on linguistic artifacts specifically within the Chinese language, including homophonic puns, slang expressions, and ""Duanzi"" (short, witty jokes). These artifacts are characterized by phonetic ambiguity, cultural-specific wordplay, and non-standard linguistic structures.

**Creative Capabilities Assessed:**
The study assesses linguistic creativity and semantic flexibility, specifically the ability to decode double meanings and resolve phonetic incongruity. It measures conceptual depth and cultural competence through the model's capacity to bridge literal text with humorous subtext and the specific social nuances inherent in modern slang.

**Evaluation Context:**
This is a text-based evaluation of Large Language Models (LLMs) using a new specialized dataset. It employs a hybrid scoring methodology: automatic metrics (Exact/Similar Match Accuracy, Precision, Recall, and F1-score) are used for structural punchline identification, while subjective human scoring (on a 0-100 scale) evaluates the qualitative depth of humor comprehension and explanation.

**Task Characteristics:**
Tasks involve a mix of constrained recognition (Punchline Entity Recognition) and interpretive judgment (Comprehensive Humor Understanding). The framework tests the model's ability to perform cross-domain mapping between standard linguistic rules and the creative, often subversive, logic of humor across 0-shot and few-shot prompts.",a972b30e9c8e3cc4eedbd6f49df5c774cced0662:Chinese Homophonic Puns and Slang Dataset,Chinese Homophonic Puns and Slang Dataset,https://github.com/YesianRohn/DuanzAI,3.0,Special,2.0,False,False,True,github,openai,text
a8d81d868f408615255a253e8566ba05b9bc2ea6,Creative Agents: Empowering Agents with Imagination for Creative Tasks,Creative Agents: Empowering Agents with Imagination for Creative Tasks,2023.0,2023.0,,"3D, Spatial & Architectural Design",Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating 3D voxel-based architectural structures in Minecraft, emphasizing spatial reasoning and structural integrity, while also assessing the 'imagination' and novelty of the generated solutions.","**Creative Artifacts Evaluated:**
The primary artifacts are 3D voxel-based architectural structures and environmental designs generated within the Minecraft sandbox environment. These structures are produced from 20 distinct, open-ended natural language instructions ranging from simple dwellings to complex thematic builds.

**Creative Capabilities Assessed:**
The benchmark evaluates structural correctness (alignment with text prompts), architectural complexity (intricacy of the build), aesthetic quality (visual appeal), and functional utility. Crucially, it measures the agent's capacity for novelty and diversity, assessing whether the ""imagination"" mechanism allows for unique creative expressions beyond standard training patterns.

**Evaluation Context:**
This is a multimodal, embodied AI setting involving text-to-3D-action. Evaluation is conducted through a hybrid approach: automated Vision-Language Model (VLM) assessment using GPT-4V and validation via human judges. Scoring includes both relative pairwise comparisons (to calculate Elo ratings) and absolute direct scoring on a 0-10 scale across multiple qualitative dimensions.

**Task Characteristics:**
Tasks consist of open-ended, instruction-based generation within a high-dimensional, constrained environment. The benchmark requires translating abstract linguistic concepts into concrete spatial configurations. It also incorporates meta-evaluative tasks where models must judge and rank the creativity of generated artifacts, distinguishing between generative and discriminative creative intelligence.",a8d81d868f408615255a253e8566ba05b9bc2ea6:Minecraft Building Creation,Minecraft Building Creation,https://doi.org/10.5281/zenodo.10275049,3.0,Special,4.0,False,False,True,url,openai,multimodal
a9fcbfe0ef7c3ba4787890995d7e0231abc9f6cf,ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges,ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges,2025.0,2025.0,,Scientific Discovery,Mathematical Reasoning; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the synthesis of complex, real-world scenarios into novel mathematical models and methodologies across various scientific and social disciplines, emphasizing innovativeness and cross-domain ideation over pure formal proof construction.","**Creative Artifacts Evaluated:**
The benchmark evaluates comprehensive mathematical modeling reports, including LaTeX-formatted problem formulations, algorithmic designs, data-driven analysis strategies, and interdisciplinary solution frameworks. These artifacts represent the synthesis of qualitative real-world descriptions into formal, quantitative structures.

**Creative Capabilities Assessed:**
Assessment focuses on ""Innovativeness"" in model design, measuring the ability to perform divergent thinking across scientific and social domains. Key capabilities include conceptual depth in abstracting reality, structural coherency in complex argumentation, and the groundedness of theoretical models in practical data constraints and real-world applicability.

**Evaluation Context:**
Evaluation is text-centric but high-complexity, utilizing the ""ModelingJudge"" framework for automated LLM-based scoring. This is uniquely supplemented by human-led arena-style rankings and a Turing Test, which specifically compares LLM-generated solutions against high-level human entries from mathematical modeling competitions to determine qualitative indistinguishability.

**Task Characteristics:**
Tasks consist of open-ended, unconstrained problem-solving challenges that require significant cross-domain transfer. Unlike standard math benchmarks, these tasks emphasize the ideation of novel methodologies and the structural synthesis of multi-variable information, moving from vague real-world challenges to rigorous, formal mathematical abstractions.",a9fcbfe0ef7c3ba4787890995d7e0231abc9f6cf:ModelingBench,ModelingBench,https://github.com/qiancheng0/ModelingAgent,1.0,API-Only,1.0,True,False,True,github,openai; google,text
ab1ed465950ba86bf1c3d50c846dcff67d5bb1fd,AGENT-DRIVEN LARGE LANGUAGE MODELS FOR MANDARIN LYRIC GENERATION,AGENT-DRIVEN LARGE LANGUAGE MODELS FOR MANDARIN LYRIC GENERATION,2024.0,2024.0,,Music & Auditory Arts,Poetry & Verse,high,"The paper focuses on the synthesis of Mandarin lyrics specifically designed to align with musical melodies and rhythmic structures, which falls under Music & Auditory Arts, while also adhering to the metrical and structural constraints typical of Poetry & Verse.","**Creative Artifacts Evaluated:**
The primary artifacts are Mandarin lyrics generated to fit pre-existing melodies. The evaluation also utilizes a statistical corpus of human-composed Mandarin pop songs (Mpop600) to establish benchmarks for lyric-melody alignment.

**Creative Capabilities Assessed:**
The research measures the ability to adhere to strict structural constraints, specifically character count accuracy per musical phrase. It assesses prosodic alignment—the mapping of linguistic rhythm to musical melody—and overall aesthetic quality, including fluency and emotional resonance, through comparative human ranking.

**Evaluation Context:**
This is a multimodal evaluation involving text and melody. It employs a hybrid scoring approach: automatic metrics for technical constraints (Exact Match Rate) and statistical alignment (Segment Match Rate), alongside human-centric listening tests. The context specifically examines the efficacy of multi-agent LLM architectures in collaborative creative workflows.

**Task Characteristics:**
The core task is constrained, cross-modal generation, requiring the model to map musical structures to linguistic patterns. It involves melody-to-lyric synthesis, where the LLM must balance technical adherence (syllable-to-note mapping) with creative expression. The process emphasizes iterative refinement and role-playing through multi-agent interaction to improve output quality.",ab1ed465950ba86bf1c3d50c846dcff67d5bb1fd:Listening Test for Agent-based Lyric Generation,Listening Test for Agent-based Lyric Generation,,2.0,GPU/Local,1.0,False,False,True,request_access,openai,multimodal
ab3e7796a24ae2ce235dd5571c02870c6272e799,Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese,Culturally-Nuanced Story Generation for Reasoning in Low-Resource Languages: The Case of Javanese and Sundanese,2025.0,2025.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,"The paper's core focus is on the generation and evaluation of short-form narrative prose (five-sentence stories), specifically assessing narrative coherence and resolution, while also utilizing the Story Cloze task to measure commonsense and cross-lingual reasoning.","**Creative Artifacts Evaluated:**
Short-form narrative prose, specifically five-sentence stories written in Javanese and Sundanese. The artifacts consist of culturally specific story premises and their corresponding concluding sentences.

**Creative Capabilities Assessed:**
The benchmark evaluates narrative coherence, logical plausibility, and linguistic fluency within low-resource language contexts. It specifically measures commonsense reasoning and the ability to generate endings that are culturally nuanced and contextually appropriate, moving beyond simple grammatical correctness to assess narrative resolution and situational logic.

**Evaluation Context:**
This text-based evaluation focuses on Large Language Models (LLMs) performing in underrepresented linguistic domains. Scoring utilizes a hybrid approach: automatic metrics (ROUGE-L, METEOR, BERTScore) measure lexical and semantic alignment, while human-centric pairwise preference testing captures qualitative nuances and cultural authenticity that automated tools often overlook in low-resource settings.

**Task Characteristics:**
The work employs a dual-task structure: discriminative judgment (Story Cloze Classification) and constrained generation (Story Generation). These tasks require models to demonstrate both an understanding of narrative trajectory and the ability to ideate a plausible conclusion based on a fixed four-sentence premise, emphasizing cultural grounding and cross-lingual reasoning.",ab3e7796a24ae2ce235dd5571c02870c6272e799:Javanese and Sundanese Story Cloze,Javanese and Sundanese Story Cloze,https://arxiv.org/abs/2502.12932,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic; google; huggingface,text
ad0c8f7b318178626baceb82330cacb8742ac54b,Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification,Concept Drift Guided LayerNorm Tuning for Efficient Multimodal Metaphor Identification,2025.0,2025.0,,Figurative Language & Rhetoric,Humor & Satire,high,"The paper's core focus is the identification of multimodal metaphors and cross-domain mappings, which are central to figurative language, while using internet memes as the primary artifact for evaluating cultural subtext and social nuance.","**Creative Artifacts Evaluated:**
The research focuses on internet memes, which are multimodal artifacts comprising specific pairings of visual imagery (photographs, illustrations, or screen captures) and overlaid or associated textual captions.

**Creative Capabilities Assessed:**
The primary capability assessed is conceptual depth, specifically the ability to decode figurative language through multimodal metaphor identification. The work also measures the model’s capacity to interpret emotional impact (sentiment analysis), social nuance (offensiveness detection), and underlying communicative intent (intention detection), reflecting a sophisticated grasp of cultural subtext and creative expression.

**Evaluation Context:**
Evaluation is conducted in a multimodal context using Vision-Language Models (VLMs) to process the interplay between text and images. Performance is quantified through automatic scoring metrics, specifically Accuracy and Weighted F1-score. The focus is on the model's proficiency in understanding and categorizing pre-existing creative content rather than generating new artifacts.

**Task Characteristics:**
The tasks are discriminative and judgment-based, requiring multimodal classification. They focus on the identification of concept drift and cross-domain mappings, where the model must navigate the semantic shift between literal imagery and figurative meaning. This distinguishes the work as an evaluation of creative understanding and figurative interpretation within constrained, high-context social media formats.",ad0c8f7b318178626baceb82330cacb8742ac54b:MET-Meme,MET-Meme,https://github.com/liaolianfoka/MET-Meme-A-Multi-modal-Meme-Dataset-Rich-in-Metaphors,2.0,GPU/Local,4.0,False,False,True,github,none,multimodal
a845a0b142944b02e1a55ce0dfec8519b9f3f58c,Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation,Aggregated Structural Representation with Large Language Models for Human-Centric Layout Generation,2025.0,2025.0,,Graphic Design & Visual Layout,,high,"The paper focuses on the spatial arrangement and hierarchical composition of UI and document elements, which is the core definition of the Graphic Design & Visual Layout domain.","**Creative Artifacts Evaluated**
The research focuses on structured graphic designs, specifically mobile user interface (UI) layouts, multi-column document layouts, and stylized magazine page layouts. These artifacts are defined by hierarchical and spatial relationships between functional elements such as buttons, icons, text blocks, and images.

**Creative Capabilities Assessed**
Assessed capabilities include spatial reasoning, structural integrity, and design fluency. The evaluation measures the model’s ability to minimize relational errors and element overlap (technical correctness) while maximizing alignment and distributional realism (FID). Human-centric ""reasonableness"" scores further assess the aesthetic and functional viability of the generated spatial arrangements.

**Evaluation Context**
The study utilizes a text-to-layout framework where Large Language Models (LLMs) process aggregated structural representations. Evaluation is a hybrid of automatic geometric metrics (mIoU, Relation Error) and subjective human judgment. This context emphasizes the translation of abstract, graph-based structural constraints into precise coordinate-based visual designs.

**Task Characteristics**
Tasks encompass the full design lifecycle: open-ended generation (from scratch), constrained generation (layout completion), and iterative refinement (graph editing). This variety tests both divergent thinking in layout ideation and convergent thinking when adhering to user-defined nodes and relational constraints, highlighting the model's adaptability to interactive design workflows.",a845a0b142944b02e1a55ce0dfec8519b9f3f58c:RICO,RICO,,2.0,GPU/Local,3.0,False,True,True,huggingface,none,multimodal
ab093afc8fc9619a3331b7e59539f67707e78f53,Using Multimodal Foundation Models and Clustering for Improved Style Ambiguity Loss,Using Multimodal Foundation Models and Clustering for Improved Style Ambiguity Loss,2024.0,2024.0,,Visual Arts & Stylized Imagery,Programming & Algorithmic Creativity,high,"The paper focuses on optimizing text-to-image diffusion models for stylistic novelty and aesthetic quality, utilizing algorithmic loss functions and foundation models to evaluate and enhance visual artistic outputs.","**Creative Artifacts Evaluated:**
Text-to-image diffusion-generated visual outputs, specifically focusing on images where stylistic execution and artistic deviation are the primary variables of interest.

**Creative Capabilities Assessed:**
The research evaluates aesthetic quality (visual appeal), human preference alignment (subjective utility), and stylistic novelty. A central capability assessed is ""style ambiguity,"" which measures the model's ability to deviate from established stylistic norms—a proxy for originality—while maintaining high prompt faithfulness (semantic utility).

**Evaluation Context:**
This work operates within a multimodal (text-to-image) framework, utilizing a suite of automatic evaluation metrics as proxies for human perception. It leverages pre-trained foundation models—specifically CLIP for semantic alignment, DINO for stylistic feature extraction, and specialized reward models—to quantify complex creative attributes in a scalable, automated manner.

**Task Characteristics:**
The tasks center on the evaluation and judgment of generated artifacts. The research focuses on constrained generation (prompt-following) paired with stylistic refinement. It specifically investigates the trade-off between adhering to semantic constraints and achieving creative ""style ambiguity,"" aiming to optimize generative models for increased stylistic diversity and novelty through the measurement of stylistic deviation from baseline norms.",ab093afc8fc9619a3331b7e59539f67707e78f53:AVA Score,AVA Score,https://github.com/jamesBaker361/clipcreate,2.0,GPU/Local,1.0,False,True,True,huggingface,none,image
b0b692bd662cc54c9925bd9fb30c4458ab6bb6e4,Probing the Limits of Stylistic Alignment in Vision-Language Models,Probing the Limits of Stylistic Alignment in Vision-Language Models,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper specifically evaluates the generation of humorous captions for cartoons and photographs, while also addressing romantic sentiment and non-literal stylistic alignment.","**Creative Artifacts Evaluated:**
The research evaluates short-form creative text in the form of humorous captions for abstract, single-panel cartoons (New Yorker) and both humorous and romantic captions for naturalistic photographs (FlickrStyle10k). These artifacts represent a synthesis of visual interpretation and specific linguistic personas.

**Creative Capabilities Assessed:**
The primary capability assessed is stylistic alignment—the model's ability to adopt specific emotional or tonal registers while maintaining relevance to visual stimuli. This involves measuring the model’s grasp of humor, wit, and romantic sentiment. The evaluation focuses on the success of this alignment through stylistic accuracy (classification) and preference (log-probability win rates), testing the model's ability to produce nuanced, non-literal interpretations of imagery.

**Evaluation Context:**
This is a multimodal (image-to-text) evaluation of Vision-Language Models (VLMs). Diverging from human-centric creativity assessments, this work utilizes automated metrics: a binary style classifier to detect the presence of the target style and length-normalized log-probabilities (WR-LogP) to determine the model's internal preference for stylistic outputs over neutral ones.

**Task Characteristics:**
The tasks consist of constrained, open-ended generation. Models must bridge the gap between visual content and a specific creative persona, moving beyond descriptive captioning. The focus is on the ""alignment"" phase of model training, exploring how well VLMs can be steered toward subjective, creative domains through prompting or fine-tuning.",b0b692bd662cc54c9925bd9fb30c4458ab6bb6e4:FlickrStyle10k,FlickrStyle10k,https://github.com/zhegan27/StyleNet,2.0,GPU/Local,2.0,False,False,True,github,openai,multimodal
aba38a6352977bb696346773c1418f7a2bb1d400,WeGen: A Unified Model for Interactive Multimodal Generation as We Chat,WeGen: A Unified Model for Interactive Multimodal Generation as We Chat,2025.0,2025.0,,Visual Arts & Stylized Imagery,Dialogue Generation & Social Interaction,high,"The paper focuses on the generation and evaluation of photorealistic and composite images, while the interactive 'as We Chat' context integrates these visual tasks into a conversational framework.","**Creative Artifacts Evaluated**
The research evaluates photorealistic images generated from text, subject-driven images (re-contextualizing specific objects or individuals), and complex multi-subject composite images featuring 2-3 distinct human faces integrated into a single coherent scene.

**Creative Capabilities Assessed**
Assessment focuses on divergent thinking (generating diverse visual outputs from identical prompts) and compositional creativity. Key capabilities include identity preservation (maintaining subject fidelity across novel contexts), prompt alignment, and the ability to synthesize multiple distinct identities without visual blending or loss of coherence. Technical quality is measured alongside the model's capacity for creative re-contextualization.

**Evaluation Context**
This study utilizes a unified multimodal model evaluated through automatic metrics. Image quality and alignment are measured via FID and CLIP-T, while diversity is quantified using PSNRd and LPIPSd. Specialized metrics, including DINO, CLIP-I, and Face Similarity scores, are employed to assess the precision of subject-driven and multi-subject generation tasks.

**Task Characteristics**
Tasks range from open-ended text-to-image synthesis to highly constrained generation. A distinguishing feature is the introduction of a new multi-subject benchmark, requiring the model to perform complex spatial and identity-based reasoning. The framework emphasizes interactive generation, balancing constrained identity preservation with the creative freedom to generate novel environments and compositions.",aba38a6352977bb696346773c1418f7a2bb1d400:COCO2014,COCO2014,https://github.com/hzphzp/WeGen,2.0,GPU/Local,2.0,False,True,True,huggingface,none,multimodal
add949bdb6beb0dfd7d54efd15db23e79c72e3b8,Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation,Luminate: Structured Generation and Exploration of Design Space with Large Language Models for Human-AI Co-Creation,2023.0,2023.0,,Functional & Professional Writing,Narrative & Story Writing; Poetry & Verse; Music & Auditory Arts; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on a co-creation tool for professional workflows such as marketing and correspondence, while also evaluating its effectiveness in divergent thinking and ideation across narrative, poetic, and lyrical domains.","**Creative Artifacts Evaluated:**
The system evaluates diverse text-based creative outputs, including marketing copy (e-commerce, travel, and health), speculative fiction short stories, professional and personal correspondence, poetry, song lyrics, and narrative video game scene descriptions.

**Creative Capabilities Assessed:**
The research focuses on divergent thinking and the structured exploration of design spaces. It assesses the system’s ability to provide creativity support—measured via the Creativity Support Index (CSI)—focusing on usability, usefulness, and the effectiveness of AI-driven ideation in professional workflows.

**Evaluation Context:**
This is a human-centric, text-based evaluation involving professional writers. Scoring is entirely qualitative and subjective, utilizing Likert scales, semi-structured interviews, and think-aloud protocols. The study design is distinctive for combining controlled laboratory tasks with a longitudinal, two-week ""in-the-wild"" study to observe real-world co-creation.

**Task Characteristics:**
Tasks involve open-ended generation and iterative ideation. They range from constrained professional writing (copywriting, emails) to highly expressive creative works (poetry, fiction). A defining characteristic is the shift from simple prompt-response generation to the structured navigation and refinement of a multi-dimensional design space.",add949bdb6beb0dfd7d54efd15db23e79c72e3b8:In-the-Wild Study,In-the-Wild Study,https://github.com/AI-Sketchpad/Luminate,3.0,Special,1.0,False,False,True,unknown,openai,text
b3118180a57ceebb81fa963e0d372b2078ee59fa,Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers,Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Scientific Discovery,high,"The paper specifically evaluates 'divergent thinking' and 'factual ideation' through semantic originality and clustering, which are key components of lateral thinking, while the study's focus on the internal architectural mechanics of LLM layers aligns with scientific discovery in the field of AI.","**Creative Artifacts Evaluated**
The artifacts consist of semantically diverse textual responses to open-domain factual queries. Rather than long-form narratives, these are multiple, distinct, and valid short-form answers (e.g., varied naming conventions, synonyms, or different correct entities) generated from filtered subsets of TriviaQA and Natural Questions.

**Creative Capabilities Assessed**
The benchmark primarily measures **divergent thinking** within a factual constraint. It assesses **semantic originality**—defined as the count of unique semantic clusters among correct answers—and **technical correctness** (the inverse of hallucination). The assessment focuses on the model's ability to navigate the trade-off between ""creative"" exploration of the latent space and ""hallucinatory"" factual errors.

**Evaluation Context**
This is an automatic, text-based evaluation of LLMs using the HCL (Hallucination-Creativity Layerwise) framework. It utilizes semantic clustering to quantify creativity and error-rate analysis for hallucinations across internal decoding layers. The context is uniquely architectural, focusing on the internal mechanics of model layers rather than external human subjective judgment.

**Task Characteristics**
The tasks involve **constrained open-ended generation**. Unlike standard QA, which seeks a single ""gold"" response, these tasks require **factual ideation**, where the model must produce a variety of truthful outputs. This involves a form of **retrieval-based divergent thinking** and **semantic refinement** as information is processed through successive transformer layers.",b3118180a57ceebb81fa963e0d372b2078ee59fa:HCL (Hallucination-Creativity Layerwise),HCL (Hallucination-Creativity Layerwise),https://github.com/ZicongHe2002/HCL-Spark,2.0,GPU/Local,2.0,False,False,True,github,none,text
b1d55fcdd68695671e566a83f1873da6c4eb147d,MusicSwarm: Biologically Inspired Intelligence for Music Composition,MusicSwarm: Biologically Inspired Intelligence for Music Composition,2025.0,2025.0,,Music & Auditory Arts,Programming & Algorithmic Creativity,high,The paper focuses on the generation and evaluation of symbolic and audio music compositions using biologically inspired algorithmic intelligence and complex systems theory.,"**Creative Artifacts Evaluated:**
The benchmark evaluates symbolic and audio-rendered two-voice piano compositions. These artifacts range from 8-bar pieces constrained by specific keys (e.g., A harmonic minor) to 16-bar compositions centered on thematic metaphors like biological growth.

**Creative Capabilities Assessed:**
Assessment focuses on musical novelty and ""creative risk-taking"" through metrics of expectation violation and surprise density. It measures structural coherence via tonal stability and tension architecture, alongside ""organic"" complexity using graph-theoretic analysis of self-similarity networks to determine small-worldness and multi-scale community persistence.

**Evaluation Context:**
This is a multimodal automatic evaluation framework that bridges symbolic MIDI-level data with audio signal processing (spectral entropy, harmonic tension, and spectrogram analysis). It distinguishes itself by utilizing complex systems theory and information-theoretic metrics rather than relying on subjective human or LLM-based qualitative scoring.

**Task Characteristics:**
Tasks involve constrained open-ended generation. Models must navigate specific music-theoretic requirements and abstract thematic prompts, shifting the focus from simple melodic imitation to the synthesis of long-range structural integrity and multi-layered harmonic variety. This requires balancing adherence to tonal rules with the generation of non-obvious, emergent musical patterns.",b1d55fcdd68695671e566a83f1873da6c4eb147d:MusicSwarm Composition Benchmark,MusicSwarm Composition Benchmark,https://github.com/lamm-mit/MusicSwarm,1.0,API-Only,2.0,True,False,True,github,openai,multimodal
afcb783b1e67c6d4afcb8678246173472ccd1144,MuseScorer: Idea Originality Scoring At Scale,MuseScorer: Idea Originality Scoring At Scale,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,,high,"The paper focuses on evaluating the Alternate Uses Test (AUT), which is a foundational psychometric assessment for lateral thinking, divergent thinking, and the ability to bypass functional fixedness.","**Creative Artifacts Evaluated**
The artifacts consist of short-form textual responses generated during the Alternate Uses Test (AUT). These are discrete ""idea units"" describing novel, non-obvious applications for common physical objects (e.g., using a brick as a coffin for a pet or a paperweight).

**Creative Capabilities Assessed**
The primary focus is on divergent thinking, specifically originality (statistical infrequency and novelty) and semantic flexibility. The benchmarks evaluate the system’s ability to perform ""idea bucketing""—grouping semantically equivalent responses—and its capacity to assign originality scores that align with human expert judgment and established psychometric measures of creativity.

**Evaluation Context**
The research operates in a text-based psychometric context, utilizing Large Language Models (LLMs) to automate the scoring of legacy datasets (e.g., beaty18, silvia17). Evaluation involves comparing AI-generated scores against human ground truth using Pearson’s r, Spearman’s ρ, and Intraclass Correlation Coefficients (ICC), alongside clustering metrics like Adjusted Mutual Information (AMI) for bucketing accuracy.

**Task Characteristics**
The tasks involve constrained ideation and automated evaluation/judgment. Specifically, the work focuses on the semantic clustering of open-ended responses and the subsequent scoring of those clusters to determine the rarity of an idea within a population, transitioning creativity assessment from manual coding to scalable algorithmic profiling.",afcb783b1e67c6d4afcb8678246173472ccd1144:socialmuse24,socialmuse24,https://github.com/cssai-research/MuseScorer,2.0,GPU/Local,1.0,False,False,True,github,openai; huggingface,text
b5403b14ae498e66eb19a110277f55852a3bed3d,Probing and Inducing Combinational Creativity in Vision-Language Models,Probing and Inducing Combinational Creativity in Vision-Language Models,2025.0,2025.0,,Visual Arts & Stylized Imagery,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the synthesis and interpretation of visual conceptual blends (mashups), requiring a combination of artistic expression, metaphorical reasoning, and lateral thinking to fuse disparate objects.","**Creative Artifacts Evaluated:**
Visual mashups; specifically, artist-generated and model-synthesized images that fuse two distinct, disparate objects into a single, cohesive conceptual blend.

**Creative Capabilities Assessed:**
Combinational creativity and conceptual blending. The benchmark measures the ability to identify constituent objects in non-standard contexts, recognize shared structural or functional attributes (bisociation), and interpret deeper semantic themes or symbolic meanings. Generative capabilities are assessed based on novelty and the effectiveness of conveying specific messages through visual synthesis.

**Evaluation Context:**
A multimodal assessment of Vision-Language Models (VLMs) and image generators. Evaluation is conducted via a hybrid pipeline: GPT-4o serves as an automated judge for object matching and pairwise comparisons (win rates), while human experts provide the gold standard for ranking generative novelty and thematic coherence against professional artist benchmarks.

**Task Characteristics:**
The framework integrates discriminative understanding, interpretive reasoning, and constrained generation. Tasks range from zero-shot identification and attribute explanation to theme-driven image synthesis. It distinguishes itself by requiring models to not only generate creative content but also articulate the underlying logic and metaphorical implications of visual blends, moving beyond simple image-to-text tagging.",b5403b14ae498e66eb19a110277f55852a3bed3d:CreativeMashup,CreativeMashup,https://github.com/PPYYQQ/aicc-code,1.0,API-Only,4.0,True,False,True,github,openai,multimodal
b56adfa2a0ddcaee3201c800ea4e9f5ba5abe66c,Materials Transformers Language Models for Generative Materials Design: a benchmark study,Materials Transformers Language Models for Generative Materials Design: a benchmark study,2022.0,2022.0,,Scientific Discovery,Engineering & Technical Design,high,"The paper focuses on the generation and discovery of novel inorganic material compositions using language models, emphasizing technical feasibility, originality, and functional utility within the STEM field of materials science.","**Creative Artifacts Evaluated:**
The primary artifacts are **inorganic material compositions** (chemical formulas) representing potential solid-state structures. These are treated as structured textual sequences that map to physical material properties within a defined chemical space.

**Creative Capabilities Assessed:**
The benchmark measures **technical correctness** through chemical validity (charge neutrality and electronegativity balance) and **functional utility** via thermodynamic stability (predicted formation energy). It assesses **originality** (novelty relative to known databases like ICSD or Materials Project), **diversity** (uniqueness and distributional coverage via t-SNE), and **goal-directed design** (the ability to generate materials with specific electronic properties, such as high bandgaps).

**Evaluation Context:**
This is a **text-based generative task** where LLMs are repurposed for scientific discovery. Evaluation is strictly **automatic and physics-informed**, utilizing machine learning surrogate models for rapid screening and high-fidelity **Density Functional Theory (DFT) calculations** to provide ground-truth validation of candidate stability and electronic properties.

**Task Characteristics:**
The study involves both **open-ended generation** (de novo discovery) and **constrained generation** (property-conditioned design). It emphasizes the transition from pattern recognition in existing chemical data to the **ideation** of physically plausible yet undiscovered materials, framing materials design as a specialized form of constrained linguistic creativity.",b56adfa2a0ddcaee3201c800ea4e9f5ba5abe66c:Generative Materials Design Benchmark,Generative Materials Design Benchmark,https://figshare.com/articles/dataset/MT_dataset/20122796,2.0,GPU/Local,2.0,False,False,True,url,none,text
b82c1b0512d25307e3c81bb8d9df1607267a7a52,MemeCap: A Dataset for Captioning and Interpreting Memes,MemeCap: A Dataset for Captioning and Interpreting Memes,2023.0,2023.0,,Humor & Satire,Figurative Language & Rhetoric; Visual Arts & Stylized Imagery,high,"The paper explicitly focuses on interpreting internet memes, which are a core component of the Humor & Satire domain, and requires decoding visual metaphors and non-literal intent, which aligns with Figurative Language & Rhetoric.","**Creative Artifacts Evaluated:**
The dataset focuses on internet memes, specifically image-text pairings that utilize visual metaphors, cultural references, and irony. The generated outputs are concise textual interpretations or ""captions"" that decode the underlying humor, intent, and metaphorical meaning of the meme.

**Creative Capabilities Assessed:**
The benchmark measures the ability to interpret non-literal language and synthesize complex visual-textual metaphors. It assesses conceptual depth and the capacity to bridge the gap between literal visual elements and abstract cultural jokes. This requires high-level semantic reasoning and the ability to recognize humor and irony, moving beyond standard object recognition to understand creative intent.

**Evaluation Context:**
This is a multimodal evaluation involving Vision-Language Models (VLMs) and LLMs. The scoring employs a hybrid approach: automatic linguistic metrics (BLEU-4, ROUGE-L, BERT-F1) are combined with nuanced human ratings. Human evaluators judge the outputs based on correctness, appropriate length, faithfulness, and both visual and textual completeness.

**Task Characteristics:**
The primary task is open-ended ""interpretive generation."" It requires cross-modal synthesis and a form of cross-domain transfer, where the model must translate a creative, indirect communication style (the meme) into a direct, explanatory textual modality. It functions as a test of ""creative intelligence"" through the lens of cultural comprehension.",b82c1b0512d25307e3c81bb8d9df1607267a7a52:MEMECAP,MEMECAP,https://github.com/eujhwang/meme-cap,2.0,GPU/Local,1.0,False,False,True,github,none,multimodal
b74d2a4b205c66b9520b1c71e7a167977696e9f5,"PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts","PuzzleWorld: A Benchmark for Multimodal, Open-Ended Reasoning in Puzzlehunts",2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Mathematical Reasoning,high,"The paper's focus on 'insight-based' reasoning, 'aha!' moments, and discovering hidden structures in puzzles aligns perfectly with the definition of Lateral Thinking & Creative Problem Solving, while its use of logical proofs and technical correctness for evaluation justifies Mathematical Reasoning as a secondary domain.","**Creative Artifacts Evaluated:**
The benchmark evaluates the generation of structured solution paths and final answer strings derived from cryptic, multimodal inputs. These artifacts include deciphered text, identified visual patterns, and logical proofs required to bridge disparate, non-obvious clues.

**Creative Capabilities Assessed:**
Assessment focuses on ""insight-based"" reasoning and the ability to discover hidden problem structures without explicit instructions. It measures divergent thinking (exploring multiple interpretations of ambiguous data), convergent thinking (synthesizing clues into a single answer), and conceptual depth. The benchmark specifically tests the ""aha!"" moment—the transition from noise to a structured solution path.

**Evaluation Context:**
The context is a high-complexity multimodal environment involving text and images. Evaluation is conducted through a hybrid scoring system: objective accuracy for the final answer and a qualitative ""LLM-as-a-judge"" (GPT-4o) mechanism to assess the logical validity and technical correctness of intermediate reasoning steps.

**Task Characteristics:**
Tasks are defined by open-ended, multi-step problem-solving where the primary challenge is meta-reasoning—determining the rules of the puzzle before solving it. This requires complex pattern discovery and cross-domain transfer, blending linguistic, mathematical, and visual logic in a way that differentiates it from standard, instruction-led VQA tasks.",b74d2a4b205c66b9520b1c71e7a167977696e9f5:PuzzleWorld,PuzzleWorld,https://github.com/MIT-MI/PuzzleWorld,1.0,API-Only,1.0,True,True,True,huggingface,openai; anthropic; google,multimodal
b4eea0939283ed47552484a673c86cdf1cf268e1,"Investigating Wit, Creativity, and Detectability of Large Language Models in Domain-Specific Writing Style Adaptation of Reddit’s Showerthoughts","Investigating Wit, Creativity, and Detectability of Large Language Models in Domain-Specific Writing Style Adaptation of Reddit’s Showerthoughts",2024.0,2024.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving; Figurative Language & Rhetoric,high,"The paper evaluates 'Showerthoughts,' which are inherently witty and humorous observations that rely on subverting expectations through lateral thinking and rhetorical devices like paradoxes.","**Creative Artifacts Evaluated:**
The primary artifacts are ""Showerthoughts"": short-form, witty, and observational text snippets modeled after a specific Reddit community. These are characterized by their brevity, paradoxical nature, and humorous insights into everyday life, requiring a blend of logical validity and creative subversion of expectations.

**Creative Capabilities Assessed:**
The study measures wit, humor, cleverness, and logical validity. Beyond these, it assesses ""stylistic mimicry"" through authorship detectability and linguistic alignment, including lexical complexity (Flesch-Kincaid), grammatical accuracy, and semantic density (SBERT similarity) to determine how well models capture a niche human voice.

**Evaluation Context:**
This text-based framework compares human-authored content against LLM outputs (e.g., GPT-4). It employs a tripartite scoring system: human Likert-scale ratings (1–6), automatic linguistic metrics, and LLM-as-a-judge preference simulation. This setup specifically tests the correlation between machine-generated ratings and human subjective judgment in creative domains.

**Task Characteristics:**
Tasks center on domain-specific style adaptation and constrained generation. They include discriminative tasks (human and automatic authorship identification) and evaluative tasks that test the ability of models to both produce and critique creative, humorous content within a specific social media context, emphasizing the challenge of ""detectability"" in creative writing.",b4eea0939283ed47552484a673c86cdf1cf268e1:Showerthoughts Generation and Evaluation,Showerthoughts Generation and Evaluation,https://github.com/aiintelligentsystems/showerthoughts-dataset,3.0,Special,5.0,False,False,True,request_access,openai,text
b245926ae580ae35b52b9e816fcd53abf7268c76,APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents,APT: Architectural Planning and Text-to-Blueprint Construction Using Large Language Models for Open-World Agents,2024.0,2024.0,,"3D, Spatial & Architectural Design",Engineering & Technical Design; Programming & Algorithmic Creativity,high,The paper explicitly focuses on generating 3D architectural blueprints and voxel structures while evaluating structural integrity and functional mechanical systems like Redstone circuits.,"**Creative Artifacts Evaluated**
The benchmark evaluates 3D architectural blueprints and voxel-based structures within a simulated open world (Minecraft). Specific artifacts include furnished residential dwellings, geometric monuments using unconventional materials (snow/ice), functional mechanical systems (Redstone-powered lighting), and complex multi-story mansions with integrated landscaping and interior layouts.

**Creative Capabilities Assessed**
Assessment focuses on spatial reasoning, structural integrity, and functional logic. It measures the agent's capacity for divergent thinking through material innovation and convergent thinking via strict instruction following. Key metrics include architectural correctness, design complexity, aesthetic creativity, and the operational functionality of internal mechanisms.

**Evaluation Context**
This is a multimodal framework involving text-to-blueprint and image-to-blueprint generation. Evaluation utilizes a hybrid approach where a Vision-Language Model (GPT-4o) performs automated scoring on a 1–10 scale across four dimensions. These VLM-based assessments are validated against qualitative rankings from 22 human judges to ensure alignment with human perceptions of architectural quality.

**Task Characteristics**
Tasks encompass open-ended creative generation, constrained architectural design, and visual replication. The benchmark emphasizes ""architectural planning,"" requiring agents to translate high-level natural language or visual references into executable, low-level spatial coordinates and block-placement sequences, bridging the gap between conceptual ideation and physical construction.",b245926ae580ae35b52b9e816fcd53abf7268c76:APT Minecraft Construction Benchmark,APT Minecraft Construction Benchmark,https://github.com/spearsheep/APT-Architectural-Planning-LLM-Agent,3.0,Special,5.0,False,False,True,github,openai,multimodal
b89a6b5436bc8f159005731ef409bead597970a4,Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content,Deceptive Humor: A Synthetic Multilingual Benchmark Dataset for Bridging Fabricated Claims with Humorous Content,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper specifically focuses on evaluating satirical news and humorous content, requiring the decoding of rhetorical strategies like irony and absurdity.","**Creative Artifacts Evaluated:**
The benchmark evaluates multilingual text-based humorous content, specifically ""fabricated claims"" designed with satirical intent. These artifacts include satirical news snippets, ironic social commentary, and deceptive narratives that blend misinformation with comedic elements such as absurdity or dark humor.

**Creative Capabilities Assessed:**
The dataset assesses a model’s ability to decode complex rhetorical strategies and tonal nuances. Specifically, it measures the capacity to distinguish between varying levels of satirical intensity (Satire Level) and to identify specific creative mechanisms such as irony, absurdity, and social commentary (Humor Attributes). This requires high-level conceptual depth and an understanding of how humor is used to mask or highlight deceptive intent.

**Evaluation Context:**
The evaluation is conducted in a multilingual text-only environment using various Large Language Models. Performance is measured through automatic classification metrics (Accuracy, F1-Score, Precision, Recall) and Pearson Correlation to determine the alignment between model predictions and the nuanced scales of satirical content.

**Task Characteristics:**
The tasks are primarily discriminative and evaluative, focusing on the judgment of creative nuances rather than generation. They involve multi-class classification and attribute prediction, requiring models to perform cross-lingual analysis of how humor and deception intersect. This differentiates the work by focusing on the ""understanding"" of deceptive creativity across linguistic boundaries.",b89a6b5436bc8f159005731ef409bead597970a4:Deceptive Humor Dataset (DHD),Deceptive Humor Dataset (DHD),https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH8Bp53wskgDgylDNGTdQnUAgjq6WwHUfo9OjeQVdH7usmE57tsvoU29MBGCYx0sHPGzHaO1BKGoXhY8IiZo2W7Bh4c5oVF73YS39qTWalIheY8tyefvZQoRdJj716GDkmbNQ==,2.0,GPU/Local,2.0,False,False,True,request_access,openai; huggingface,text
b931ce81c7ced2cf7b5b049836be06af5c049c00,Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems,Assessing the Creativity of LLMs in Proposing Novel Solutions to Mathematical Problems,2024.0,2024.0,,Mathematical Reasoning,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating technically correct and novel mathematical proofs while intentionally avoiding known solution paths, which combines formal mathematical rigor with divergent, 'outside-the-box' problem-solving.","**Creative Artifacts Evaluated:**
The primary artifacts are **novel mathematical solutions and reasoning paths**. These consist of text-based proofs or step-by-step derivations that solve specific mathematical problems while intentionally avoiding pre-existing, known solution methods provided in the prompt.

**Creative Capabilities Assessed:**
The benchmark measures **technical correctness** (logical validity) and **originality** (novelty relative to a reference set). It specifically evaluates **divergent thinking** within a formal domain, focusing on the model's ability to identify alternative conceptual frameworks or techniques to reach a correct conclusion. Metrics like the ""Novel-Unknown Ratio"" further assess the capacity to generate solutions entirely absent from the provided knowledge base, testing the limits of the model's creative search space.

**Evaluation Context:**
This is a **text-based mathematical reasoning** task. Evaluation is conducted via a **multi-model LLM panel** (GPT-4, Claude 3.5 Sonnet, and Gemini 1.5 Pro). This approach uses a consensus mechanism—requiring a unanimous vote for correctness and a majority vote for novelty—to provide a scalable proxy for expert human judgment in a highly technical, objective domain.

**Task Characteristics:**
The core task is **novelty-constrained problem-solving**. It moves beyond simple generation to **constrained ideation**, where the model must navigate a formal search space to find valid solutions that do not overlap with $k$ provided examples, requiring a high degree of strategic flexibility and refinement.",b931ce81c7ced2cf7b5b049836be06af5c049c00:CREATIVEMATH,CREATIVEMATH,https://github.com/JunyiYe/CreativeMath,1.0,API-Only,1.0,True,False,True,github,openai; google,text
bb5f873632616c2cdc07ef1bb139db0c96c8e5f6,Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models,2024.0,2024.0,,Scientific Discovery,,high,"The paper's core focus is the generation and verification of biochemical hypotheses and biological mechanisms, which directly aligns with the Scientific Discovery domain's emphasis on novel research hypotheses in STEM fields.","**Creative Artifacts Evaluated:**
The primary artifacts are biochemical hypotheses that propose specific relationships (e.g., stimulation or inhibition) between two scientific entities. These hypotheses are supported by multi-step reasoning chains that articulate the underlying biological mechanisms or pathways.

**Creative Capabilities Assessed:**
The benchmark measures technical correctness and scientific plausibility by assessing the model's ability to ground novel ideas in existing knowledge. It evaluates conceptual depth and logical consistency through the verification of intermediate reasoning steps. The focus is on ""grounded creativity""—the capacity to generate useful, defensible scientific insights rather than purely divergent or speculative thought.

**Evaluation Context:**
This text-based evaluation utilizes Large Language Models (LLMs) within a specialized biochemical framework. Scoring is a hybrid ""Confidence"" metric that combines traditional accuracy with a novel verification pipeline; an LLM judge (GPT-4o-mini) and a Knowledge Graph (KG) are used to cross-reference the factual validity of the model’s reasoning against established scientific data.

**Task Characteristics:**
The work centers on constrained, open-ended ideation. It involves a complex interplay of discovery and verification, requiring the model to perform multi-step reasoning to bridge knowledge gaps. The task structure emphasizes knowledge-grounded generation and the internal refinement of ideas to ensure scientific viability.",bb5f873632616c2cdc07ef1bb139db0c96c8e5f6:Scientific Hypothesis Generation Dataset,Scientific Hypothesis Generation Dataset,https://arxiv.org/abs/2411.02382,3.0,Special,1.0,False,True,True,huggingface,openai; google,text
bf746159ec6008fa8e4d4134c848f8611066d62d,DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts,DataNarrative: Automated Data-Driven Storytelling with Visualizations and Texts,2024.0,2024.0,,Functional & Professional Writing,Graphic Design & Visual Layout; Narrative & Story Writing,high,"The paper focuses on the professional task of data-driven storytelling, combining narrative construction with visual design elements like charts and infographics to communicate insights from tabular data.","**Creative Artifacts Evaluated:**
Multimodal data stories comprising structured textual narratives interleaved with visualization specifications (e.g., chart configurations) derived from tabular data. These artifacts are not merely single charts but sequenced, data-driven ""narrative arcs"" that combine natural language explanations with corresponding visual representations.

**Creative Capabilities Assessed:**
The benchmark measures narrative construction (the ability to weave data points into a coherent story) and multimodal synthesis (the effective integration of text and visuals). It assesses visualization quality (design appropriateness), informativeness, and factual correctness to ensure the creative output remains grounded in data. Additionally, it evaluates clarity, coherence, and the ability to fulfill specific user intents through storytelling.

**Evaluation Context:**
This involves a complex multimodal generation task using Large Language Models (LLMs). Evaluation utilizes a mixed-methods approach: an LLM-judge (Gemini-1.5-pro) performs pairwise comparisons between different generation strategies (agentic vs. direct prompting), supplemented by human expert evaluation across five qualitative dimensions.

**Task Characteristics:**
The core task is open-ended, data-constrained generation. It requires complex synthesis, transforming raw tabular inputs and user intents into structured, persuasive narratives. This involves cross-modal translation from structured data to both natural language and visual code, requiring the model to identify salient insights and organize them logically.",bf746159ec6008fa8e4d4134c848f8611066d62d:DataNarrative,DataNarrative,https://github.com/saidul-islam98/DataNarrative,1.0,API-Only,1.0,True,False,True,github,openai,multimodal
b89042e4994f2b8da5eddc920877253743c63d57,Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling,Mathemyths: Leveraging Large Language Models to Teach Mathematical Language through Child-AI Co-Creative Storytelling,2024.0,2024.0,,Narrative & Story Writing,Functional & Professional Writing; Dialogue Generation & Social Interaction,high,"The paper focuses on the co-creation of narrative stories (Narrative) designed for pedagogical purposes to teach mathematical language (Functional) through interactive, turn-taking exchanges between children and AI (Dialogue).","**Creative Artifacts Evaluated:**
The primary artifacts are co-created narrative short stories, Socratic prompts/questions designed to elicit child participation, and narrative continuations that integrate specific mathematical vocabulary (e.g., spatial and quantitative terms).

**Creative Capabilities Assessed:**
The framework assesses the AI’s capacity for *inspiration* (the ability to spark child ideas) and *narrative coherence* (story relevancy). It measures *domain-specific integration* (mathematical language relevancy), *fluency*, and *elaboration* within the child-AI dyad. Human raters also judge *perceived creativity*, *readability*, and the *aesthetic quality* of the storytelling prompts.

**Evaluation Context:**
This is a text-based, child-centered evaluation involving human-in-the-loop assessment. It utilizes 5-point Likert scales and pairwise comparisons by adult raters to evaluate GPT-4 and fine-tuned GPT-3.5 models against human benchmarks. Additionally, a user study with children (ages 4–8) employs mixed-methods scoring, including human coding of engagement and pre/post-test assessments of mathematical vocabulary.

**Task Characteristics:**
The tasks feature open-ended, turn-taking co-creative generation and constrained ideation. The process emphasizes *scaffolded creativity*, where the LLM must balance narrative flow with educational constraints (cross-domain transfer between storytelling and mathematical pedagogy) to facilitate child-led story refinement and ideation.",b89042e4994f2b8da5eddc920877253743c63d57:LLM Story Co-Creator Model Evaluation,LLM Story Co-Creator Model Evaluation,https://github.com/Mathemyths-CHI2024/Mathemyths,3.0,Special,2.0,False,False,True,github,openai,text
b99973ab1c714c2da29334d113be348477426743,What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models,What Shapes a Creative Machine Mind? Comprehensively Benchmarking Creativity in Foundation Models,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Programming & Algorithmic Creativity; Scientific Discovery,high,"The paper establishes a comprehensive benchmark for machine creativity by evaluating convergent and divergent thinking across diverse tasks including storytelling, functional coding, and strategic ideation for real-world problems.","**Creative Artifacts Evaluated**
The benchmark evaluates textual and multimodal artifacts, including open-domain answers, functional source code, and short narrative stories. It also assesses strategic solutions (ideas) for constrained real-world problems and image-grounded textual responses, such as visual question answers and narrative continuations based on visual prompts.

**Creative Capabilities Assessed**
The framework measures both convergent thinking (finding correct solutions to constrained problems) and divergent thinking (generating varied, open-ended possibilities). Capabilities are quantified through three social science-grounded dimensions: Usefulness (accuracy, utility, and pass rates), Originality (statistical uniqueness determined by clustering distinct correct solutions), and Surprise (measured by the inverse of the model’s self-reported confidence).

**Evaluation Context**
Evaluation targets LLMs and VLMs using a hybrid scoring approach. Convergent tasks utilize automatic metrics, including cosine similarity for solution clustering and self-reported confidence scores. Divergent tasks employ LLM-as-a-Judge and MLLM-as-a-Judge frameworks, where models score outputs on a 0-5 scale based on detailed qualitative rubrics.

**Task Characteristics**
Tasks involve a mix of constrained problem-solving (QA and coding), knowledge-intensive reasoning (VQA), and open-ended generation (storytelling and ideation). The benchmark is distinguished by its requirement for models to navigate real-world constraints while maintaining narrative coherence and technical correctness across text and vision modalities.",b99973ab1c714c2da29334d113be348477426743:C2-Eval,C2-Eval,https://github.com/ZicongHe2002/LLM-Creativity-Benchmarking,2.0,GPU/Local,6.0,False,False,True,github,openai,multimodal
c3f8698adeeae07d81114f08cd178248a49c774e,Past Meets Present: Creating Historical Analogy with Large Language Models,Past Meets Present: Creating Historical Analogy with Large Language Models,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper focuses on identifying structural isomorphisms and performing cross-domain analogical mapping, which are explicitly listed as core components of lateral thinking and creative problem solving.","**Creative Artifacts Evaluated:**
Textual historical analogies that map contemporary events to past occurrences across four specific thematic domains: War, Politics, Culture/Society, and Economy. These artifacts consist of comparative event pairings and explanatory narratives.

**Creative Capabilities Assessed:**
The benchmark measures divergent thinking and conceptual depth through the identification of structural isomorphisms. It specifically evaluates innovation by rewarding high abstract/relational similarity while penalizing literal lexical overlap, testing the model's capacity for non-obvious, high-level relational mapping rather than surface-level imitation.

**Evaluation Context:**
This is a text-based LLM generation framework. Scoring utilizes a hybrid approach: Pass@1 against standard answers for popular analogies, and a custom Multi-dimensional Similarity (MDS) score for general analogies. MDS integrates LLM-based abstract judgment with Jaccard-based literal similarity metrics, supplemented by human ranking to validate qualitative depth.

**Task Characteristics:**
The work focuses on cross-domain transfer (temporal mapping) and constrained ideation. Tasks include both the retrieval of culturally established ""popular"" analogies and the generation of ""general"" analogies within thematic constraints. This requires models to perform complex relational reasoning, moving beyond simple pattern matching to achieve deep structural alignment between disparate historical contexts.",c3f8698adeeae07d81114f08cd178248a49c774e:Historical Analogy Acquisition,Historical Analogy Acquisition,https://github.com/Nianqi-Li/Historical-Analogy-of-LLMs,1.0,API-Only,2.0,True,False,True,github,openai; google,text
bbe490c3ccfe1efcf41f797bdff97ac9932692b2,The Creative Psychometric Item Generator: a Framework for Item Generation and Validation Using Large Language Models,The Creative Psychometric Item Generator: a Framework for Item Generation and Validation Using Large Language Models,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Functional & Professional Writing,high,"The paper focuses on generating and validating psychometric items designed to measure divergent thinking and open-ended problem solving, while the framework itself serves a professional pedagogical and psychological assessment purpose.","**Creative Artifacts Evaluated:**
The framework focuses on textual open-ended problem-solving scenarios (test items) and their corresponding textual solutions (responses). These artifacts represent structured prompts designed to elicit creative thought and the resulting ideational output.

**Creative Capabilities Assessed:**
The primary metric is originality, specifically the statistical novelty of solutions. Additionally, the framework assesses psychometric properties including item complexity and difficulty. It evaluates the capacity for divergent thinking and the ability of LLMs to function as human simulacra in creative testing environments.

**Evaluation Context:**
This is a text-based psychometric validation study. It employs a hybrid evaluation model: automated originality scoring is performed by a fine-tuned RoBERTa-base model, while qualitative validation of the generated items is conducted by human experts using 5-point Likert scales.

**Task Characteristics:**
The research defines a meta-creative process involving iterative item generation, constrained response generation, and automated judgment. It distinguishes itself by focusing on the automated construction and validation of psychometric instruments (CPIG) rather than the singular assessment of creative performance, bridging the gap between item generation and automated scoring.",bbe490c3ccfe1efcf41f797bdff97ac9932692b2:Creative Problem-Solving (CPS) Task,Creative Problem-Solving (CPS) Task,https://github.com/Beaty-Lab/CREAI-item-generation,1.0,API-Only,4.0,True,False,True,github,openai,text
c00c67bb564f8f4e9912439f2283da691a16541e,MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation,2024.0,2024.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving; Performing Arts & Video Synthesis,high,"The paper focuses on automatic story generation and premise synthesis (Narrative), explicitly measures divergent thinking and ideational variety (Lateral Thinking), and includes scripts as a target narrative format (Performing Arts).","**Creative Artifacts Evaluated:**
The research evaluates modular story premises—structured narrative foundations comprising specific components like twists, endings, and events—alongside downstream long-form narratives (novels and scripts) and short stories.

**Creative Capabilities Assessed:**
Assessment focuses on narrative quality through Fascination (engagement), Completeness (structural coherence), and Originality (novelty). Crucially, it quantifies divergent thinking and ideational variety using semantic diversity metrics: Breadth (the area of the embedding polygon) and Density (the uniformity of the distribution in latent space).

**Evaluation Context:**
This text-based study employs a hybrid evaluation framework. It combines LLM-as-a-judge (GPT-4-Turbo and Claude-3-Opus) and human ratings with novel automated geometric metrics to objectively measure the ""creative spread"" and distributional variety of generated premises.

**Task Characteristics:**
The tasks center on modular ideation and synthesis, where premises are constructed from discrete narrative modules. It encompasses both open-ended generation and constrained generation (producing stories from specific premises), supported by ablation studies to determine how specific structural elements, such as plot twists, influence the perceived creativity and complexity of the final output.",c00c67bb564f8f4e9912439f2283da691a16541e:MoPS Premise Evaluation,MoPS Premise Evaluation,https://huggingface.co/datasets/ManTle/mops,1.0,API-Only,2.0,True,True,True,huggingface,openai,text
c1202ab1afd2cc2103b4a9efc1844524015535be,Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders,Interesting Scientific Idea Generation using Knowledge Graphs and LLMs: Evaluations with 100 Research Group Leaders,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on the generation of novel scientific research hypotheses and cross-domain synthesis, while utilizing divergent thinking to create connections between disparate research fields.","**Creative Artifacts Evaluated:**
Personalized scientific research ideas and collaborative hypotheses. These artifacts are text-based proposals synthesized from the specific publication histories and expertise of two distinct research group leaders.

**Creative Capabilities Assessed:**
The benchmark measures ""interest"" as a proxy for scientific value, encompassing conceptual depth, novelty, and cross-domain synthesis. It evaluates the system’s ability to perform divergent thinking (generating new connections between disparate fields) and convergent judgment (predicting which specific ideas domain experts will find compelling).

**Evaluation Context:**
This is a high-expertise, text-based domain. The evaluation framework is notable for its scale, utilizing 100 human research group leaders to provide a ""gold standard"" for scientific utility. It employs a hybrid scoring approach, comparing human Likert-scale ratings against supervised neural networks and zero-shot LLM judges using AUC-ROC and top-N precision metrics.

**Task Characteristics:**
The tasks involve constrained generation (bridging two specific, real-world research profiles) and evaluative judgment. It focuses on personalized ideation rather than general knowledge retrieval, requiring the model to act as both a creative generator and a discriminative critic of scientific potential within a specialized professional context.",c1202ab1afd2cc2103b4a9efc1844524015535be:SciMuse Scientific Idea Evaluation,SciMuse Scientific Idea Evaluation,https://github.com/artificial-scientist-lab/SciMuse,3.0,Special,3.0,False,False,True,request_access,openai; anthropic; google,text
c476c92f86c590ada95421cf597910259927771b,Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications,Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications,2024.0,2024.0,,Scientific Discovery,,high,"The paper specifically addresses the assessment of novelty and originality in scientific hypotheses and scholarly contributions, which aligns directly with the Scientific Discovery domain's focus on research originality and literature synthesis.","**Creative Artifacts Evaluated:**
The artifacts consist of scholarly publications, specifically represented by their titles and abstracts. These texts encapsulate complex scientific hypotheses, experimental methodologies, and theoretical contributions within specialized academic domains.

**Creative Capabilities Assessed:**
The core capability assessed is the ability to evaluate scholarly novelty (originality). This requires the model to distinguish between incremental improvements and significant conceptual breakthroughs. It measures the model’s capacity for high-level conceptual depth and its ability to recognize the ""newness"" of a scientific idea within the context of existing literature.

**Evaluation Context:**
This is a text-based evaluation where Large Language Models (LLMs) serve as automated judges. Using the SchNovel benchmark, models perform pairwise comparisons of research papers. The evaluation is conducted through automatic scoring (Accuracy), comparing model judgments against ground-truth data derived from publication timelines or citation patterns.

**Task Characteristics:**
The task is a constrained evaluation and judgment exercise. Rather than generating creative content, the model must analyze and compare two distinct artifacts to determine relative novelty. This involves complex information synthesis and an understanding of the temporal evolution of scientific knowledge, differentiating it from general-purpose creativity tasks by focusing on the intellectual ""delta"" between two technical documents.",c476c92f86c590ada95421cf597910259927771b:SchNovel,SchNovel,https://github.com/ethannlin/SchNovel,1.0,API-Only,1.0,True,True,True,huggingface,openai,text
c4270cece55dbdf1f7dd202a89c63d812d63ed2a,Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs,Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs,2024.0,2024.0,,Narrative & Story Writing,Figurative Language & Rhetoric,high,"The paper's primary focus is evaluating the quality, coherence, and originality of short fictional stories and open-ended creative writing, while specifically assessing linguistic elements like vivid imagery and emotional resonance.","**Creative Artifacts Evaluated:**
The primary artifacts are open-ended creative writing responses and short fictional stories. Specifically, the evaluation focuses on sets of three stories generated from identical prompts and structured narrative outputs constrained within a JSON schema to facilitate multi-dimensional scoring.

**Creative Capabilities Assessed:**
The research measures narrative quality, originality, and emotional impact. Specific capabilities include the ability to generate vivid imagery, maintain narrative flow, and produce diverse outputs within a set (intra-prompt diversity). It distinguishes between general engagement and specific creative dimensions like emotional resonance and conceptual novelty.

**Evaluation Context:**
The evaluation is situated within text-based LLM generation. It employs a hybrid scoring model: LLM-as-a-judge (utilizing GPT-4 Turbo and GPT-4o) for win rates and structured attribute scoring, alongside human evaluation via Prolific participants who rate story quality and diversity on a 1–10 scale.

**Task Characteristics:**
Tasks involve both open-ended generation and constrained generation (JSON-formatted stories). A defining characteristic is the comparative analysis of sampling behavior, specifically assessing the trade-off between coherence and creativity. The framework evaluates the model's ability to maintain logical consistency (coherence) while maximizing divergent output (creativity) across narrative tasks.",c4270cece55dbdf1f7dd202a89c63d812d63ed2a:AlpacaEval Creative Writing,AlpacaEval Creative Writing,https://github.com/menhguin/minp_paper,2.0,GPU/Local,1.0,False,True,True,huggingface,openai,text
bad287184c6739fd6f476f89cb83e09415982d9f,ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base,ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric; Scientific Discovery,high,"The paper's core focus is on analogical reasoning and relational mapping across disparate domains, which aligns with lateral thinking, while also specifically evaluating metaphorical mappings and scientific analogies.","This paper focuses on **analogical reasoning** as a foundational element of creative cognition, utilizing a million-scale knowledge base (ANALOGYKB) to evaluate and enhance language models.

**Creative Artifacts Evaluated:**
The primary artifacts are word-level and phrase-level analogies (A:B::C:D), metaphorical mappings, scientific analogies, and natural language rationales/explanations that justify the relational links between disparate concepts.

**Creative Capabilities Assessed:**
The benchmarks measure **relational reasoning** and **conceptual depth**, specifically the ability to identify and map abstract structures across different semantic domains. It assesses **convergent thinking** through analogy completion and **rationalization** through the generation of logical explanations for metaphorical or scientific connections.

**Evaluation Context:**
Evaluation is strictly text-based, spanning from simple morphological relations to complex, high-stakes educational problems (SAT, UNIT benchmarks). The methodology employs a hybrid scoring approach: automatic metrics (Accuracy, MRR) for recognition tasks and human expert judgment (Fleiss’s kappa) for assessing the quality of generated analogies and explanations.

**Task Characteristics:**
Tasks include **constrained generation** (completing the final term of an analogy), **recognition** (multiple-choice problem-solving), and **cross-domain transfer** (applying relational logic to metaphorical and scientific contexts). The work distinguishes itself by moving from simple word-pair similarity to complex, multi-step relational ideation and explanation.",bad287184c6739fd6f476f89cb83e09415982d9f:ANALOGYKB,ANALOGYKB,https://github.com/siyuyuan/analogykb,2.0,GPU/Local,2.0,False,False,True,github,openai,text
c59d685a21f5b8f4750a28a5733a80fbaa0f410b,Making New Connections: LLMs as Puzzle Generators for The New York Times' Connections Word Game,Making New Connections: LLMs as Puzzle Generators for The New York Times' Connections Word Game,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper focuses on generating word puzzles that require identifying non-obvious semantic links and 'outside-the-box' reasoning, which aligns with lateral thinking, while utilizing wordplay and linguistic nuances characteristic of figurative language.","**Creative Artifacts Evaluated:**
The primary artifacts are 4x4 word grids for the ""Connections"" puzzle game, comprising 16 words partitioned into four distinct thematic groups. These groups are defined by specific semantic, linguistic, or cultural associations (e.g., synonyms, homophones, or members of a specific set).

**Creative Capabilities Assessed:**
The study measures divergent thinking through the identification of non-obvious or polysemous word links and convergent thinking via the synthesis of words into cohesive, hidden categories. It specifically assesses the ability to calibrate difficulty—matching the NYT’s color-coded levels—and the capacity to generate puzzles that provide human enjoyment and aesthetic satisfaction.

**Evaluation Context:**
The framework employs a text-based modality, comparing LLM-generated puzzles against human-expert benchmarks. Evaluation is hybrid: a human preference study measures subjective quality (creativity, difficulty, and enjoyment) and gameplay performance (solve rates, mistakes), while automatic cosine similarity of word embeddings is used to validate difficulty proxies.

**Task Characteristics:**
This research involves highly constrained generation requiring adherence to strict structural rules and semantic logic. The tasks include both open-ended creative ideation and a secondary task of difficulty estimation/judgment based on semantic distance and word association.",c59d685a21f5b8f4750a28a5733a80fbaa0f410b:Connections Puzzle Generation & Human Preference Study,Connections Puzzle Generation & Human Preference Study,https://anonymous.4open.science/r/making-new-connections-78D1,3.0,Special,2.0,False,False,True,github,openai,text
c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1,Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Scientific Discovery; Figurative Language & Rhetoric,high,"The paper's core focus is on structure abduction and analogical mapping to identify structural isomorphisms between disparate systems, which are central components of lateral thinking and figurative reasoning applied to scientific domains.","**Creative Artifacts Evaluated:**
The primary artifacts are scientific analogies and relational mappings. These include word-level analogies (A:B::C:D), relational structure identifications (e.g., identifying ""part-of"" or ""cause-effect"" relationships), and complex system-level mappings between scientific domains (e.g., mapping components of a camera to the human eye).

**Creative Capabilities Assessed:**
The research measures relational thinking and structure abduction, focusing on the ability to move beyond surface-level similarities to identify deep structural isomorphisms. Key capabilities include conceptual depth, cross-domain transfer, and convergent thinking required to align disparate systems. In open-ended tasks, it assesses the model's ability to extract and synthesize relevant concepts from raw scientific descriptions to form coherent mappings.

**Evaluation Context:**
Evaluation is text-based, utilizing Large Language Models (LLMs) across a spectrum of complexity. It employs standard word-analogy benchmarks (SAT, BATS) alongside a novel scientific analogy dataset (SCAR). Scoring is primarily automatic (Accuracy, F1, Overlap), supplemented by human annotation for precision in open-ended generation tasks.

**Task Characteristics:**
Tasks range from constrained multiple-choice word analogies to complex, open-ended structural abduction. The work emphasizes cross-domain transfer, requiring models to perform relational structure identification and system-level mapping. This involves both ""closed"" settings (mapping predefined concept lists) and ""open"" settings (identifying and mapping concepts from background text), distinguishing it from simple pattern matching.",c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1:Relational Structure Identification (RSI) Test,Relational Structure Identification (RSI) Test,https://github.com/siyuyuan/scar,1.0,API-Only,1.0,True,False,True,github,openai,text
c7d2c6b926fc2c2971b7c6c440658fe117d41062,Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art,Can AI Examine Novelty of Patents?: Novelty Evaluation Based on the Correspondence between Patent Claim and Prior Art,2025.0,2025.0,,Scientific Discovery,Engineering & Technical Design; Functional & Professional Writing,high,"The paper evaluates the model's ability to judge the novelty and originality of technical inventions (patents), which aligns with the assessment of originality in STEM fields and involves analyzing technical designs within a professional legal framework.","**Creative Artifacts Evaluated:**
The artifacts consist of patent claims—highly structured, technical legal descriptions of inventions—and their corresponding prior art documents (specifically, cited text segments from existing patents that potentially invalidate the claim’s novelty).

**Creative Capabilities Assessed:**
The benchmark measures the ability to assess novelty, a critical component of technical creativity and inventiveness. Specifically, it evaluates technical discernment and comparative reasoning: the capacity to identify whether the specific technical configurations described in a new claim are already present in existing literature (anticipation). It also assesses the model's ability to provide qualitative justifications for these novelty judgments.

**Evaluation Context:**
This is a text-based evaluation comparing Large Language Model (LLM) performance against a human expert baseline. Evaluation involves binary classification (Novel vs. Non-Novel) using automatic metrics (Accuracy, Precision, Recall, F1) alongside a qualitative analysis of the explanations generated by the models to support their decisions.

**Task Characteristics:**
The tasks focus on evaluation and judgment rather than generation. They involve constrained classification and comparative analysis, requiring the model to map specific claim elements to prior art. This represents a complex, domain-specific reasoning task that tests the model's ability to navigate the intersection of technical language and legal standards for originality.",c7d2c6b926fc2c2971b7c6c440658fe117d41062:Patent Novelty Evaluation Dataset,Patent Novelty Evaluation Dataset,,3.0,Special,3.0,False,False,True,unknown,openai,text
c7791adf924e4493c3fdbd7245e46f303a1591bd,Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models,Beyond Memorization: Mapping the Originality-Quality Frontier of Language Models,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Narrative & Story Writing; Poetry & Verse,high,"The paper evaluates LLM creativity through three specific lenses: unconventional problem-solving (MacGyver dataset), short narratives (TinyStories), and poetic verses (CoPoet), focusing on the tension between originality and quality.","**Creative Artifacts Evaluated:**
Short narrative continuations (based on the TinyStories dataset), single-line poetic verses (based on the CoPoet dataset), and unconventional problem-solving solutions involving physical reasoning and tool use (based on the MacGyver dataset).

**Creative Capabilities Assessed:**
The benchmark measures ""Novelty,"" defined as the harmonic mean of **originality** and **quality**. Originality is quantified through n-gram uniqueness (4, 5, and 6-grams) relative to the model's training data to detect memorization. Quality is assessed via LLM-based scoring of coherence, instruction following, and functional utility. The framework specifically targets the model's ability to navigate the ""originality-quality frontier.""

**Evaluation Context:**
This is a text-based LLM benchmarking suite that utilizes a hybrid scoring methodology. It distinguishes itself by combining objective, automatic n-gram analysis (to penalize verbatim regurgitation) with subjective LLM-as-a-judge evaluations. This dual approach isolates creative synthesis from simple data retrieval.

**Task Characteristics:**
The tasks encompass open-ended narrative generation, constrained stylistic generation, and creative ideation for problem-solving. These tasks require a mix of divergent thinking (generating multiple novel solutions) and convergent thinking (ensuring solutions are logically sound and high-quality), emphasizing the tension between statistical rarity and contextual appropriateness.",,,,,,,,,,,,
c44760d1864e62f09aed92bbe0a23d12b9d6983c,Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation,Fùxì: A Benchmark for Evaluating Language Models on Ancient Chinese Text Understanding and Generation,2025.0,2025.0,,Poetry & Verse,Figurative Language & Rhetoric; Functional & Professional Writing,high,"The benchmark primarily evaluates classical Chinese poetry and couplets with strict metrical and tonal constraints, while also assessing literary analysis of imagery and functional tasks like ancient-to-modern translation and TCM explanations.","**Creative Artifacts Evaluated**
The benchmark evaluates classical Chinese poetry (*Shi*), *Ci* poetry (structured by specific *cipai* rhythmic patterns), antithetical couplets, and allegorical sayings (*xiehouyu*). It also includes modern Chinese translations of ancient texts, literary appreciations (analyses of imagery and sentiment), and Traditional Chinese Medicine (TCM) prescription explanations.

**Creative Capabilities Assessed**
Assessment focuses on formal technical correctness—specifically adherence to rigid tonal patterns, character counts, and rhythmic constraints unique to classical Chinese. It measures linguistic fluency, semantic fidelity in cross-temporal translation, and conceptual depth through literary analysis. The tasks evaluate both divergent generation (creating poetry from topic words) and convergent reconstruction (recovering original ancient verses from modern prose).

**Evaluation Context**
This is a text-based LLM benchmark characterized by high linguistic complexity. Evaluation employs a hybrid methodology: automatic metrics (BLEU, Accuracy), rule-based validation for structural constraints in poetry, and a domain-specific LLM evaluator (fine-tuned Qwen2-7B) to judge the quality of open-ended creative generation.

**Task Characteristics**
The benchmark features a mix of constrained generation (pattern-following for *Ci* and couplets), open-ended creative production, and cross-temporal transfer (Ancient-to-Modern translation). It integrates creative tasks with analytical challenges, such as source-tracing for quotes and idioms, and domain-specific knowledge application in TCM and historical bibliography.",c44760d1864e62f09aed92bbe0a23d12b9d6983c:Fùxì,Fùxì,https://github.com/cubenlp/FuxiBench,2.0,GPU/Local,21.0,False,False,True,github,openai,text
c863800ec76e62fed66bbf5ec84ceae8642fd910,Harnessing Large Language Models for Scientific Novelty Detection,Harnessing Large Language Models for Scientific Novelty Detection,2025.0,2025.0,,Scientific Discovery,,high,"The paper focuses on evaluating the novelty and originality of scientific research hypotheses and ideas, which is the core definition of the Scientific Discovery domain.","**Creative Artifacts Evaluated:**
The artifacts consist of textual scientific research ideas and hypotheses specifically situated within the Marketing and Natural Language Processing (NLP) domains. These include both original ""novel"" ideas and synthesized ""non-novel"" ideas designed to mimic or slightly vary existing literature.

**Creative Capabilities Assessed:**
The primary capability assessed is the detection of scientific originality (novelty). This involves evaluating conceptual depth and the ability to distinguish between incremental extensions and truly novel contributions. The benchmark also measures the model's capacity for precise information retrieval—specifically, the ability to link derivative or synthesized ideas back to their original ""anchor"" papers.

**Evaluation Context:**
This is a text-based evaluation framework utilizing LLMs as both retrievers and evaluators. The methodology employs a hybrid scoring system: automatic metrics (Acc@k, MAP) for retrieval tasks, and a mixed approach for classification. In the latter, LLMs assign scores (0.0–1.0) based on a specific rubric, which are then processed by a decision tree classifier to determine binary novelty.

**Task Characteristics:**
The tasks are discriminative and evaluative rather than generative. They focus on novelty judgment and idea-to-paper retrieval. This involves cross-referencing specific research proposals against a vast corpus of existing scientific literature to identify redundancy, conceptual overlap, or unique scientific leaps.",c863800ec76e62fed66bbf5ec84ceae8642fd910:Scientific Novelty Detection Datasets (Marketing & NLP),Scientific Novelty Detection Datasets (Marketing & NLP),https://anonymous.4open.science/r/NoveltyDetection-10FB/,2.0,GPU/Local,2.0,False,False,True,github,openai,text
c81fd487a418268d42dce8613236297a9dc127fc,Learning Personalized Alignment for Evaluating Open-ended Text Generation,Learning Personalized Alignment for Evaluating Open-ended Text Generation,2023.0,2023.0,,Narrative & Story Writing,Functional & Professional Writing,high,"The paper focuses on evaluating narrative elements like movie and story plots through dimensions such as character development and narrative surprise, while also generating personalized critical reviews.","**Creative Artifacts Evaluated:**
The research focuses on narrative-driven text, specifically movie plots, short story plots, and book descriptions. It also evaluates the generation of personalized critical reviews that reflect specific user perspectives on these creative works.

**Creative Capabilities Assessed:**
The benchmarks measure a model's ability to simulate subjective human judgment across creative dimensions such as interestingness, narrative surprise, and character development. Rather than assessing universal quality, it evaluates the capacity for personalized alignment—the ability to weight different creative aspects according to individual aesthetic preferences and to generate reviews that mirror a specific reviewer's critical style.

**Evaluation Context:**
This is a text-based LLM evaluation framework. It employs a mix of scalar rating predictions (1–10 scale) and natural language generation. Scoring is conducted through correlation coefficients (Pearson, Spearman, Kendall) for quantitative alignment and semantic metrics (BERTScore, BARTScore, ROUGE) to assess the fidelity of generated reviews against ground-truth human critiques.

**Task Characteristics:**
The tasks are primarily evaluative and judgmental, involving personalized scalar scoring and fine-grained pairwise preference selection between competing narratives. A key feature is cross-domain transfer, where models are tested on their ability to generalize personalized evaluation logic from cinematic narratives to the literary domain (books) in a zero-shot capacity, distinguishing this work from static, non-personalized creativity benchmarks.",c81fd487a418268d42dce8613236297a9dc127fc:Per-MPST,Per-MPST,https://github.com/facebookresearch/perse,2.0,GPU/Local,1.0,False,False,True,github,openai,text
c9448a5be184331c0cc0e2767dc9b2c7b4064d0b,AI Idea Bench 2025: AI Research Idea Generation Benchmark,AI Idea Bench 2025: AI Research Idea Generation Benchmark,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper describes a benchmark specifically designed to evaluate the generation of novel research hypotheses, experimental designs, and technical feasibility in the field of AI, which is the core definition of the Scientific Discovery domain.","**Creative Artifacts Evaluated:**
Structured AI research ideas, specifically comprising problem motivations, detailed experimental plans, and sequential conceptual steps for implementation.

**Creative Capabilities Assessed:**
The benchmark measures originality through novelty (historical and contemporary uniqueness) and technical correctness via feasibility (alignment with existing literature). It also assesses conceptual depth through idea-to-topic alignment and comparative quality via pairwise competition.

**Evaluation Context:**
This is a text-based benchmark for LLM-generated scientific content. It employs a hybrid evaluation approach: LLM-based judging (Deepseek V3) for qualitative alignment and automatic scoring for quantitative novelty and feasibility, reflecting high-complexity academic ideation.

**Task Characteristics:**
The tasks involve both open-ended and constrained ideation, alongside discriminative evaluation. It includes multiple-choice matching, pairwise ranking, and literature-grounded assessment of technical viability, distinguishing it from general creative writing benchmarks by focusing on the rigorous requirements of scientific discovery.",c9448a5be184331c0cc0e2767dc9b2c7b4064d0b:AI Idea Bench 2025,AI Idea Bench 2025,https://github.com/ai-idea-bench/ai-idea-bench.github.io,1.0,API-Only,6.0,True,False,True,github,openai; huggingface,text
caa098989011d7fdfe21ae7a08796cfb5942e64d,TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques,TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques,2023.0,2023.0,,Poetry & Verse,Figurative Language & Rhetoric,high,"The paper specifically focuses on the generation of Persian poem couplets, emphasizing the structural, rhythmic, and rhyming constraints inherent to classical verse forms.","**Creative Artifacts Evaluated:**
The primary artifacts are Persian poem couplets, specifically focusing on the linguistic and structural constraints inherent to classical Persian poetic forms.

**Creative Capabilities Assessed:**
The research measures linguistic fluency, thematic coherence, and semantic meaningfulness. Crucially, it evaluates ""poeticness""—a domain-specific metric for aesthetic and structural quality—and output diversity (originality), using Self-BLEU to ensure the model avoids repetitive or formulaic generation.

**Evaluation Context:**
This is a text-based evaluation comparing a specialized transformer model (TPPoet) against general-purpose Persian LLMs (GPT2-Persian) and human-authored benchmarks. The study employs a hybrid scoring methodology: automatic n-gram overlap metrics (BLEU and Self-BLEU) provide quantitative data on quality and diversity, while human judges provide qualitative Likert-scale ratings (1-5) and perform a Turing-style identification task to distinguish AI-generated poetry from human work.

**Task Characteristics:**
The core task is constrained creative generation, requiring the model to navigate the complex rhythmic and rhyming conventions of Persian prosody. It encompasses open-ended generation from minimal data and a discriminative evaluation component where humans judge the ""human-likeness"" and artistic merit of the generated artifacts.",caa098989011d7fdfe21ae7a08796cfb5942e64d:Persian Poem Generation,Persian Poem Generation,https://resodate.com/item/TPPoet,3.0,Special,2.0,False,False,True,url,,text
ca1261a7e058ea3339fbdaec07ffcf6d0b617c8d,Can Language Models Laugh at YouTube Short-form Videos?,Can Language Models Laugh at YouTube Short-form Videos?,2023.0,2023.0,,Humor & Satire,Performing Arts & Video Synthesis,high,"The paper's central focus is on multimodal humor comprehension, reasoning, and classification within the context of short-form video content.","**Creative Artifacts Evaluated**
The benchmark evaluates short-form YouTube videos (YouTube Shorts) paired with human-annotated textual rationales. These rationales are natural language explanations that articulate the comedic timing, incongruity, or punchlines within the video content. Additionally, the work utilizes a 20-category humor taxonomy to label these creative video-text pairs.

**Creative Capabilities Assessed**
The primary focus is on multimodal humor comprehension and ""humor reasoning."" This includes the ability to identify specific visual or auditory triggers that elicit laughter and the capacity to generate coherent, fluent explanations for subjective comedic elements. It also assesses the model’s ability to categorize humor types (e.g., slapstick, irony, parody) and its temporal grounding—linking linguistic rationales to specific video timestamps.

**Evaluation Context**
This multimodal framework combines video and text processing. Evaluation is highly complex, employing a hybrid approach: automatic linguistic metrics (SentBERT, ROSCOE), human qualitative ratings (preference and 0-1 scales), and a novel ""rationale quality"" score. The latter uses a video moment retrieval model (QD-DETR) to determine if a generated explanation is precise enough to allow a secondary model to locate the correct funny moment.

**Task Characteristics**
Tasks include open-ended generation of humor explanations, constrained classification into a humor taxonomy, and a discriminative localization task. The benchmark emphasizes the intersection of cross-modal grounding and subjective interpretation, moving beyond simple video captioning toward deep semantic understanding of entertainment-based content.",ca1261a7e058ea3339fbdaec07ffcf6d0b617c8d:ExFunTube,ExFunTube,https://github.com/dayoon-ko/ExFunTube,2.0,GPU/Local,3.0,False,False,True,github,openai,multimodal
ca8441db8a31988a7add559ae86db3f522eac81f,Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment,Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment,2025.0,2025.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the evaluation of short-form creative stories and narrative responses using the Torrance Test of Creative Writing (TTCW), which assesses both narrative quality and divergent thinking capabilities.","**Creative Artifacts Evaluated:**
Short-form creative stories and narrative responses elicited through standardized prompts from the Torrance Test of Creative Writing (TTCW).

**Creative Capabilities Assessed:**
The research evaluates the model’s ability to simulate personalized creative judgment across specific dimensions such as originality, fluency, and elaboration. It specifically measures the alignment between LLM-as-a-judge predictions and individual human expert verdicts, focusing on the subjective and idiosyncratic nature of creativity rather than a single ""gold standard.""

**Evaluation Context:**
This is a text-based meta-evaluation study where LLMs act as judges of human-authored content. The framework compares LLM predictions against binary (yes/no) labels from human experts. Performance is quantified using statistical alignment metrics, including Pearson correlation, Cohen’s κ, and F1-score. The context includes both in-distribution and out-of-distribution (cross-dimension) scenarios to test the robustness of the judge's ""personality.""

**Task Characteristics:**
The primary task is the evaluation and judgment of creative work rather than its generation. It emphasizes personalized prediction and cross-dimension transfer, where a model trained on specific creative dimensions must generalize its judgment to unseen criteria. This distinguishes the work from traditional aggregate evaluations by focusing on the model's ability to mimic the specific ""taste"" or curiosity-driven exploration of individual human raters.",,,,,,,,,,,,
caae2e287e1d4e49a8179d721c6b0e7200cf537c,Sparks of Science: Hypothesis Generation Using Structured Paper Data,Sparks of Science: Hypothesis Generation Using Structured Paper Data,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving; Functional & Professional Writing,high,"The paper explicitly focuses on generating novel scientific hypotheses and conceptual insights from research data, which is the core definition of the Scientific Discovery domain.","**Creative Artifacts Evaluated:**
Structured scientific hypotheses and ""Sparks""—the core conceptual insights that bridge existing research gaps. These artifacts represent formal research propositions and actionable ideas derived from ""Bits"" (conventional scientific assumptions or specific research problems).

**Creative Capabilities Assessed:**
The benchmark measures novelty (originality), feasibility (technical soundness and usefulness), and idea distinctiveness (divergent thinking). It specifically evaluates the model's capacity for scientific synthesis—the ability to generate non-obvious, high-quality research directions that are both creative and practically viable within a professional domain.

**Evaluation Context:**
A text-based framework utilizing a hybrid scoring system. It combines automatic metrics (Perplexity, Idea Distinctiveness Index, and IAScore) with sophisticated pairwise comparisons conducted by frontier LLM judges (Claude 3.7 Sonnet, o3-mini) and human experts. This multi-layered approach validates qualitative attributes like preference and conceptual depth.

**Task Characteristics:**
Constrained ideation and knowledge synthesis. The task requires the model to transform structured paper data into a ""Spark,"" necessitating a creative leap from established facts to innovative theoretical extensions. It focuses on the ideation phase of the scientific method, distinguishing it from simple summarization or data extraction.",caae2e287e1d4e49a8179d721c6b0e7200cf537c:HypoGen,HypoGen,https://huggingface.co/datasets/UniverseTBD/hypogen-dr1,1.0,API-Only,1.0,True,True,True,huggingface,openai,text
ccdfb2af92259f84c3cef8f7b9f79b0b988f3780,Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations,Richer Output for Richer Countries: Uncovering Geographical Disparities in Generated Stories and Travel Recommendations,2024.0,2024.0,,Narrative & Story Writing,Functional & Professional Writing,high,"The paper evaluates LLM performance across two distinct text types: imaginative narrative stories and practical, goal-oriented travel itineraries, analyzing how geographical and socio-economic factors affect the quality of these creative outputs.","**Creative Artifacts Evaluated:**
The research evaluates two distinct text-based artifacts: geo-anchored narrative stories and structured travel itineraries/recommendations. These represent a blend of purely imaginative prose and practical, knowledge-intensive planning across approximately 4,000 global locations.

**Creative Capabilities Assessed:**
The framework measures lexical diversity and vocabulary richness (Type-Token Ratio), semantic originality (modified TF-IDF uniqueness), and informational density via geographical entity grounding (NER). It specifically assesses emotional depth and affective range—quantifying the presence of joy, hardship, and sadness—while also measuring the model’s capacity for substantive content generation versus generic ""absence of information"" responses.

**Evaluation Context:**
This is a large-scale, text-only evaluation of LLMs. It employs a hybrid scoring pipeline: automatic statistical metrics for linguistic variety, Named Entity Recognition for factual richness, and LLM-as-a-judge (GPT-4) for nuanced sentiment and thematic analysis. The context is uniquely defined by its global scope, linking output quality to real-world socio-economic data.

**Task Characteristics:**
The tasks involve open-ended creative generation and constrained ideation. A key discriminative feature is the focus on geographical disparity; the tasks are designed to reveal how socio-economic variables (like GDP) correlate with the ""creativity"" and detail of the output, moving beyond general performance to measure cultural and spatial equity in generative models.",ccdfb2af92259f84c3cef8f7b9f79b0b988f3780:Geographical Disparity Evaluation Framework,Geographical Disparity Evaluation Framework,https://github.com/FLAIR-IISc/richer-countries-have-richer-output,1.0,API-Only,2.0,True,False,True,github,openai,text
c959d653bfea954bc779a5acd6feacd6d45fdb15,NanoVLMs: How small can we go and still make coherent Vision Language Models?,NanoVLMs: How small can we go and still make coherent Vision Language Models?,2025.0,2025.0,,Narrative & Story Writing,Scientific Discovery,high,"The paper's core evaluation focuses on the generation of narrative text and short stories from visual stimuli, while its research objective investigates the technical feasibility and scaling limits of small-scale Vision-Language Models.","**Creative Artifacts Evaluated:**
Narrative text completions and short story segments generated in response to visual stimuli (COCO-based images).

**Creative Capabilities Assessed:**
The research evaluates narrative coherence (consistency and plot), imaginative quality (creativity and meaningfulness), and linguistic fluency. It specifically measures originality through Rouge-1 scores to distinguish between genuine creative generation and the rote memorization of training data.

**Evaluation Context:**
A multimodal setting focusing on the performance of extremely small-scale Vision-Language Models (NanoVLMs). The evaluation employs a hybrid approach: an LLM-as-a-judge (GPT-4o) provides qualitative scoring on creative dimensions, while automated metrics assess text overlap for novelty. The training context utilizes specialized datasets (ShortDesc and LongDesc) designed to mimic developmental learning processes.

**Task Characteristics:**
The primary task is multimodal partial text completion, a form of constrained open-ended generation. Models must synthesize visual information with a textual prefix to produce a logical and creative continuation. This requires cross-modal reasoning and narrative expansion, moving beyond simple image captioning into the realm of creative storytelling.",c959d653bfea954bc779a5acd6feacd6d45fdb15:LLM-based VLM Quality Evaluation,LLM-based VLM Quality Evaluation,https://github.com/huggingface/nanoVLM,2.0,GPU/Local,1.0,False,False,True,github,openai; huggingface,multimodal
ccd02cb6535ee8331c1ad7829ba57a0ae7b613eb,Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench,Which LLMs Get the Joke? Probing Non-STEM Reasoning Abilities with HumorBench,2025.0,2025.0,,Humor & Satire,Lateral Thinking & Creative Problem Solving,high,"The paper introduces HumorBench to evaluate LLMs' ability to resolve semantic incongruity in jokes, while framing humor comprehension as a proxy for lateral thinking and non-STEM reasoning.","**Creative Artifacts Evaluated:**
The primary artifacts are textual explanations of sophisticated visual humor, specifically interpreting the relationship between a cartoon’s visual description and its caption. The benchmark also evaluates the quality of these explanations as secondary artifacts through an automated judging process.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and non-STEM reasoning, specifically the ability to resolve semantic incongruity and identify the ""aha"" moment (the ""element"") of a joke. It assesses convergent thinking by requiring models to pinpoint specific humorous triggers and evaluates the capability of LLMs to act as creative critics by judging the accuracy of generated explanations.

**Evaluation Context:**
This is a text-only evaluation where visual scenes are provided as textual descriptions. It employs a hybrid scoring model: an LLM-based autograder (GPT-4o) identifies the presence of pre-annotated objective joke components. This autograder is itself validated against human expert benchmarks to ensure alignment in creative judgment.

**Task Characteristics:**
Tasks include constrained generation (explaining a specific joke), evaluation/judgment (validating the quality of explanations), and cross-domain correlation analysis. The latter compares humor comprehension—a proxy for creative reasoning—against STEM-focused reasoning (GPQA-Diamond) and general abstract problem-solving (ARC-AGI) to differentiate non-technical cognitive abilities.",ccd02cb6535ee8331c1ad7829ba57a0ae7b613eb:HumorBench,HumorBench,https://arxiv.org/abs/2507.21476,1.0,API-Only,1.0,True,True,True,huggingface,openai; anthropic; google; together,text
cdfaafb336e3b17728eb2d8cae5612cd914c816c,Can Large Language Models Unlock Novel Scientific Research Ideas?,Can Large Language Models Unlock Novel Scientific Research Ideas?,2024.0,2024.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on generating novel scientific hypotheses and research ideas, which directly aligns with the Scientific Discovery domain, while its emphasis on divergent thinking and originality links it to Lateral Thinking.","**Creative Artifacts Evaluated:**
The primary artifacts are **scientific research hypotheses** and **future research ideas (FRIs)**. These are structured textual proposals that suggest specific, novel directions for scientific inquiry derived from existing academic literature.

**Creative Capabilities Assessed:**
The benchmark measures **originality (novelty)**, **usefulness (feasibility and relevance)**, and **conceptual depth**. It specifically evaluates **divergent thinking** through an embedding-based ""Idea Distinctness Index"" and assesses the model's capacity for **knowledge synthesis** by requiring the integration of external background information into new proposals.

**Evaluation Context:**
This is a high-complexity **text-based** evaluation within the scientific domain. It employs a **hybrid scoring system** comprising human expert qualitative ratings, an LLM-based ""Idea Alignment Score"" (IAScore), and automated embedding-based metrics. This framework distinguishes itself by requiring deep domain-specific grounding rather than general linguistic creativity.

**Task Characteristics:**
Tasks encompass **open-ended ideation** (generating FRIs from source papers), **constrained generation** (incorporating specific background knowledge), and **evaluative judgment** (semantic idea matching). The focus is on the transition from understanding established scientific results to projecting viable and distinct future research trajectories.",cdfaafb336e3b17728eb2d8cae5612cd914c816c:Future Research Idea Generation Benchmark,Future Research Idea Generation Benchmark,https://github.com/sandeep82945/Future-Idea-Generation,1.0,API-Only,3.0,True,False,True,github,openai; anthropic; google,text
cfda60d69d39674549d15a78e65fc5ec2f81de7f,Visually Grounded Story Generation Challenge,Visually Grounded Story Generation Challenge,2023.0,2023.0,,Narrative & Story Writing,,high,"The paper's core focus is on generating sequential multimodal narratives from image series, specifically evaluating narrativity, character persistence, and global coherence in storytelling.","**Creative Artifacts Evaluated:**
Sequential multimodal narratives (stories) generated from image series. These artifacts are characterized by their requirement to bridge discrete visual frames into a continuous, plot-driven textual output rather than isolated captions.

**Creative Capabilities Assessed:**
Narrativity (the ability to tell a story versus simply describing a scene), character grounding (maintaining identity persistence across frames), visual-textual alignment, and global coherence. The benchmark specifically measures the ability to synthesize divergent narrative events while maintaining an ""entity grid"" for structural consistency and character-matching.

**Evaluation Context:**
Multimodal Vision-Language Models (VLMs) are assessed using a hybrid framework. This includes standard linguistic metrics (BLEU, CIDEr), semantic similarity (BERTScore), and specialized structural metrics (character-matching, event diversity) alongside human evaluation of narrativity, character grounding, and overall coherence.

**Task Characteristics:**
Open-ended generation constrained by visual sequences. The domain spans three distinct tracks: Strict (limited training resources), Open (unconstrained pre-training), and Grounding (requiring explicit text-to-image entity mapping). This emphasizes the intersection of creative synthesis, temporal reasoning, and spatial-semantic precision within a vision-to-text framework.",cfda60d69d39674549d15a78e65fc5ec2f81de7f:Visually Grounded Story Generation (VGSG) Challenge,Visually Grounded Story Generation (VGSG) Challenge,https://github.com/vwprompt/vwp,2.0,GPU/Local,3.0,False,True,True,huggingface,openai; google,multimodal
d3a95b0727ef67ef9264cd20da10a545977ace51,TwistList: Resources and Baselines for Tongue Twister Generation,TwistList: Resources and Baselines for Tongue Twister Generation,2023.0,2023.0,,Poetry & Verse,Humor & Satire,high,"The paper focuses on generating text under rigid phonetic constraints such as alliteration and consonance, which aligns with the structured linguistic forms and metrical/tonal constraints defined in the Poetry & Verse domain, while the entertainment and wordplay aspects link it to Humor & Satire.","**Creative Artifacts Evaluated:**
The primary artifacts are **tongue twisters**: short-form text sequences characterized by dense phonetic repetition, alliteration, and consonance. These are generated as keyword-to-twister outputs, where the resulting text must be semantically grounded in specific input prompts.

**Creative Capabilities Assessed:**
The research assesses **phonetic complexity** (the density of repeating initial and internal phonemes) and **semantic relevance** (the ability to integrate keywords logically). It further measures **linguistic fluency**, **thematic cohesion**, and **entertainment value**, evaluating the model's capacity to balance strict phonological constraints with grammatical and narrative sense.

**Evaluation Context:**
This is a **text-based** generation task using Large Language Models (e.g., GPT-3, T5). Evaluation is a **hybrid framework** combining standard NLP metrics (BLEU, ROUGE, BERTScore) with domain-specific **Phonetic Overlap (PO)** and **Initial Phonetic Overlap (Init-PO)** scores. These are supplemented by human Likert-scale ratings focusing on articulation difficulty and amusement.

**Task Characteristics:**
The work centers on **constrained creative generation** and **computational wordplay**. Unlike open-ended storytelling, this task requires satisfying rigid phonological patterns while adhering to external semantic anchors, representing a specialized intersection of phonetic optimization and creative linguistic expression.",d3a95b0727ef67ef9264cd20da10a545977ace51:TwistList,TwistList,https://github.com/tangg555/TwistList,2.0,GPU/Local,1.0,False,True,True,huggingface,openai,text
d0653e0910d34f34c24f97052886bed5067eb64f,Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,Amuse: Human-AI Collaborative Songwriting with Multimodal Inspirations,2024.0,2024.0,,Music & Auditory Arts,Poetry & Verse,high,"The paper focuses on a collaborative system for generating symbolic chord progressions, audio renderings, and song choruses, integrating harmonic structures with lyrical content.","**Creative Artifacts Evaluated:**
The research evaluates symbolic musical chord progressions (represented as text) and their corresponding audio renderings. It also assesses complete 8-bar song choruses that integrate harmonic structures with lyrical and melodic potential.

**Creative Capabilities Assessed:**
Assessment focuses on musical coherence (structural and harmonic logic), diversity (the variety and uniqueness of generated sequences), and keyword relevance (semantic alignment with multimodal prompts). From a human-AI interaction perspective, it measures perceived creativity, user agency, and the system’s efficacy as a creativity support tool using the Creativity Support Index (CSI).

**Evaluation Context:**
The evaluation is situated in a multimodal, LLM-driven songwriting environment. It utilizes a hybrid scoring methodology: automatic metrics (Self-BLEU for diversity and Jensen-Shannon Divergence for distributional coherence), human pairwise preference tests of audio clips, and a within-subjects user study with experienced songwriters involving Likert scales and semi-structured interviews.

**Task Characteristics:**
Tasks range from constrained generation (producing specific chord sequences based on textual/multimodal prompts) to open-ended co-creation (collaborative composition of a chorus). The workflow emphasizes the transition from AI-assisted ideation to iterative human-led refinement, highlighting the system's role in a collaborative creative process.",d0653e0910d34f34c24f97052886bed5067eb64f:Technical Evaluation of Chord Progression Generation,Technical Evaluation of Chord Progression Generation,https://github.com/elianakim/Amuse,3.0,Special,3.0,False,False,True,github,openai,multimodal
d01f6e76e67a445f23f807c0d3e68fa0be9a2c9e,The Next Chapter: A Study of Large Language Models in Storytelling,The Next Chapter: A Study of Large Language Models in Storytelling,2023.0,2023.0,,Narrative & Story Writing,Functional & Professional Writing,high,"The paper evaluates LLMs on fictional storytelling (ROCStories, WritingPrompts) and journalistic news generation (CNN News), focusing on narrative coherence and creative synthesis.","**Creative Artifacts Evaluated:**
The study evaluates three distinct types of text-based narratives: short-form commonsense stories (ROCStories), mid-length fictional narratives (WritingPrompts), and long-form journalistic news stories (CNN News). 

**Creative Capabilities Assessed:**
Assessment focuses on narrative fluency, logical coherence, and thematic relatedness to prompts. Beyond standard linguistic quality, the research measures ""interestingness"" and creative originality. A key capability assessed is the model's ability to generate novel content without resorting to lexical plagiarism or ""soft plagiarism"" (the reproduction of specific plot points, named entities, or unique scenarios from the training data).

**Evaluation Context:**
This is a text-only LLM benchmark comparing multiple models using a hybrid evaluation framework. It synthesizes automatic NLP metrics (e.g., BLEU, BERTScore, and lexical repetition) with human-centered Likert scales. Uniquely, it incorporates professional plagiarism detection software (iThenticate) and manual human auditing to distinguish between genuine creative synthesis and data memorization.

**Task Characteristics:**
The tasks involve constrained, open-ended generation where models must expand brief prompts or titles into structured narratives of varying lengths (4 to 20 sentences). The work distinguishes itself by moving beyond simple generation to include a critical evaluation phase, treating the detection of ""soft plagiarism"" as a necessary component of assessing machine creativity.",d01f6e76e67a445f23f807c0d3e68fa0be9a2c9e:Conditional Story Generation Evaluation,Conditional Story Generation Evaluation,https://github.com/ZhuohanX/TheNextChapter,3.0,Special,3.0,False,False,True,github,openai,text
d0e0942d5f2074227060c2dfdd6bdadb6b9f8ab8,BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle,BottleHumor: Self-Informed Humor Explanation using the Information Bottleneck Principle,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper explicitly focuses on explaining and deconstructing humor and satire in multimodal artifacts like memes and cartoons, requiring the interpretation of irony, visual metaphors, and incongruity resolution.","**Creative Artifacts Evaluated:**
Internet memes with associated titles, New Yorker cartoons with captions, and ""YesBut"" satirical image pairs featuring contrasting ""yes"" and ""but"" panels. These artifacts represent complex, multimodal humor that relies on the specific interplay between visual imagery and textual context.

**Creative Capabilities Assessed:**
Conceptual depth, irony detection, and semantic synthesis. The research measures the model's ability to decode satirical intent, identify incongruities between visual and textual elements, and articulate the underlying logic of humor. It specifically assesses the capacity to extract and communicate the ""atomic facts"" that constitute a joke's premise and punchline.

**Evaluation Context:**
A multimodal (vision-language) framework involving high-complexity reasoning. Performance is quantified using an LLM-as-a-judge (Gemini-Flash-1.5) to calculate precision, recall, and macro-F1 scores. This approach moves beyond subjective Likert scales to a fact-based verification of whether the model captured the essential creative elements and cultural nuances of the humor.

**Task Characteristics:**
Open-ended interpretive generation. The tasks require cross-modal reasoning to explain the ""why"" behind creative artifacts, moving beyond simple image description to the deconstruction of humor. This involves identifying cultural references, visual metaphors, and the resolution of incongruity within satirical and humorous contexts.",,,,,,,,,,,,
cdb0e126f03caeb95ad947e13180d3219a2ffe04,I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors,I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors,2023.0,2023.0,,Visual Arts & Stylized Imagery,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the generation and evaluation of visual metaphor images, requiring the translation of abstract figurative language into symbolic visual motifs through conceptual blending and lateral thinking.","**Creative Artifacts Evaluated:**
Visual metaphor images representing abstract linguistic concepts through conceptual blending. These artifacts are multimodal syntheses that translate figurative language (e.g., ""time is a thief"") into concrete, often surreal, visual scenes.

**Creative Capabilities Assessed:**
Conceptual depth, visual-linguistic translation, and ""fixability"" (measured by the number of suggested edits). The benchmarks assess the models' ability to perform complex ideation and divergent thinking by mapping abstract metaphors to symbolic visual elements while maintaining semantic coherence and aesthetic quality.

**Evaluation Context:**
A multimodal pipeline integrating LLMs (for conceptual elaboration) and Diffusion models (for image synthesis). Evaluation is predominantly expert-led, employing professional illustrators to provide high-granularity human judgments, such as ""Lost Cause"" vs. ""Perfect"" classifications and pairwise preference rankings. It also includes automatic visual entailment tasks to test machine ""understanding"" of the generated metaphors.

**Task Characteristics:**
Cross-modal transfer (linguistic to visual) and constrained ideation. The tasks involve open-ended generation governed by specific metaphoric constraints, alongside comparative analysis of prompting strategies (Chain-of-Thought vs. Completion) to determine the most effective method for generating conceptually rich and interpretable visual representations.",cdb0e126f03caeb95ad947e13180d3219a2ffe04:HAIVMet (Human-AI Visual Metaphor),HAIVMet (Human-AI Visual Metaphor),https://zenodo.org/record/7965907,3.0,Special,3.0,False,False,True,url,openai,multimodal
cbf052125c90f78d8750f7d447121193ef53d2f8,FunQA: Towards Surprising Video Comprehension,FunQA: Towards Surprising Video Comprehension,2023.0,2023.0,,Humor & Satire,Performing Arts & Video Synthesis; Lateral Thinking & Creative Problem Solving,high,"The paper introduces a benchmark specifically for 'surprising' video content, focusing on the interpretation of humor, magic tricks, and counter-intuitive actions that require resolving incongruity and understanding performance-based creativity.","**Creative Artifacts Evaluated:**
Multimodal video content featuring counter-intuitive and ""surprising"" sequences, specifically humorous skits, innovative/creative human actions, and magic tricks. Generated outputs include natural language explanations (reasoning), objective event descriptions, and concise, vivid titles.

**Creative Capabilities Assessed:**
The benchmark measures the ability to comprehend ""surprisingness"" through counter-intuitive reasoning and causal explanation. It assesses temporal sensitivity (identifying the ""climax"" or ""hook"" of a creative act), conceptual depth (explaining *why* a trick or joke works), and the ability to quantify creative intensity via numerical scoring.

**Evaluation Context:**
A large-scale VideoQA framework designed for Vision-Language Models (VLMs). Evaluation is multifaceted, employing traditional NLP metrics (BLEU, CIDEr), semantic similarity (BLEURT), LLM-based qualitative scoring (GPT-4), and temporal Intersection-over-Union (IoU) for localization tasks. It contrasts surprising content against standard daily-life reasoning (NExT-QA).

**Task Characteristics:**
The framework integrates open-ended generation (reasoning and description) with constrained generation (titling), evaluation/judgment (creativity scoring), and temporal grounding. It shifts the focus from simple action recognition to the high-level cognitive interpretation of visual anomalies and deceptive sequences across three distinct sub-domains: humor, creativity, and magic.",cbf052125c90f78d8750f7d447121193ef53d2f8:FunQA,FunQA,https://github.com/Fun-QA/FunQA,2.0,GPU/Local,13.0,False,True,True,huggingface,openai,video
ce3af93ca7e04fbb7dfdf813f109655739433139,Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation and Composition Style Transfer,Harmonizing Pixels and Melodies: Maestro-Guided Film Score Generation and Composition Style Transfer,2024.0,2024.0,,Music & Auditory Arts,Performing Arts & Video Synthesis,high,"The paper's core focus is the generation and style transfer of cinematic musical scores, emphasizing harmonic logic and rhythmic synchronization with visual media.","**Creative Artifacts Evaluated**
The primary artifacts are 10-second cinematic musical scores and composition style transfers. These are multimodal outputs where audio is synthesized to align with the semantic, temporal, and emotional cues of film clips or music videos.

**Creative Capabilities Assessed**
The research measures ""creative tension"" through two primary axes: originality (novelty via feature variance) and recognizability (stylistic adherence via classification accuracy). It also assesses technical alignment through rhythm correspondence (beat and hit synchronization), melodic accuracy, and macro/micro dynamics correlation, alongside standard music quality metrics like fidelity, diversity, and generation stability.

**Evaluation Context**
Evaluation is conducted in a multimodal (video-to-audio) setting using diffusion-based models. It employs a mixed-methods approach, combining objective signal-based metrics (FAD, KL divergence) with subjective human Mean Opinion Scores (MOS) and a specialized classification-based framework to quantify the balance between artistic innovation and genre-specific expectations.

**Task Characteristics**
The work focuses on constrained generation and cross-domain transfer. Tasks include generating scores from visual prompts, performing composition style transfer (mapping target melodies and dynamics onto new visual contexts), and evaluating the trade-off between generating unique content versus maintaining recognizable stylistic signatures.",ce3af93ca7e04fbb7dfdf813f109655739433139:FilmScoreDB,FilmScoreDB,https://anonymous.4open.science/r/HPM,2.0,GPU/Local,2.0,False,False,True,github,none,multimodal
d45776f8d75b2505e5fd0349275896053f5a4ead,Generating similes like a Pro: A Style Transfer Approach for Simile Generation,Generating similes like a Pro: A Style Transfer Approach for Simile Generation,2020.0,2020.0,,Figurative Language & Rhetoric,Narrative & Story Writing,high,"The paper focuses specifically on the generation and evaluation of similes, a core component of figurative language, and applies these to enhance machine-generated narrative stories.","**Creative Artifacts Evaluated:**
The primary artifacts are similes (figurative comparisons) and short machine-generated narrative stories that have been modified to include these similes.

**Creative Capabilities Assessed:**
The research measures figurative mapping ability, specifically focusing on the semantic relevance between the ""vehicle"" (the comparison object) and the ""topic"" or ""property."" It assesses originality (novelty), aesthetic quality, and ""evocativeness""—the capacity of a linguistic device to enhance the sensory or emotional impact of a narrative.

**Evaluation Context:**
This text-based study utilizes LLMs for style transfer. Evaluation is a hybrid of automatic NLP metrics (BLEU, BERTScore, Novelty) and multi-dimensional human assessment. Human judges provide 1-5 ratings for specific creative dimensions—such as vehicle-topic relevance and overall creativity—and perform pairwise comparisons to determine the qualitative impact of similes on downstream story generation.

**Task Characteristics:**
The work involves constrained generation (transforming literal sentences into figurative ones) and creative refinement (integrating similes into existing stories). It focuses on a specific form of cross-domain transfer—mapping literal concepts to figurative vehicles—to improve the creative depth and evocativeness of machine-generated prose.",d45776f8d75b2505e5fd0349275896053f5a4ead:Simile Generation from Literal Sentences,Simile Generation from Literal Sentences,https://github.com/tuhinjubcse/SimileGeneration-EMNLP2020,2.0,GPU/Local,1.0,False,False,True,github,none,text
d948d316bc2c74d8d31d0128e68e1e813642fb40,MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search,MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search,2025.0,2025.0,,Scientific Discovery,Lateral Thinking & Creative Problem Solving,high,"The paper focuses specifically on the generation of novel, detailed scientific hypotheses and experimental protocols in chemistry, which is the core definition of the Scientific Discovery domain.","**Creative Artifacts Evaluated:**
The primary artifacts are detailed, actionable scientific hypotheses within the chemistry domain. These include specific chemical mechanisms, proposed experimental protocols, molecular interaction predictions, and structured research formulations derived from broad scientific directions.

**Creative Capabilities Assessed:**
The benchmark measures scientific originality (novelty) and technical feasibility (usefulness). It specifically assesses conceptual depth and elaboration through ""fine-grained"" detailing, requiring the model to move beyond vague suggestions to specific, testable propositions. The evaluation also captures the model’s ability to perform convergent scientific reasoning by aligning generated ideas with established scientific breakthroughs.

**Evaluation Context:**
This is a text-based evaluation focused on specialized scientific knowledge. The assessment framework is multi-modal in its methodology, utilizing LLM-based pairwise comparisons for qualitative traits (effectiveness, novelty, detailedness), human expert ranking for domain-specific validity, and objective Soft/Hard Recall metrics that measure the model's ability to replicate ground-truth hypotheses from published literature.

**Task Characteristics:**
The task is a constrained, fine-grained ideation process. It requires synthesizing a research background and methodological survey into a specific hypothesis based on a coarse-grained prompt. This represents a complex form of informed ideation and refinement, emphasizing the transition from abstract conceptualization to concrete scientific discovery within a high-stakes, specialized domain.",d948d316bc2c74d8d31d0128e68e1e813642fb40:Fine-grained Scientific Hypothesis Discovery Benchmark,Fine-grained Scientific Hypothesis Discovery Benchmark,https://github.com/ZonglinY/MOOSE-Chem2,1.0,API-Only,1.0,True,False,True,github,openai; google,text
d3e916564f8d009266df3eeef0ad75d3f83b6813,Benchmarking Cognitive Domains for LLMS: Insights from Taiwanese Hakka Culture,Benchmarking Cognitive Domains for LLMS: Insights from Taiwanese Hakka Culture,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Functional & Professional Writing,high,"The paper benchmarks LLMs on higher-order cognitive skills such as cultural synthesis and the generation of novel conceptual solutions, which aligns with creative problem-solving, while utilizing Bloom's Taxonomy and educational psychology frameworks.","**Creative Artifacts Evaluated:**
Textual ideation proposals, innovative conceptual solutions, and critical assessments formatted as multiple-choice selections. While the format is closed-ended, the content focuses on the synthesis of cultural elements into novel ideas and creative expressions.

**Creative Capabilities Assessed:**
Cultural synthesis, conceptual depth, and the ability to identify or generate innovative ideas (ideation) within a specific heritage context. The benchmark measures higher-order cognitive skills, specifically the capacity for critical judgment and the synthesis of traditional cultural elements into novel, practical, or artistic applications.

**Evaluation Context:**
A text-based, closed-ended benchmark utilizing multiple-choice questions (MCQs). Evaluation is strictly automatic based on accuracy, assessing LLMs across a hierarchy of cognitive complexity derived from Bloom’s Taxonomy. It focuses on how cultural knowledge is processed from basic recall to complex creative synthesis.

**Task Characteristics:**
The benchmark employs a structured progression from lower-order tasks (Remembering, Understanding) to higher-order creative tasks (Evaluating, Creating). It features constrained ideation and judgment tasks requiring the application of Taiwanese Hakka cultural knowledge to solve novel problems. This distinguishes the work from general-purpose creativity benchmarks by its deep integration of specific cultural heritage and educational psychology frameworks.",d3e916564f8d009266df3eeef0ad75d3f83b6813:Hakka Cultural Knowledge Cognitive Domain Dataset,Hakka Cultural Knowledge Cognitive Domain Dataset,,,Unknown,6.0,False,False,False,unknown,,unknown
db35bb43daad7d7f85dfbbe0f9cc7b77b6e57272,GAVEL: Generating Games Via Evolution and Language Models,GAVEL: Generating Games Via Evolution and Language Models,2024.0,2024.0,,Programming & Algorithmic Creativity,Lateral Thinking & Creative Problem Solving,high,"The paper focuses on generating functional game rules in a formal domain-specific language (L-GDL), requiring both technical code generation and the creative ideation of novel, balanced gameplay mechanics.","**Creative Artifacts Evaluated:**
The primary artifacts are novel board game rulebooks encoded in Ludii Game Description Language (L-GDL). These artifacts are formal, code-like specifications that define game boards, piece types, movement rules, turn structures, and terminal win/loss conditions.

**Creative Capabilities Assessed:**
The framework measures functional viability alongside strategic complexity. Assessed capabilities include technical correctness (compilability and playability) and gameplay quality metrics such as strategic depth (MCTS vs. random agent performance), balance, agency, and decisiveness. Human experts further evaluate subjective creative qualities, specifically assessing novelty, engagement, and the conceptual coherence of the generated game mechanics.

**Evaluation Context:**
This is a text-based code generation task where LLMs function as mutation and crossover operators within an evolutionary algorithm. The evaluation is a hybrid of automated MCTS self-play, which generates objective gameplay statistics, and qualitative expert playtesting used to validate the strategic potential and ""fun factor"" of high-fitness candidates.

**Task Characteristics:**
The work involves constrained generation within a domain-specific language, requiring the model to maintain syntactic rigor while performing open-ended mechanical ideation. It emphasizes iterative refinement and search-based optimization to navigate the vast, non-linear state space of game design, distinguishing it from simple one-shot text generation.",db35bb43daad7d7f85dfbbe0f9cc7b77b6e57272:GAVEL Game Generation and Evaluation Framework,GAVEL Game Generation and Evaluation Framework,https://github.com/gdrtodd/gavel,3.0,Special,2.0,False,True,True,huggingface,none,text
db82caf8d2bab9decfc863e711735839807fa6b8,Collective Critics for Creative Story Generation,Collective Critics for Creative Story Generation,2024.0,2024.0,,Narrative & Story Writing,Dialogue Generation & Social Interaction; Figurative Language & Rhetoric,high,"The paper focuses on the hierarchical generation of fictional prose and narrative plans, while emphasizing the interactive, multi-agent collaborative process and granular stylistic sentence-level enhancements.","**Creative Artifacts Evaluated:**
The research evaluates structured story plans (narrative outlines and plot points) and refined long-form prose, specifically focusing on sentence-level stylistic enhancements and expressive sentence transformations within a broader narrative context.

**Creative Capabilities Assessed:**
Assessment focuses on narrative originality, logical coherence, and stylistic expressiveness. Key metrics include ""interestingness"" and ""creativity,"" alongside the model's ability to maintain writing style consistency. Furthermore, the work evaluates interactive alignment—the capacity of the AI to successfully incorporate and respond to iterative human critiques during the writing process.

**Evaluation Context:**
The evaluation is situated within a text-based multi-agent LLM framework (""Collective Critics""). It utilizes a mixed-methods approach, employing human pairwise comparisons and GPT-4-based automatic proxies to judge subjective creative quality. A human-in-the-loop user study specifically measures the effectiveness of AI as a collaborative writing partner, tracking ""edit"" and ""acceptance"" rates of AI suggestions.

**Task Characteristics:**
Tasks encompass open-ended generation (developing plans from a premise), constrained refinement (improving existing prose), and interactive ideation. The work is distinguished by its focus on the hierarchical storytelling process, separating high-level conceptual planning from granular stylistic editing, and emphasizing the role of iterative critique in creative refinement.",db82caf8d2bab9decfc863e711735839807fa6b8:Creative Story Plan Generation Evaluation,Creative Story Plan Generation Evaluation,https://github.com/EMNLP-2024-CritiCS/Collective-Critics-for-Creative-Story-Generation,1.0,API-Only,1.0,True,False,True,github,openai,text
d85d49d4429a63244286f7def1ec5c09490d6cc7,ViPE: Visualise Pretty-much Everything,ViPE: Visualise Pretty-much Everything,2023.0,2023.0,,Figurative Language & Rhetoric,Visual Arts & Stylized Imagery,high,The paper's core contribution is the interpretation and cross-domain mapping of abstract figurative language (metaphors and idioms) into concrete visual descriptions for image synthesis.,"**Creative Artifacts Evaluated:**
The research evaluates textual interpretations of figurative language, detailed visual elaboration prompts, synthesized images derived from metaphors, and creative transformations of emotionally charged tweets and song lyrics.

**Creative Capabilities Assessed:**
Assessment focuses on the ability to interpret abstract figurative language (metaphors and idioms) and translate it into concrete, descriptive imagery. Key capabilities include semantic alignment across modalities, the preservation of emotional valence during visual transformation, conceptual depth in prompt engineering, and the safety/appropriateness of generated creative content.

**Evaluation Context:**
The evaluation is multimodal, bridging LLM-based text generation with diffusion-based image synthesis. It employs a hybrid scoring approach: automatic metrics (CIDEr, CLIP-based cosine similarity, and classification accuracy) are used for scale, while human preference studies validate the aesthetic and conceptual relevance of the visual elaborations.

**Task Characteristics:**
Tasks center on cross-domain transfer (mapping abstract text to visual prompts) and constrained generation. This includes ""visual elaboration""—the process of expanding figurative phrases into descriptive scenes—and evaluative judgment through Winograd-style figurative QA and emotion preservation benchmarks. The work distinguishes itself by focusing on the intermediate cognitive step between understanding a metaphor and visualizing it.",d85d49d4429a63244286f7def1ec5c09490d6cc7:Fig-QA,Fig-QA,https://github.com/Hazel1994/ViPE,2.0,GPU/Local,1.0,False,True,True,huggingface,none,text
d6b9dd094941944a74d6c72fa6dfc6b79470df72,Few-shot multi-token DreamBooth with LoRa for style-consistent character generation,Few-shot multi-token DreamBooth with LoRa for style-consistent character generation,2025.0,2025.0,,Visual Arts & Stylized Imagery,,high,The paper focuses on generating stylized character images and evaluating their aesthetic consistency and stylistic fidelity using diffusion models and professional artist assessments.,"**Creative Artifacts Evaluated:**
The primary artifacts are novel artistic character images. These are specifically stylized character designs generated from a limited reference set (10–30 images) representing a distinct, pre-defined artistic style.

**Creative Capabilities Assessed:**
The framework measures stylistic fidelity (the ability to replicate specific aesthetic markers of a reference style), visual diversity (the capacity to generate varied character forms without repetition or mode collapse), and stylistic consistency. It assesses the model's ability to generalize a visual grammar to new subjects while maintaining aesthetic coherence.

**Evaluation Context:**
This research operates within the image-based diffusion model domain, specifically utilizing DreamBooth and LoRa. Evaluation is hybrid: automatic quantitative metrics use embedding-based cosine similarity and standard deviation to proxy fidelity and diversity, while human evaluation employs Bradley-Terry modeling for global rankings and 1–5 Likert-scale assessments by professional artists to judge nuanced aesthetic alignment.

**Task Characteristics:**
The core task is few-shot, style-constrained character generation. It involves open-ended synthesis where the model must invent new character entities while adhering to the strict visual constraints of a small dataset. The benchmark also includes evaluative tasks where both algorithms and humans judge the success of stylistic transfer and creative variety.",d6b9dd094941944a74d6c72fa6dfc6b79470df72:Style-Consistent Character Generation,Style-Consistent Character Generation,,3.0,Special,5.0,False,False,True,request_access,none,image
dffa7d18141338a4863302ddcf44bf0199e36c13,Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint,Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper evaluates rebus puzzles, which are explicitly framed as visual-linguistic riddles requiring lateral thinking and the decoding of figurative idioms and metaphors.","**Creative Artifacts Evaluated:**
The benchmark evaluates hand-annotated rebus puzzles—visual-linguistic riddles where images, typography, and spatial arrangements represent idiomatic phrases. Outputs include text-based answer phrases and natural language reasoning chains explaining the visual-to-linguistic mapping.

**Creative Capabilities Assessed:**
The dataset measures lateral thinking and abstract reasoning, specifically the ability to decode visual metaphors involving phonetics, spatial positioning, color, and size. It assesses convergent thinking (identifying a single correct idiom from abstract cues) and the capacity for iterative refinement and self-correction when provided with feedback or cognitive hints.

**Evaluation Context:**
This is a multimodal evaluation comparing Vision-Language Models (VLMs) against text-only baselines (using image captions). Scoring is a hybrid of automatic string matching (Naive Matching) and semantic equivalence judging performed by high-capability LLMs (GPT-4o and Qwen3-8B), alongside contrastive retrieval metrics like Recall@K and Mean Reciprocal Rank (MRR).

**Task Characteristics:**
The tasks involve constrained problem-solving and cross-modal transfer. They range from zero-shot and in-context generation to skill-guided prompting and iterative refinement. The core challenge lies in bridging the gap between literal visual perception and figurative interpretation, distinguishing it from standard object recognition or captioning tasks.",dffa7d18141338a4863302ddcf44bf0199e36c13:Rebus Puzzle Probe Dataset,Rebus Puzzle Probe Dataset,https://github.com/Kyunnilee/visual_puzzles,1.0,API-Only,6.0,True,True,True,huggingface,openai; google; anthropic; huggingface,multimodal
e030d0ee02b101ebd467d61684c0ff86ab782258,Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results,Multimodal Prediction of Spontaneous Humour: A Novel Dataset and First Results,2022.0,2022.0,,Humor & Satire,Dialogue Generation & Social Interaction,high,"The paper explicitly focuses on the recognition and interpretation of spontaneous humor and its social functions, such as rapport-building and intent, within a professional conversational context.","**Creative Artifacts Evaluated:**
Spontaneous verbal humor segments (2-second audio/speech clips) captured from professional football press conferences. These represent unscripted, real-world linguistic artifacts rather than prepared comedic scripts or staged performances.

**Creative Capabilities Assessed:**
The benchmark measures the capacity for humor recognition (identifying humorous intent), social-emotional attribution (distinguishing self-directed from others-directed humor), and affective valence (differentiating between positive and negative sentiment). It specifically targets the ability to decode the social function and directionality of creative expression within a specific professional subculture.

**Evaluation Context:**
A multimodal classification framework utilizing automatic scoring (Area Under the Curve). The evaluation context is highly domain-specific, focusing on the high-pressure, public-facing environment of sports media, where humor serves as a strategic social tool for rapport-building or deflection.

**Task Characteristics:**
Evaluation and judgment tasks focused on discriminative classification. Unlike benchmarks for generative creativity, this work emphasizes the *interpretation* and *contextualization* of spontaneous creative acts. It requires models to navigate the nuances of ""spontaneous"" vs. ""prepared"" creativity, focusing on the refinement of social context, intent, and target within a constrained, real-world professional domain.",e030d0ee02b101ebd467d61684c0ff86ab782258:Passau-Spontaneous Football Coach Humor (Passau-SFCH),Passau-Spontaneous Football Coach Humor (Passau-SFCH),,2.0,GPU/Local,3.0,False,False,True,request_access,none,multimodal
df15665ebb3896cb9bb296535af14e9065b4ba29,KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models,KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,,high,"The paper's focus on visual analogical reasoning, relational mapping, and conceptual abstraction through structural isomorphisms directly matches the core criteria of the Lateral Thinking & Creative Problem Solving domain.","**Creative Artifacts Evaluated:**
Image pairs depicting discrete visual and semantic transformations of real-world objects. These include physical modifications (changes in color, size, or orientation) and more complex, abstract transformations (changes in material, part-whole relationships, or functional state).

**Creative Capabilities Assessed:**
Visual analogical reasoning and relational mapping. The benchmark measures the model’s capacity for conceptual abstraction (inferring a transformation rule) and convergent thinking (applying that rule to a novel instance). It specifically evaluates the depth of understanding through a cognitive hierarchy: general domain classification, precise verbal specification, and visual extrapolation to new stimuli.

**Evaluation Context:**
A multimodal (image-to-text and image-to-image) assessment framework that benchmarks Large Multimodal Models (LMMs) against human developmental stages. It utilizes two distinct difficulty tiers—one solvable by young children (KiVA) and a more complex version requiring adult-level logic (KiVA-adults)—with performance measured through automated multiple-choice accuracy.

**Task Characteristics:**
Constrained problem-solving and cross-domain rule transfer. Rather than open-ended generation, the tasks focus on the ""understanding"" and ""application"" phases of creative intelligence. Models must demonstrate the ability to map abstract relationships from a source pair to a target pair across different object categories, testing the robustness of their internal conceptual representations.",df15665ebb3896cb9bb296535af14e9065b4ba29:KiVA (Kid-inspired Visual Analogies),KiVA (Kid-inspired Visual Analogies),https://github.com/ey242/KiVA,2.0,GPU/Local,3.0,False,False,True,github,openai,multimodal
deffecb4aa157fc2be084e4a4560f954eb4b8a4d,A Novel Mathematical Framework for Objective Characterization of Ideas,A Novel Mathematical Framework for Objective Characterization of Ideas,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Engineering & Technical Design; Mathematical Reasoning,high,"The paper focuses on quantifying divergent thinking and ideational diversity (core components of lateral thinking) within the specific context of engineering and industrial design, using a formal mathematical framework to characterize the 'idea space'.","**Creative Artifacts Evaluated:**
Textual product design concepts and conceptual solution descriptions addressing six specific engineering and industrial design problem statements.

**Creative Capabilities Assessed:**
Divergent thinking and ideational diversity (quantified via Idea Sparsity, Cluster Sparsity, and Distribution Scores), semantic coherence of conceptual embeddings, novelty/originality, and the practical utility of clustered representations for human selection.

**Evaluation Context:**
A hybrid framework integrating LLM-generated text with human validation from both expert designers (N=30) and novices (N=40). Evaluation methods combine automated mathematical metrics (PCA eigenvalues), existing NLP-based creativity benchmarks (SemDis, OCSAI), and multimodal 2D visual plots for human-in-the-loop assessment of idea distribution and dispersion.

**Task Characteristics:**
Large-scale open-ended ideation (generating 100 ideas per prompt), semantic similarity judgment (triplet testing), and constrained selection tasks within a structured idea space. The work focuses on the objective quantification of a global ""idea space"" and the distribution of generated content rather than subjective individual scoring, distinguishing it from traditional qualitative design assessments.",deffecb4aa157fc2be084e4a4560f954eb4b8a4d:Mathematical Framework for Objective Characterization of Ideas,Mathematical Framework for Objective Characterization of Ideas,,1.0,API-Only,4.0,True,False,True,unknown,openai,multimodal
e1612a809298c9cc9a70c3a17b1ddb7b5df9507e,Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings,Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings,2025.0,2025.0,,Humor & Satire,Figurative Language & Rhetoric; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the translation of puns, which are explicitly listed under Humor & Satire, while requiring linguistic mapping and divergent thinking to resolve cross-lingual wordplay constraints.","**Creative Artifacts Evaluated:**
The research evaluates English-to-French pun translations, specifically focusing on sentences containing homographic (same spelling, different meaning) and homophonic (same sound, different meaning) wordplay. It also assesses intermediate artifacts, including bilingual synonym lists and phonetic-semantic embeddings designed to bridge cross-lingual linguistic gaps.

**Creative Capabilities Assessed:**
Capabilities include the ability to preserve dual-meaning structures across languages, linguistic equivalence, and ""authenticity"" (naturalness in the target language). The system is measured on its divergent thinking (finding semantic overlaps) and convergent execution (maintaining sentence fluency and emotional resonance).

**Evaluation Context:**
This text-based study employs a multi-agent LLM framework. Evaluation is highly hybrid, integrating traditional automatic metrics (BLEU, BERTScore), bilingual embedding similarity, and a panel of LLM agents that rate qualitative aspects like ""mistranslation"" and ""emotion."" These are further validated by human expert judgment.

**Task Characteristics:**
The work centers on constrained generation (translating while maintaining specific wordplay) and cross-lingual transfer. It involves a complex pipeline of identification, translation, and refinement, distinguishing itself by using contrastive learning to discriminate between successful wordplay and literal translation, specifically targeting the intersection of phonetic similarity and semantic divergence.",e1612a809298c9cc9a70c3a17b1ddb7b5df9507e:CLEF JOKER 2025 Task 2: Wordplay Translation,CLEF JOKER 2025 Task 2: Wordplay Translation,https://github.com/dsgt-arc/joker-2025,2.0,GPU/Local,1.0,False,False,True,request_access,openai; google,text
decb581f0c991693a687221be39877791285de1a,PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement,PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement,2025.0,2025.0,,Visual Arts & Stylized Imagery,Poetry & Verse; Narrative & Story Writing; Figurative Language & Rhetoric,high,"The research focuses on generating stylized visual sequences from poetic text, requiring the translation of abstract metaphors into visual art while maintaining narrative consistency across a series of images.","**Creative Artifacts Evaluated:**
The primary artifacts are **narrative image sequences** and **illustrative visual representations** generated from a specialized corpus of 1,111 annotated **poems**. These images are designed to visually manifest specific poetic themes, protagonists, and shifting emotional moods across stanzas.

**Creative Capabilities Assessed:**
The research evaluates **cross-modal semantic alignment** and **emotional resonance**, specifically the model's ability to translate abstract poetic metaphors into visual atmospheres. It also assesses **narrative consistency**, measuring the stability of character identity and artistic style across a sequence of images derived from a single literary work.

**Evaluation Context:**
This is a **multimodal (text-to-image)** study utilizing **Diffusion models**. Evaluation is conducted through a hybrid framework: **human expert scoring** on a 1-5 scale for subjective resonance and **specialized automatic metrics**, including BLIP, Long-CLIP, and custom **Emotion and Character Consistency CLIP scores**, to quantify alignment and temporal stability.

**Task Characteristics:**
The work centers on **cross-domain transfer** and **constrained generation**. It employs **multi-stage prompt refinement** to mitigate information loss, transforming raw poetic text into structured visual prompts. The task is distinguished by its focus on **sequential storytelling** and the preservation of complex literary nuances during the generative process.",decb581f0c991693a687221be39877791285de1a:P4I (PoemForImage) Dataset,P4I (PoemForImage) Dataset,https://github.com/SofeeyaJ/PoemTale-Diffusion,2.0,GPU/Local,1.0,False,False,True,github,openai,multimodal
d5d661b43692a79d760e0b2011f056a459eb374c,Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?,Diffusion Models as Artists: Are we Closing the Gap between Humans and Machines?,2023.0,2023.0,,Visual Arts & Stylized Imagery,Lateral Thinking & Creative Problem Solving,high,The paper evaluates the generation of sketches and drawings (Visual Arts) through the lens of divergent thinking and originality metrics (Lateral Thinking).,"**Creative Artifacts Evaluated:**
One-shot generated handwritten characters and simplified line drawings or sketches representing diverse visual concepts.

**Creative Capabilities Assessed:**
The research measures the trade-off between originality (distance from a single exemplar) and recognizability (semantic validity). It specifically assesses diversity (divergent thinking) and perceptual alignment, comparing machine feature importance against human visual saliency to determine if models prioritize the same features as human artists during the creative process.

**Evaluation Context:**
The study focuses on one-shot generation using diffusion models (CFGDM) and multimodal architectures. Evaluation is primarily automatic, utilizing SimCLR-based distance metrics for originality and classifier-based scoring for recognizability. This is supplemented by human-generated feature importance maps (ClickMe-QuickDraw) to validate the alignment of machine vs. human creative strategies.

**Task Characteristics:**
Tasks involve one-shot constrained generation, where models must produce novel variations of a single input image. It includes feature importance mapping and comparative analysis between machine-generated heatmaps and human perceptual data, moving beyond simple output quality to evaluate the underlying visual strategies and internal representations used in creative tasks.",d5d661b43692a79d760e0b2011f056a459eb374c:Diversity vs. Recognizability Framework,Diversity vs. Recognizability Framework,https://github.com/VictorBoutin/DiffusionModelsAsArtists,2.0,GPU/Local,1.0,False,False,True,github,none,image
d607efe5c974cf8ce06cfee60103417f0de4c25b,Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX,Eye of Judgement: Dissecting the Evaluation of Russian-speaking LLMs with POLLUX,2025.0,2025.0,,Functional & Professional Writing,Narrative & Story Writing; Dialogue Generation & Social Interaction; Programming & Algorithmic Creativity; Scientific Discovery; Lateral Thinking & Creative Problem Solving,high,"The POLLUX benchmark is a comprehensive evaluation framework that primarily focuses on the functional utility and stylistic versatility of LLMs across professional, journalistic, and technical contexts, while also incorporating a broad range of creative, conversational, and algorithmic tasks.","**Creative Artifacts Evaluated**
The benchmark evaluates a diverse array of text-based outputs, including literary and journalistic narratives, scientific and official/business documents, and persona-driven dialogues (formal and informal). It also assesses technical artifacts such as original and modified source code, STEM exercise solutions, editorial concepts, structured plans, and practical recommendations.

**Creative Capabilities Assessed**
POLLUX measures stylistic versatility through style transfer and persona consistency, alongside ideational fluency and originality in creative brainstorming. It evaluates the balance between divergent thinking (generating unique ideas) and convergent logic (summarization, data retrieval, and fact-checking). Technical correctness is assessed in coding and STEM tasks, while linguistic precision and conceptual depth are measured through concept explanation and subjective text interpretation.

**Evaluation Context**
This is a text-to-text benchmark specifically designed for the Russian language. It prioritizes high-granularity expert human evaluation, using a 0/1/2 scoring taxonomy across specific criteria to capture nuances in generative quality. The framework also includes a meta-evaluation component, assessing the ability of LLMs to act as judges by measuring their correlation with human expert scores.

**Task Characteristics**
Tasks range from open-ended generation (literary writing, brainstorming) to highly constrained transformations (editing, rephrasing, and translation). The benchmark incorporates problem-solving (logic puzzles), cross-domain application (scientific text generation), and evaluative tasks where the model must objectively analyze or subjectively interpret existing texts, distinguishing it from standard accuracy-based benchmarks.",d607efe5c974cf8ce06cfee60103417f0de4c25b:POLLUX,POLLUX,https://huggingface.co/datasets/ai-forever/POLLUX,2.0,GPU/Local,36.0,False,True,True,huggingface,none,text
eb16b5ac4035510a3c523114c3fd09ff4113f120,Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,Sudoku-Bench: Evaluating creative reasoning with Sudoku variants,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Mathematical Reasoning,high,"The paper focuses on evaluating 'break-ins' and novel problem-solving in unconventional logic puzzles, which directly aligns with the lateral thinking domain's emphasis on brain teasers and non-obvious solutions.","**Creative Artifacts Evaluated:**
The artifacts consist of completed 9x9 digit matrices representing solutions to unconventional Sudoku variants. These variants introduce non-standard constraints (e.g., Thermo, Killer, or Arrow Sudoku) that deviate from traditional row, column, and box rules.

**Creative Capabilities Assessed:**
The benchmark measures creative logical reasoning and convergent thinking, specifically the ability to identify ""break-ins""—novel or non-obvious entry points into a problem. It assesses technical correctness, logical consistency, and the capacity for novel problem-solving in unfamiliar systems, ensuring the model is not merely relying on memorized patterns from standard puzzle datasets.

**Evaluation Context:**
This is a text-based evaluation of Large Language Models (LLMs) using fully automatic scoring. Performance is quantified via solve rates and the average number of correct digit placements. The evaluation spans both single-shot attempts and multi-round interactions, testing the model's ability to maintain logical integrity throughout a multi-step reasoning process.

**Task Characteristics:**
Tasks involve highly constrained generation and multi-step problem-solving. While the final solution is unique, the solution path is open-ended, requiring the model to synthesize novel rules and apply them to a blank grid. This necessitates a transition from abstract constraint understanding to precise, iterative execution.",eb16b5ac4035510a3c523114c3fd09ff4113f120:Sudoku-Bench,Sudoku-Bench,https://huggingface.co/datasets/SakanaAI/Sudoku-Bench,1.0,API-Only,2.0,True,True,True,huggingface,openai; anthropic; google; together,text
e446951ee5f4f558b27b3852b8b1fdeb0b7d45c8,GraphicBench: A Planning Benchmark for Graphic Design with Language Agents,GraphicBench: A Planning Benchmark for Graphic Design with Language Agents,2025.0,2025.0,,Graphic Design & Visual Layout,Programming & Algorithmic Creativity,high,The paper explicitly benchmarks graphic design elements like typography and layout while utilizing executable scripts and workflow planning as the primary mechanism for creative generation.,"**Creative Artifacts Evaluated:**
The benchmark evaluates complex graphic designs, specifically multi-layered compositions incorporating typography, color palettes, and image assets. It also assesses hierarchical workflow plans—structured textual instructions designed to be executed via Adobe Creative Cloud scripts.

**Creative Capabilities Assessed:**
Assessment focuses on multi-step planning and tool-use efficiency (expert utilization). Creative dimensions include originality and elaboration (via GPT-o1), alongside technical design quality covering color harmony, typographic relevance, and spatial layout. It measures the agent's ability to translate open-ended, high-level user intent into precise, executable design sequences.

**Evaluation Context:**
This is a multimodal framework (GRAPHICTOWN) where LLM agents act as planners for professional design software. Evaluation is a hybrid of automated execution metrics (Delivery Rate, Step Efficiency), content-based similarity (CLIPSCORE), and LLM-as-a-judge (GPT-4, GPT-o1, and LLaVA-1.5) to verify design pass rates and aesthetic coherence.

**Task Characteristics:**
Tasks involve open-ended, constrained generation requiring both divergent ideation (workflow planning) and convergent execution (script-based rendering). The benchmark emphasizes tool-augmented creativity, where agents must navigate the constraints of professional software environments to solve complex, multi-component design problems from natural language prompts.",e446951ee5f4f558b27b3852b8b1fdeb0b7d45c8:GraphicBench,GraphicBench,https://github.com/adobe-research/GraphicBench,3.0,Special,2.0,False,False,True,github,openai,multimodal
e7c698bdace380f7183dedbe657686f1885f615c,PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,PlotMachines: Outline-Conditioned Generation with Dynamic Plot State Tracking,2020.0,2020.0,,Narrative & Story Writing,Functional & Professional Writing,high,"The paper focuses on generating long-form fictional stories and news articles from outlines, emphasizing narrative coherence and logical plot progression.","**Creative Artifacts Evaluated:**
The benchmarks evaluate multi-paragraph fictional story plots (spanning television, film, and literature) and non-fiction news articles. These artifacts are characterized by their long-form structure and reliance on specific thematic anchors.

**Creative Capabilities Assessed:**
Assessment focuses on narrative coherence, logical sequencing, and global consistency. Key capabilities include ""outline usage"" (the ability to integrate specific constraints), narrative flow, and the reduction of semantic repetition. The benchmarks specifically measure the model's ability to maintain a ""plot state,"" ensuring that generated content progresses logically rather than drifting aimlessly.

**Evaluation Context:**
The research utilizes a text-based LLM framework (GPT-2 with memory extensions). Evaluation is a hybrid of automatic lexical metrics (ROUGE), diversity measures (Self-BLEU), and human qualitative ratings for transition quality and relevance. A distinctive discriminative feature is the use of paragraph re-ordering accuracy to quantitatively measure discourse-level structure.

**Task Characteristics:**
This work centers on constrained, long-form generation. Unlike purely open-ended generation, these tasks require transforming unordered key phrases into a structured narrative. It introduces a ""state-tracking"" requirement, where the model must manage which parts of a creative plan have been fulfilled, bridging the gap between high-level ideation and low-level textual realization.",e7c698bdace380f7183dedbe657686f1885f615c:Outline-Conditioned Story Generation,Outline-Conditioned Story Generation,https://github.com/hrashkin/plotmachines,2.0,GPU/Local,3.0,False,False,True,github,,text
e2f05b7885ec5295e80a51d7264adc08681d9f14,Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation,Thinking Outside the (Gray) Box: A Context-Based Score for Assessing Value and Originality in Neural Text Generation,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Mathematical Reasoning; Poetry & Verse,high,"The paper's central focus is on measuring 'outside the box' originality and divergent thinking through a novel T-LCS score, while specifically using mathematical word problems and structured poetry (sonnets/haikus) as its primary evaluation datasets.","This research evaluates neural text generation across three distinct domains: creative writing, mathematical reasoning, and general-purpose ideation. 

**Creative Artifacts Evaluated:**
Artifacts include stylized poems (specifically sonnets and haikus), multi-step mathematical solution procedures for grade-school and competition-level word problems, and sets of ten distinct text responses to curated open-ended prompts.

**Creative Capabilities Assessed:**
The framework assesses the dual pillars of creativity: value and originality. This includes technical correctness (mathematical accuracy, adherence to poetic meter and syllable constraints), stylistic tone adherence, and divergent thinking. Originality is specifically measured through semantic diversity (equivalence classes) and a novel ""context-based"" score (T-LCS) that checks for verbatim overlap against a massive external corpus.

**Evaluation Context:**
The context is strictly text-based LLM evaluation. It employs a hybrid scoring approach combining traditional automatic metrics (SBERT, EAD), LLM-based reward models for quality (Skywork-Reward), and a new reference-based originality metric using the GutenVerse corpus to distinguish between ""gray box"" memorization and true novelty.

**Task Characteristics:**
Tasks range from highly constrained generation (adhering to rigid poetic forms) to divergent problem-solving, where the model must generate multiple unique paths to a single correct mathematical answer, and open-ended ideation requiring the production of high-utility, non-redundant outputs.",e2f05b7885ec5295e80a51d7264adc08681d9f14:Poetry Generation Evaluation,Poetry Generation Evaluation,https://anonymous.4open.science/r/CoVO-grpo/,2.0,GPU/Local,1.0,False,False,True,github,none,text
ddcfc7b0fafbe8b47699f12c59772ccff11a4d32,AidanBench: Evaluating Novel Idea Generation on Open-Ended Questions,AidanBench: Evaluating Novel Idea Generation on Open-Ended Questions,,,,Scientific Discovery,Programming & Algorithmic Creativity; Engineering & Technical Design; Dialogue Generation & Social Interaction,high,"The paper explicitly focuses on evaluating novel idea generation and functional synthesis in expert-level fields such as AI research, biological protocols, and complex software engineering.","**Creative Artifacts Evaluated:**
Competitive programming code, real-world software pull requests, full-scale AI research paper implementations, biological lab protocols, cybersecurity exploits, and nuanced health-related conversational responses.

**Creative Capabilities Assessed:**
Technical correctness, complex multi-step planning, and synthesis are prioritized. The benchmark measures the ability to generate novel, functional solutions to expert-level problems, focusing on tacit knowledge application, troubleshooting, and the practical utility of generated artifacts in high-stakes scientific and engineering domains. It evaluates the transition from divergent ideation to convergent, functional implementation.

**Evaluation Context:**
The evaluation is primarily text-based with specific multimodal tasks in virology troubleshooting. It employs a hybrid scoring approach: automatic metrics (pass@k, Elo rating), LLM-based grading for safety and hallucinations, and rigorous human assessment by PhD-level experts for scientific protocols and AI research replication.

**Task Characteristics:**
The suite emphasizes open-ended, high-complexity problem-solving and end-to-end execution. Tasks range from identifying subtle errors in non-public protocols to replicating state-of-the-art ICML papers from scratch. This distinguishes the work by focusing on ""creative"" engineering and scientific synthesis—where ""novelty"" is defined by the ability to bridge high-level goals with functional, expert-level outputs—rather than traditional artistic or linguistic divergent thinking.",ddcfc7b0fafbe8b47699f12c59772ccff11a4d32:Codeforces,Codeforces,https://github.com/aidanmclaughlin/AidanBench,,Unknown,1.0,False,False,False,unknown,,unknown
e8795958aad9c8514d5d7d28b022d42ca4a3a243,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT,2022.0,2022.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric; Scientific Discovery; Functional & Professional Writing,high,"The paper focuses on generating scientific analogies, a task that explicitly requires analogical mapping and identifying structural isomorphisms (Lateral Thinking) using cross-domain conceptual mapping (Figurative Language) to explain STEM concepts (Scientific Discovery) for pedagogical purposes (Functional Writing).","**Creative Artifacts Evaluated:**
The artifacts consist of **scientific analogies** comprising two distinct components: a **source concept** (a familiar or concrete system used for comparison) and a **textual explanation** that details the structural or functional mapping between that source and a target scientific concept.

**Creative Capabilities Assessed:**
The research evaluates **cross-domain mapping** and **relational reasoning**. Specifically, it measures **conceptual depth** (the ability to identify underlying structural similarities rather than surface features), **originality** (generating non-obvious source concepts), and **technical validity**, ensuring the generated analogies are pedagogically meaningful and logically sound.

**Evaluation Context:**
This is a **text-based** evaluation of **Large Language Models** (specifically InstructGPT) using both established and newly curated datasets (STD and SAQA). The study employs a **mixed-methods scoring** approach, combining **automatic linguistic metrics** (BLEURT, METEOR, ROUGE-L) with **human-centric qualitative assessment** to verify the ""meaningfulness"" of the creative output.

**Task Characteristics:**
Tasks include **open-ended ideation** (generating a source concept from a target concept) and **constrained generation** (explaining the relationship between a provided target and source). The work focuses on **cross-domain transfer**, requiring the model to bridge abstract scientific principles with disparate domains to facilitate conceptual understanding.",e8795958aad9c8514d5d7d28b022d42ca4a3a243:Standard Science Analogies (STD),Standard Science Analogies (STD),https://github.com/Bhaavya/InstructGPT-Analogies,1.0,API-Only,1.0,True,False,True,github,openai,text
ea6b0b5904d6e8eccbccb609ac35911ae967cd2c,Creating Suspenseful Stories: Iterative Planning with Large Language Models,Creating Suspenseful Stories: Iterative Planning with Large Language Models,2024.0,2024.0,,Narrative & Story Writing,,high,"The paper specifically focuses on the generation and evaluation of narrative short stories, character-driven arcs, and the structural elements required to create suspense and empathy in fiction.","**Creative Artifacts Evaluated:**
The research evaluates narrative short stories and structured story outlines. These artifacts are characterized by protagonist-driven arcs that detail specific sequences of actions, associated failure reasons, and escalating stakes designed to build narrative tension.

**Creative Capabilities Assessed:**
The primary focus is on affective storytelling, specifically the ability to evoke suspense and reader empathy. Beyond emotional impact, the study assesses narrative logic, novelty, and naturalness. It also evaluates structural planning capabilities, such as the plausibility of causal failures and the strategic modulation of action success likelihood to create a perceived trend of increasing difficulty.

**Evaluation Context:**
This is a text-based study utilizing Large Language Models (LLMs) for iterative generation. The evaluation is heavily human-centric, employing pairwise preference testing (Win/Lose/Tie) and 5-point Likert scales. It distinguishes itself through controlled intervention studies that isolate specific narrative variables to test their individual contributions to suspense.

**Task Characteristics:**
Tasks involve open-ended narrative generation, iterative planning, and constrained refinement. The methodology includes comparative judgment, causal intervention to test suspense factors, and correlation analysis between distinct creative dimensions—specifically the relationship between character empathy and reader-perceived suspense—emphasizing the interplay between character development and structural tension.",ea6b0b5904d6e8eccbccb609ac35911ae967cd2c:Human Evaluation of Suspenseful Story Generation,Human Evaluation of Suspenseful Story Generation,,3.0,Special,4.0,False,False,True,unknown,openai,text
ed7172b60ad1dc21d27243d99692198c261c3047,Probing the Creativity of Large Language Models: Can models produce divergent semantic association?,Probing the Creativity of Large Language Models: Can models produce divergent semantic association?,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,,high,"The paper evaluates divergent thinking and semantic divergence through tasks that require models to bridge disparate conceptual spaces, which directly aligns with the 'outside-the-box' reasoning and analogical mapping described in the Lateral Thinking domain.","**Creative Artifacts Evaluated:**
The primary artifacts are sets of ten discrete nouns. Unlike traditional creative outputs such as narratives or poems, these artifacts are isolated linguistic units selected for their conceptual isolation and lack of semantic overlap rather than their aesthetic or functional cohesion.

**Creative Capabilities Assessed:**
The study measures divergent thinking, specifically semantic divergence (the ability to bridge disparate conceptual spaces). It also assesses lexical originality through word frequency (surprisal) and the model’s capacity for intentionality—the ability to distinguish between ""random"" generation and ""unrelated"" generation.

**Evaluation Context:**
This is a text-only, zero-shot evaluation of various LLMs compared against human performance benchmarks. The evaluation is strictly automatic and objective, utilizing GLoVe word embeddings to calculate pairwise cosine distances. This methodology removes human subjectivity, focusing on the mathematical distribution of concepts within a high-dimensional vector space.

**Task Characteristics:**
The tasks involve constrained ideation and conceptual search. They range from unconstrained generation (base) to highly constrained generation (unrelated nouns), testing the model's ability to navigate the semantic landscape to find ""far"" associations. The framework specifically probes the difference between stochastic randomness and deliberate divergent association, a key differentiator in modeling creative agency.",ed7172b60ad1dc21d27243d99692198c261c3047:Divergent Association Task (DAT),Divergent Association Task (DAT),https://github.com/DingNLab/probing_creativity,2.0,GPU/Local,3.0,False,False,True,github,openai,text
e7d8075e2c2b20faaeef7c8408abd9bddef80d18,IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers,IconShop: Text-Guided Vector Icon Synthesis with Autoregressive Transformers,2023.0,2023.0,,Graphic Design & Visual Layout,Visual Arts & Stylized Imagery; Lateral Thinking & Creative Problem Solving,high,"The paper focuses on the synthesis of vector icons, which are core components of graphic design and UI layouts, while also addressing stylized visual representation and conceptual blending.","**Creative Artifacts Evaluated:**
The primary artifacts are scalable vector icons (SVG) composed of sequential geometric paths and commands. These include both abstract symbols and representational depictions of objects, animals, and concepts, generated from either keywords or complex natural language descriptions.

**Creative Capabilities Assessed:**
The model is assessed on its ability to produce high-quality, novel, and diverse designs that maintain structural integrity. Key capabilities include semantic alignment (mapping text to visual symbols), stylistic consistency, and divergent thinking through semantic combination (blending distinct concepts). It also demonstrates technical proficiency in path completion and latent space interpolation.

**Evaluation Context:**
This research utilizes a multimodal framework (text-to-vector) using autoregressive transformers trained on the FIGR-8-SVG dataset. Evaluation is dual-pronged: automated metrics (FID via CLIP, CLIP Score, Uniqueness, and Novelty) measure statistical distribution and semantic fidelity, while human subjective studies validate perceptual quality and design relevance.

**Task Characteristics:**
Tasks span the entire design workflow, including open-ended synthesis, text-constrained generation, and iterative refinement. The work distinguishes itself by addressing both the ""cold start"" problem (generation from scratch) and interactive assistance, such as path auto-suggestion and bidirectional icon editing, highlighting its utility in professional design environments.",e7d8075e2c2b20faaeef7c8408abd9bddef80d18:FIGR-8-SVG,FIGR-8-SVG,https://github.com/kingnobro/IconShop,2.0,GPU/Local,7.0,False,False,True,url,none,multimodal
e7eb216744470319ddc6b1ff630b81f07a564fca,Trade-offs in Image Generation: How Do Different Dimensions Interact?,Trade-offs in Image Generation: How Do Different Dimensions Interact?,2025.0,2025.0,,Visual Arts & Stylized Imagery,Lateral Thinking & Creative Problem Solving,high,"The paper evaluates the generation of high-resolution digital images and artistic renderings, focusing on aesthetic dimensions and the model's ability to navigate complex trade-offs and 'creative compromises' when faced with conflicting prompt instructions.","**Creative Artifacts Evaluated:**
The benchmark evaluates high-resolution digital images generated from text, edited versions of source images, and subject-consistent generations where a specific entity is transposed into new contexts. These artifacts encompass photorealistic scenes, stylized artistic renderings, and intentionally ambiguous visual compositions designed to test the limits of model interpretation.

**Creative Capabilities Assessed:**
Assessment focuses on ten distinct dimensions, prioritizing creative facets such as **Originality** (novelty and uniqueness), **Aesthetics** (visual appeal), and **Ambiguity** (the capacity for multiple interpretations). Crucially, it measures the model's ability to navigate multi-dimensional trade-offs, such as balancing **Realism** against **Originality** or **Content** fidelity against **Style** adherence and **Knowledge** (factual accuracy).

**Evaluation Context:**
This is a large-scale multimodal evaluation of 14 diffusion models. It utilizes a VLM-as-judge framework (TRIGScore) to provide nuanced ratings across all ten dimensions, which is validated against human evaluations on a continuous scale. The context shifts from simple quality assessment to analyzing the friction between competing creative and technical objectives.

**Task Characteristics:**
Tasks include open-ended text-to-image generation, constrained image editing, and subject-driven synthesis. The methodology is distinguished by the use of ""trade-off prompts"" specifically engineered to force models to prioritize conflicting instructions, evaluating complex decision-making and ""creative compromise"" within generative workflows.",e7eb216744470319ddc6b1ff630b81f07a564fca:TRIG-Bench,TRIG-Bench,https://github.com/fesvhtr/TRIG,2.0,GPU/Local,4.0,False,True,True,huggingface,openai,multimodal
ee7f769a73a533a871cd5fd9cd566d96a3f09107,Zero-shot Image Editing with Reference Imitation,Zero-shot Image Editing with Reference Imitation,2024.0,2024.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on zero-shot image editing, texture re-mapping, and the synthesis of photorealistic and stylized composite images, which directly aligns with the Visual Arts & Stylized Imagery domain.","**Creative Artifacts Evaluated:**
The benchmark evaluates photorealistic and stylized composite images. Specifically, it focuses on localized part integration (e.g., grafting a specific animal’s wing or a unique architectural element) and surface texture re-mapping (e.g., applying a specific fabric pattern or material finish from a reference onto a target object).

**Creative Capabilities Assessed:**
The tasks measure visual fidelity to reference motifs, stylistic and structural harmony within composite scenes, and technical image quality. It assesses ""imitative"" creativity—the ability to precisely replicate specific visual identities and textures from an external source while maintaining the contextual and lighting coherence of the original source image.

**Evaluation Context:**
This is a zero-shot image-to-image generation context using diffusion models. Evaluation employs a dual-track approach: an ""inner-ID"" track using pixel-level reconstruction metrics (SSIM, PSNR, LPIPS) and an ""inter-ID"" track using semantic similarity metrics (DINO, CLIP). These are supplemented by human preference rankings focusing on fidelity, harmony, and overall aesthetic quality.

**Task Characteristics:**
The tasks involve constrained generation and cross-image transfer. Unlike standard text-to-image synthesis, these tasks require high-precision spatial alignment and texture synthesis within a masked region. The process is characterized by ""reference imitation,"" where the model must synthesize new content by imitating external visual cues rather than relying solely on linguistic descriptions.",ee7f769a73a533a871cd5fd9cd566d96a3f09107:Imitative Editing Benchmark,Imitative Editing Benchmark,https://github.com/ali-vilab/MimicBrush,2.0,GPU/Local,2.0,False,False,True,unknown,none,image
f1cae5ab9481ccb0ccbba81c96a8d1e7fac80761,Stylebreeder: Exploring and Democratizing Artistic Styles through Text-to-Image Models,Stylebreeder: Exploring and Democratizing Artistic Styles through Text-to-Image Models,2024.0,2024.0,,Visual Arts & Stylized Imagery,Scientific Discovery,high,"The paper focuses on the generation, categorization, and fidelity of artistic styles using diffusion models, while also emphasizing the 'unsupervised discovery' and modeling of aesthetic patterns as a technical research contribution.","**Creative Artifacts Evaluated:**
The benchmark evaluates AI-generated images spanning a vast spectrum of diverse artistic styles. Rather than focusing on generic photorealism, the artifacts consist of stylized visual content categorized into aesthetic clusters, including specific artistic movements, techniques, and user-defined visual identities.

**Creative Capabilities Assessed:**
The research assesses three primary capabilities: *stylistic discovery* (the ability to identify and categorize unique aesthetic patterns within a large-scale dataset), *stylistic fidelity and personalization* (the capacity of diffusion models to maintain visual consistency and specific artistic nuances across different prompts), and *aesthetic preference modeling* (the ability to predict and recommend styles based on user interaction history).

**Evaluation Context:**
This is a multimodal (text-to-image) evaluation context utilizing state-of-the-art diffusion models (e.g., Stable Diffusion) and fine-tuning techniques like LoRA and DreamBooth. Evaluation is entirely automatic, employing CSD embeddings for clustering quality (Silhouette scores), CLIP and DINO for image-text and image-image alignment, and MAE/RMSE for recommendation accuracy.

**Task Characteristics:**
The tasks represent a hybrid of *unsupervised discovery* (clustering styles), *constrained generation* (personalizing models to specific aesthetics), and *evaluative judgment* (style-based recommendation). This framework shifts the focus from simple prompt-following to the broader ecosystem of style curation, refinement, and democratization within creative AI workflows.",f1cae5ab9481ccb0ccbba81c96a8d1e7fac80761:STYLEBREEDER,STYLEBREEDER,https://stylebreeder.github.io,2.0,GPU/Local,3.0,False,True,True,huggingface,none,multimodal
f1845b7e6255270b5b1505fffb85b3d004409cde,ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed Visual-Ability Families,ArtInsight: Enabling AI-Powered Artwork Engagement for Mixed Visual-Ability Families,2025.0,2025.0,,Dialogue Generation & Social Interaction,Functional & Professional Writing; Visual Arts & Stylized Imagery; Narrative & Story Writing,high,"The paper's core focus is on 'social creativity' and generating discussion prompts to facilitate interpersonal engagement and shared family experiences, while also involving evocative narrative descriptions of visual artwork for accessibility purposes.","**Creative Artifacts Evaluated:**
Interpretive and vivid textual descriptions of children’s artwork, alongside sets of social-facilitation discussion questions designed to bridge communication gaps within families.

**Creative Capabilities Assessed:**
The framework measures interpretive depth, descriptive completeness, and vividness. It specifically evaluates the ability to avoid reductive or presumptive language (e.g., oversimplifying a child's intent). It also assesses ""social creativity""—the capacity to generate useful, engaging prompts that facilitate interpersonal discussion and emotional connection.

**Evaluation Context:**
This multimodal (image-to-text) research operates at the intersection of accessibility and creative engagement. Evaluation utilizes a hybrid approach: state-of-the-art LLMs (GPT-4o, Claude 3.5 Sonnet) are assessed via a custom ""LLM Scorer"" using a formal four-category rubric, supplemented by human Likert-scale ratings and preference ranking from users in mixed visual-ability households.

**Task Characteristics:**
Tasks focus on open-ended interpretive generation and social ideation. Unlike standard image captioning, these tasks require ""creative description""—transforming visual input into evocative, non-visual narratives—and the refinement of content to support cross-modal understanding. The work emphasizes social-relational outcomes, moving beyond technical accuracy to prioritize the quality of shared family experiences.",f1845b7e6255270b5b1505fffb85b3d004409cde:ArtInsight Artwork Description Evaluation,ArtInsight Artwork Description Evaluation,https://github.com/makeabilitylab/ArtInsight,2.0,GPU/Local,4.0,False,False,True,github,openai; anthropic; google,multimodal
f1104653cea1d99cfdd6d9bf0f6105f2391af88f,DiCTI: Diffusion-based Clothing Designer via Text-guided Input,DiCTI: Diffusion-based Clothing Designer via Text-guided Input,2024.0,2024.0,,Visual Arts & Stylized Imagery,Engineering & Technical Design,high,"The paper focuses on the generation of photorealistic images and aesthetic fashion designs using text-to-image diffusion models, while the specific application to clothing synthesis relates to functional product design.","**Creative Artifacts Evaluated:**
The primary artifacts are photorealistic images of human models wearing synthesized clothing. These include specific garment types (e.g., shirts, dresses), diverse fabric textures, complex patterns, and varied garment silhouettes generated based on descriptive text prompts.

**Creative Capabilities Assessed:**
The research evaluates the model’s ability to translate abstract design concepts into visual reality, focusing on prompt adherence (semantic alignment) and aesthetic realism. It assesses technical correctness through identity and pose preservation, as well as the capacity for divergent design generation—specifically the ability to produce diverse styles and textures while maintaining structural integrity in ""in-the-wild"" settings.

**Evaluation Context:**
This is a multimodal (text-to-image) diffusion-based framework. Evaluation occurs across both standardized, high-resolution datasets (VITON-HD) and unconstrained, cluttered environments (Fashionpedia). Success is quantified using a hybrid approach: automated metrics for image quality and semantic similarity (KID, CLIP-S, CLIP-IQA) and human user studies to judge subjective qualities like realism and design preference.

**Task Characteristics:**
The tasks involve constrained generation, where text-guided inputs modify specific masked regions of an image. This includes both open-ended fashion synthesis and iterative refinement (e.g., mask dilation ablation), testing the model’s robustness in translating creative intent into localized, high-fidelity visual modifications.",f1104653cea1d99cfdd6d9bf0f6105f2391af88f:Text-guided Clothing Generation on VITON-HD,Text-guided Clothing Generation on VITON-HD,https://github.com/ajdal/DiCTI,2.0,GPU/Local,2.0,False,True,True,huggingface,none,multimodal
f00999d63d4224c2d996b2e5a2dd58c56a49738b,Thinking with Generated Images,Thinking with Generated Images,2025.0,2025.0,,Visual Arts & Stylized Imagery,Graphic Design & Visual Layout; Lateral Thinking & Creative Problem Solving,high,"The paper evaluates text-to-image generation with a specific focus on the spatial composition, relational logic, and semantic alignment of visual elements within synthetic images.","**Creative Artifacts Evaluated:**
The research evaluates synthetic images generated from complex text prompts. These artifacts are characterized by multi-object compositions, specific numerical counts of entities, and precise spatial arrangements (e.g., ""a cat to the left of a blue vase"").

**Creative Capabilities Assessed:**
Assessment focuses on compositional fidelity and semantic alignment rather than subjective aesthetics. Key capabilities include attribute binding (correctly associating colors or textures with specific objects), spatial reasoning (accurate positioning), numerical precision (counting), and relational understanding (interpreting interactions between multiple entities). It measures the model's ability to translate multi-layered linguistic logic into coherent visual scenes.

**Evaluation Context:**
The evaluation is situated in a multimodal (text-to-image) framework using Vision-Language Models (VLMs) for automated, objective scoring. By utilizing benchmarks like GenEval and DPGBench, the research moves away from holistic human preference toward granular, category-based metrics (e.g., Global, Entity, and Relation scores) to quantify how well the visual output mirrors the prompt's logical structure.

**Task Characteristics:**
The tasks involve constrained open-ended generation. Unlike simple prompt-following, these tasks require the model to resolve complex, often competing, semantic constraints. This necessitates a form of visual reasoning where the model must coordinate multiple entities and attributes within a single generative pass, testing the intersection of creative synthesis and deductive accuracy.",f00999d63d4224c2d996b2e5a2dd58c56a49738b:GenEval,GenEval,https://github.com/djghosh13/geneval,2.0,GPU/Local,1.0,False,False,True,github,none,image
f2ad8f7a6726903f607401e92bf473be8f3f174e,InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts,InfoChartQA: A Benchmark for Multimodal Question Answering on Infographic Charts,2025.0,2025.0,,Graphic Design & Visual Layout,Figurative Language & Rhetoric; Visual Arts & Stylized Imagery,high,"The paper evaluates infographics, which are explicitly listed under Graphic Design & Visual Layout, while focusing on the interpretation of visual metaphors and rhetorical intent.","**Creative Artifacts Evaluated**
The benchmark evaluates stylized infographic charts that integrate quantitative data with creative visual elements, including pictograms, symbolic icons, and metaphorical imagery. These artifacts differ from standard statistical graphics by using artistic representations to convey narrative intent and thematic context alongside raw information.

**Creative Capabilities Assessed**
The primary focus is on the interpretation of conceptual depth and communicative intent. The benchmark assesses a model’s ability to decode visual metaphors—understanding how a specific creative element (e.g., a wilting flower representing economic decline) maps to data. It measures the capacity to bridge the gap between literal visual features and their symbolic meanings, evaluating the model's ""understanding"" of creative rhetorical devices in visual communication.

**Evaluation Context**
This is a multimodal evaluation of Vision-Language Models (VLMs). It utilizes a comparative framework between creative infographics and their ""plain"" counterparts. Scoring is automated, employing Average Normalized Levenshtein Similarity (ANLS) for textual responses, relaxed accuracy for numerical extraction, and exact match for multiple-choice questions regarding metaphorical reasoning.

**Task Characteristics**
Tasks involve a mix of constrained information retrieval and interpretive judgment. They range from basic data extraction (identifying values within creative layouts) to complex problem-solving that requires identifying specific visual elements and explaining their metaphorical significance within the broader communicative context of the chart.",f2ad8f7a6726903f607401e92bf473be8f3f174e:InfoChartQA,InfoChartQA,https://github.com/CoolDawnAnt/InfoChartQA,1.0,API-Only,3.0,True,True,True,huggingface,openai; anthropic; google,multimodal
f0378278a29a692c00e0db9674088825c5a36ef2,JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment,JAM: A Tiny Flow-based Song Generator with Fine-grained Controllability and Aesthetic Alignment,2025.0,2025.0,,Music & Auditory Arts,,high,"The paper focuses on the generation of full-length audio songs, evaluating musicality, vocal synthesis, and rhythmic alignment, which falls directly under Music & Auditory Arts.","**Creative Artifacts Evaluated:**
Full-length audio songs featuring synthesized vocals and instrumental accompaniment across diverse musical genres, generated from text-based lyrics and style prompts.

**Creative Capabilities Assessed:**
The research measures aesthetic alignment (clarity, presence, and quality), musicality, vocal naturalness, and song structure clarity (SSC). It specifically evaluates technical precision in rhythmic timing through phoneme-level duration control and genre-specific stylistic fidelity, alongside the accuracy of lyric-to-audio alignment.

**Evaluation Context:**
A multimodal audio-text framework utilizing the new JAME dataset. Evaluation is tripartite: objective audio metrics (FAD, MuQ-MuLan), linguistic accuracy (WER, PER), and expert human judgment on a 1-5 Likert scale. The context emphasizes ""tiny"" model efficiency and flow-based generation compared to larger, resource-intensive autoregressive models.

**Task Characteristics:**
The work focuses on constrained generation where models synthesize long-form audio from structured text inputs. Tasks include fine-grained temporal control (duration prediction), phoneme-to-audio mapping, and the alignment of aesthetic preferences with model outputs. It highlights the interplay between linguistic structure and musical timing through ablation studies on token-level modeling.",f0378278a29a692c00e0db9674088825c5a36ef2:JAME,JAME,https://github.com/declare-lab/jamify,2.0,GPU/Local,2.0,False,True,True,huggingface,none,audio
f274f97ca7445b6ea3cdf9c398ea44449c627e02,Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models,Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models,2024.0,2024.0,,Narrative & Story Writing,Poetry & Verse; Performing Arts & Video Synthesis; Lateral Thinking & Creative Problem Solving,high,"The paper introduces the CoGS benchmark which evaluates large language models across 12 creative domains, specifically highlighting short stories, poetry, dance choreography descriptions, and game inventions.","**Creative Artifacts Evaluated:**
The Creativity-Oriented Generation Suite (CoGS) evaluates text-based outputs across 12 diverse creative domains. Specific artifacts include short stories, poetry, dance choreography descriptions, game inventions, and other open-ended creative compositions. These represent complex, non-deterministic outputs where subjective expression is prioritized over factual accuracy.

**Creative Capabilities Assessed:**
Assessment focuses on divergent thinking and stylistic adaptability. Rather than measuring technical fluency, the benchmark evaluates the model’s ability to navigate representational diversity. It measures the consistency of creative ""voice"" across different identity prompts and the subjective appeal (affinity) of the content, assessing how models balance neutral versus demographically specific creative choices.

**Evaluation Context:**
The evaluation utilizes a text-only modality within a dual-metric framework. It employs automatic scoring via cosine similarity of vector embeddings to measure Representative Bias (RBS). This is complemented by a mixed-method approach for Affinity Bias (ABS), utilizing both LLM-as-a-judge and human preference selection to capture the nuances of creative judgment.

**Task Characteristics:**
Tasks consist of open-ended generation and comparative evaluation. CoGS distinguishes itself by moving beyond constrained NLP tasks to ""subtle"" creative prompts where no single ""correct"" answer exists. The framework requires models to perform both primary creation and secondary evaluative judgment across multiple distinct creative disciplines.",f274f97ca7445b6ea3cdf9c398ea44449c627e02:Creativity-Oriented Generation Suite (CoGS),Creativity-Oriented Generation Suite (CoGS),https://github.com/akkeshav/subtleBias,2.0,GPU/Local,2.0,False,False,True,github,openai,text
f32f1e5081ade1b0d82d588734b5c40537718a94,Learning to Reason for Long-Form Story Generation,Learning to Reason for Long-Form Story Generation,2025.0,2025.0,,Narrative & Story Writing,,high,"The paper focuses on the generation and evaluation of long-form narrative content, specifically book chapters, emphasizing narrative continuity, character consistency, and plot progression.","**Creative Artifacts Evaluated:**
The primary artifacts are full-length book chapters within a long-form narrative. These text-based outputs are characterized by their length and the requirement for high-level narrative continuity and stylistic consistency with extensive preceding context.

**Creative Capabilities Assessed:**
The benchmark measures narrative development, character consistency, and plot progression. It specifically evaluates linguistic creativity through lexical diversity (percentage of unique words) and novelty (unseen trigrams), alongside qualitative dimensions including aesthetic language use, divergent plot evolution, and overall creative impact.

**Evaluation Context:**
This is a text-centric LLM evaluation framework utilizing a hybrid scoring approach. It combines expert human judgment (creative writers) using Bradley-Terry modeling for relative preference with standard automated linguistic metrics (ROUGE-L). Notably, it introduces the VR-CLI reward metric, which uses per-token perplexity improvement as a proxy for human-perceived creative quality.

**Task Characteristics:**
The core task is open-ended, long-form generation (Next-Chapter Prediction). It is highly context-constrained, requiring models to synthesize rich narrative history to produce coherent continuations. The work also involves a meta-evaluation task: validating the correlation between automated reasoning metrics and subjective human preferences in creative domains.",f32f1e5081ade1b0d82d588734b5c40537718a94:Next-Chapter Prediction (NCP),Next-Chapter Prediction (NCP),https://github.com/Alex-Gurung/ReasoningNCP,3.0,Special,3.0,False,False,True,github,none,text
eee80c7eff186a241b3b6776027a30d31a441c6d,Evaluating Creative Short Story Generation in Humans and Large Language Models,Evaluating Creative Short Story Generation in Humans and Large Language Models,2024.0,2024.0,,Narrative & Story Writing,Lateral Thinking & Creative Problem Solving,high,The paper evaluates the generation of short-form narrative prose and uses constrained tasks designed to measure divergent thinking and semantic novelty.,"**Creative Artifacts Evaluated:**
Short-form narrative prose, specifically five-sentence creative stories constrained by the integration of three disparate cue words.

**Creative Capabilities Assessed:**
The study measures divergent thinking through originality, surprise, and semantic novelty (calculated via semantic distance between dominant terms and consecutive sentences). It also assesses technical execution through lexical and syntactic complexity—using metrics such as dependency path length, constituency tree depth, and POS ratios—and overall narrative impact via effectiveness and n-gram diversity.

**Evaluation Context:**
A text-based comparative framework pitting human writers (stratified into experts and non-experts) against LLMs. The evaluation is multi-layered, combining automated linguistic and semantic metrics, human subjective ratings, and LLM-as-judge meta-evaluations. This setup specifically tests the alignment between AI-driven ratings and human expert consensus to validate LLMs as creative critics.

**Task Characteristics:**
Constrained creative generation requiring the synthesis of specific semantic anchors into a coherent, brief narrative. The research also features evaluative judgment tasks, where models function as critics to benchmark the ""creativity"" of generated content, distinguishing between human-authored and AI-generated stylistic signatures and identifying ""inverse homogenization"" in LLM outputs.",eee80c7eff186a241b3b6776027a30d31a441c6d:Cue-word-based Creative Story Generation and Evaluation,Cue-word-based Creative Story Generation and Evaluation,https://github.com/mismayil/creative-story-gen,2.0,GPU/Local,3.0,False,False,True,github,openai; anthropic; google; huggingface,text
f7c49c6c322036e569816ea66585bf9d6ebdb07e,Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk,2022.0,2022.0,,Humor & Satire,Dialogue Generation & Social Interaction; Performing Arts & Video Synthesis,high,"The paper's central focus is evaluating the ability of language models to generate humor within the specific cultural and structural constraints of Chinese crosstalk, which is a traditional comedic performance dialogue.","**Creative Artifacts Evaluated:**
The primary artifacts are Chinese Comical Crosstalk (*Xiangsheng*) scripts. These are specialized comedic dialogues characterized by specific linguistic structures, cultural wordplay, and traditional performance rhythms (typically involving a ""lead"" and a ""support"" speaker).

**Creative Capabilities Assessed:**
The benchmark evaluates the model's ability to generate humor and maintain conversational coherence within a niche cultural genre. Key capabilities include linguistic fluency, lexical diversity (to avoid repetitive ""robotic"" dialogue), and the mastery of comedic timing. Human evaluators specifically assess the subjective ""funniness"" and general quality of the creative output, alongside ethical safety.

**Evaluation Context:**
This is a text-based generation task utilizing Large Language Models (LLMs). Evaluation employs a mixed-methods approach: automatic NLP metrics (BLEU, ROUGE, GLEU, and Distinct-1/2) measure surface-level similarity and diversity, while human judges provide 5-point Likert scales for humor and quality, plus binary flags for coherence and ethical risks.

**Task Characteristics:**
The task is a constrained generation challenge where models must produce a continuation of a script based on a ten-utterance prefix. It requires the model to navigate highly domain-specific constraints, including cultural references and the unique structural demands of Chinese oral performance art, distinguishing it from general-purpose dialogue or standard storytelling.",f7c49c6c322036e569816ea66585bf9d6ebdb07e:Chinese Comical Crosstalk (C3) Generation Benchmark,Chinese Comical Crosstalk (C3) Generation Benchmark,https://github.com/FreedomIntelligence/crosstalk-generation,3.0,Special,1.0,False,False,True,github,openai,text
f3c3c22716629df9f5353956461222ffc8e5e802,Plot Writing From Pre-Trained Language Models,Plot Writing From Pre-Trained Language Models,2022.0,2022.0,,Narrative & Story Writing,,high,"The paper focuses specifically on the generation of narrative stories, plot outlines, and story endings, emphasizing narrative coherence and thematic consistency.","**Creative Artifacts Evaluated:**
The primary artifacts are short-form narrative stories and their corresponding story endings. Uniquely, the research also evaluates intermediate ""content plans,"" which are structured plot metadata consisting of specific locations, character casts, genres, and thematic elements.

**Creative Capabilities Assessed:**
The benchmarks measure narrative coherence, linguistic fluency, and ""interestingness."" Beyond surface-level generation, the study assesses structural planning abilities—specifically the capacity to maintain consistency between a plot outline and the final prose. It also tests commonsense reasoning through story resolution prediction and lexical diversity via self-BLEU and distinct-n metrics.

**Evaluation Context:**
This is a text-based LLM evaluation involving a hybrid scoring model. It combines automatic accuracy (Story Cloze Test), crowd-sourced human pairwise comparisons for subjective ranking, and expert-level qualitative assessment of plot elements using both binary and Likert scales.

**Task Characteristics:**
The tasks involve hierarchical constrained generation (transforming plans into stories), open-ended ideation (generating plot elements from scratch), and discriminative judgment (ranking narrative endings). This distinguishes the work by focusing on the pipeline from abstract conceptualization to narrative realization rather than simple prompt-to-text generation, emphasizing the structural logic required for storytelling.",f3c3c22716629df9f5353956461222ffc8e5e802:Story Ending Ranking Evaluation,Story Ending Ranking Evaluation,https://github.com/YipingNUS/scratchplot-story-generation,2.0,GPU/Local,1.0,False,False,True,github,none,text
f75da65c02872ef7378cae9fa100d5dd33811fe3,Manipulating Embeddings of Stable Diffusion Prompts,Manipulating Embeddings of Stable Diffusion Prompts,2023.0,2023.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on the generation and optimization of synthetic images using Stable Diffusion, specifically evaluating aesthetic quality, visual consistency, and stylistic control.","**Creative Artifacts Evaluated:**
The research evaluates high-resolution synthetic images generated from text prompts using Stable Diffusion. These artifacts specifically include aesthetically optimized visual compositions and feature-consistent images that maintain specific visual characteristics across different random noise seeds.

**Creative Capabilities Assessed:**
The study assesses aesthetic quality (via LAION aesthetic scores) and technical image clarity (sharpness and blurriness). It measures the capacity for fine-grained control and iterative refinement, specifically focusing on the system's ability to align with subjective user intent. Furthermore, it evaluates the capability for feature reproduction and visual consistency, moving beyond stochastic generation to purposeful, directed creativity.

**Evaluation Context:**
This is a multimodal (text-to-image) evaluation centered on latent diffusion models. The assessment framework is hybrid, utilizing automatic metrics for technical and aesthetic benchmarks alongside human-in-the-loop user studies. These studies employ preference ranking and qualitative feedback to measure the efficiency and ""tedium"" of the creative process compared to traditional prompt engineering.

**Task Characteristics:**
The tasks involve constrained generation and iterative refinement. Key characteristics include metric-based optimization (using gradient ascent to improve specific visual attributes), human-directed steering (selecting outputs to navigate the embedding space), and seed-invariant manipulation (recoupling specific visual features with new noise seeds to ensure stylistic or structural persistence).",f75da65c02872ef7378cae9fa100d5dd33811fe3:Prompt Embedding Manipulation Evaluation,Prompt Embedding Manipulation Evaluation,https://doi.org/10.5281/zenodo.8274625,3.0,Special,3.0,False,False,True,url,none,image
f72258bcfd4872ba5425b799297f6be21a4e7a40,"S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment","S-DAT: A Multilingual, GenAI-Driven Framework for Automated Divergent Thinking Assessment",2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Scientific Discovery,high,"The paper describes a framework for assessing divergent thinking and the ability to bypass functional fixedness (e.g., Alternative Uses Task), which are the defining characteristics of lateral thinking and creative problem solving.","**Creative Artifacts Evaluated:**
The framework evaluates sets of semantically unrelated individual words, lists of novel functional uses for common household objects (e.g., a brick or paperclip), and single-word associative bridges that provide a conceptual link between two disparate terms.

**Creative Capabilities Assessed:**
The primary focus is on divergent thinking, specifically measured through semantic distance (calculated as 1 - cosine similarity of embeddings). The system also assesses fluency (total volume of ideas), originality (statistical rarity of responses), and flexibility (the variety of conceptual categories explored). Furthermore, it evaluates convergent thinking—the ability to find a single ""correct"" associative link—to establish discriminant validity between different cognitive creative processes.

**Evaluation Context:**
This is a text-based, multilingual psychometric assessment environment. It compares automated scoring derived from modern GenAI/LLM word embeddings against traditional human-coded ratings and legacy static embedding models (e.g., GloVe). The framework is designed for high-throughput, scalable evaluation, aiming to replace labor-intensive manual scoring in psychological and educational research.

**Task Characteristics:**
Tasks range from open-ended generation (producing semantically distant words) to constrained ideation (finding unconventional uses for specific objects) and targeted associative problem-solving (bridging semantic gaps). The methodology emphasizes quantifying the ""semantic leap"" in human-generated text to provide a standardized, automated measure of creative potential across different languages.",f72258bcfd4872ba5425b799297f6be21a4e7a40:S-DAT (Synthetic-Divergent Association Task),S-DAT (Synthetic-Divergent Association Task),https://sdat.iol.zib.de/,1.0,API-Only,1.0,True,False,True,url,none,text
fa9cc2ec4eba32b3f4ad89863fb894fed1775530,CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models,CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models,2025.0,2025.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on content-style decomposition for generating stylized images, specifically evaluating the model's ability to replicate artistic textures and aesthetic motifs while maintaining structural fidelity.","**Creative Artifacts Evaluated:**
The research evaluates stylized images generated through content-style decomposition. These artifacts consist of novel visual syntheses that merge the structural and semantic content of a reference image (or text prompt) with the specific artistic textures, color palettes, and aesthetic motifs of a separate style reference.

**Creative Capabilities Assessed:**
Assessment focuses on the model’s capacity for disentangled representation—specifically the ability to isolate and recombine visual elements. Evaluated capabilities include content fidelity (preserving object structure), style consistency (replicating artistic nuances), and prompt adherence. The benchmarks also measure aesthetic quality and the model's ability to perform zero-shot synthesis without task-specific fine-tuning.

**Evaluation Context:**
This multimodal generative research utilizes a hybrid evaluation framework. It introduces the CSD-100 dataset and a 35-image validation set for empirical testing. Scoring combines automatic metrics (CLIP-I/T for semantic alignment, DINO for style, and specialized CSD-C/S scores) with human user studies that rank subjective preferences across image quality and stylistic accuracy.

**Task Characteristics:**
The tasks involve constrained generation and cross-domain transfer. Specifically, the model performs content-style decomposition from a single reference image, enabling complex image-to-image translation and style-driven synthesis. This requires the model to navigate the tension between maintaining structural integrity and applying transformative aesthetic overlays in an open-ended generative space.",fa9cc2ec4eba32b3f4ad89863fb894fed1775530:CSD-100,CSD-100,,2.0,GPU/Local,2.0,False,True,True,huggingface,,image
f988c1e34742883ac4ff3cab25fc5434076a1f18,Probing and Generalization of Metaphorical Knowledge in Pre-Trained Language Models Anonymous ACL-IJCNLP submission,Probing and Generalization of Metaphorical Knowledge in Pre-Trained Language Models Anonymous ACL-IJCNLP submission,2021.0,2021.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper specifically investigates metaphorical knowledge and cross-domain conceptual mapping, which are the core components of figurative language and analogical reasoning.","**Creative Artifacts Evaluated**
The research evaluates natural language sentences containing metaphorical and literal expressions. Specifically, it focuses on verb-based metaphors within news text (Wall Street Journal), diverse linguistic genres (VUA Corpus), and a large-scale collection of conceptual mappings (LCC Metaphor Dataset).

**Creative Capabilities Assessed**
The study assesses the model’s capacity for conceptual depth and figurative understanding. Rather than measuring generation, it evaluates the ability to recognize non-literal semantic shifts and the structural alignment between disparate concepts. A key focus is the model’s grasp of cross-domain mapping—the ability to identify how a source domain (e.g., ""War"") is used to describe a target domain (e.g., ""Argument"").

**Evaluation Context**
This is a probing study of Pre-trained Language Models (PLMs) using text-based benchmarks. Evaluation is conducted through automatic scoring, specifically classification accuracy and Minimum Description Length (MDL). MDL serves as a discriminative feature, measuring the ""compression"" or efficiency with which a model’s internal representations encode metaphorical knowledge compared to literal data.

**Task Characteristics**
The tasks are centered on judgment and classification. They include metaphor detection (binary classification of word usage) and domain prediction (multi-class categorization of conceptual source/target domains). These tasks emphasize the model’s ability to perform cross-domain semantic transfer and recognize the abstract relational patterns that define creative language.",f988c1e34742883ac4ff3cab25fc5434076a1f18:LCC Metaphor Datasets,LCC Metaphor Datasets,https://github.com/EhsanAghazadeh/Metaphors_in_PLMs,2.0,GPU/Local,2.0,False,False,True,github,none,text
f88ab3ef0e3ef4dad581cf5f6dc04c0afcc116b6,YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment,YINYANG-ALIGN: Benchmarking Contradictory Objectives and Proposing Multi-Objective Optimization based DPO for Text-to-Image Alignment,2025.0,2025.0,,Visual Arts & Stylized Imagery,,high,"The paper focuses on benchmarking and optimizing text-to-image diffusion models, specifically evaluating their ability to generate artistic, photorealistic, and emotionally resonant 2D digital images while balancing stylistic freedom with instruction faithfulness.","**Creative Artifacts Evaluated:**
The benchmark evaluates 2D digital images generated by diffusion models. These include photorealistic depictions, abstract artistic compositions, emotionally evocative scenes, and images that either reference established historical styles or represent specific cultural contexts.

**Creative Capabilities Assessed:**
The framework measures the model’s ability to navigate contradictory objectives, specifically balancing **artistic freedom** (stylistic abstraction) against **faithfulness** to instructions. It assesses **originality** (divergence from existing datasets like WikiArt), **emotional impact** (affective resonance), and **visual realism**. Additionally, it evaluates **verifiability** (factual accuracy) and **cultural sensitivity**, measuring the model's ability to maintain creative liberty without violating cultural nuances.

**Evaluation Context:**
This is an automated evaluation framework for text-to-image (T2I) diffusion models. It employs a multi-metric suite including CLIP and DINO ViT embeddings for content/factual similarity, VGG Gram features for style analysis, and DeepEmotion for affective scoring. It also introduces Simulated Cultural Context Matching (SCCM) using LLMs to evaluate cultural alignment.

**Task Characteristics:**
Tasks involve constrained generation where the model must resolve trade-offs between competing axes (e.g., Realism vs. Artistic Freedom). This moves beyond simple prompt-matching toward multi-objective optimization, focusing on the inherent tensions in the creative process rather than single-axis performance.",f88ab3ef0e3ef4dad581cf5f6dc04c0afcc116b6:YinYangAlign,YinYangAlign,https://huggingface.co/datasets/factsoverhallucinations/yinyang-dataset,2.0,GPU/Local,6.0,False,True,True,huggingface,openai; huggingface,image
fc3ddf3292653047b6cf0f33b739e7af299191ad,Building-Block Aware Generative Modeling for 3D Crystals of Metal Organic Frameworks,Building-Block Aware Generative Modeling for 3D Crystals of Metal Organic Frameworks,2025.0,2025.0,,Scientific Discovery,"3D, Spatial & Architectural Design; Engineering & Technical Design",high,"The paper focuses on the generative discovery of novel 3D crystal structures (MOFs) within a STEM context, emphasizing scientific novelty, spatial structural integrity, and physical synthesizability.","**Creative Artifacts Evaluated:**
The primary artifacts are 3D crystal structures of Metal-Organic Frameworks (MOFs). These consist of complex spatial arrangements of inorganic nodes and organic linkers (edges) organized within specific periodic topologies. The evaluation extends from digital molecular geometries to a physically synthesized crystalline material, specifically $[Zn(1,4-TDC)(EtOH)_2]$.

**Creative Capabilities Assessed:**
The framework assesses technical correctness through chemical and geometric validity rates, originality via novelty metrics compared to existing databases, and divergent thinking through uniqueness and distributional similarity. It specifically measures the model’s ability to generate synthesizable structures that maintain structural fidelity, moving beyond mere visual plausibility to functional, physical viability.

**Evaluation Context:**
Evaluation is conducted through a multi-tiered hybrid approach. It utilizes automatic scoring based on revised autocorrelation features and property distributions (e.g., node RMSD), qualitative human expert assessment of structural plausibility, and empirical laboratory validation. The latter involves Powder X-ray Diffraction (PXRD), Thermogravimetric Analysis (TGA), and nitrogen sorption to confirm the successful realization of a generated hypothesis.

**Task Characteristics:**
Tasks include open-ended de novo generation and highly constrained generation, where specific building blocks must be designed to fit fixed inorganic nodes and topologies. The workflow represents a complete pipeline from digital ideation and constrained refinement to experimental problem-solving, bridging the gap between generative AI and physical materials discovery.",fc3ddf3292653047b6cf0f33b739e7af299191ad:MOF Crystal Structure Generation,MOF Crystal Structure Generation,https://arxiv.org/abs/2505.08531,,Unknown,3.0,False,False,False,unknown,,unknown
fef461207f173c03231aa9a8ddf30c01bf4caca0,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models,2024.0,2024.0,,Dialogue Generation & Social Interaction,Narrative & Story Writing; Poetry & Verse,high,"The paper introduces a benchmark specifically structured around a multi-turn conversational hierarchy (Perception to Reasoning to Creation) to evaluate the coherence and progressive ideation of models, while the creative outputs themselves consist of stories and poems.","**Creative Artifacts Evaluated:**
The benchmark evaluates text-based creative content generated in response to visual stimuli, specifically focusing on imaginative stories, poems, and narrative expansions derived from images. These artifacts are produced in the final stage of a three-turn conversational hierarchy.

**Creative Capabilities Assessed:**
ConvBench measures imaginative synthesis, originality, and the ability to generate content with conceptual depth. It specifically assesses the model’s capacity for divergent thinking—moving beyond literal description to creative interpretation. Evaluation is guided by annotated ""Focus Points,"" which ensure the output demonstrates specific creative goals while maintaining consistency with the visual and logical foundations established in previous turns.

**Evaluation Context:**
The context is a multimodal vision-language setting involving Large Vision-Language Models (LVLMs). Evaluation utilizes an LLM-as-a-judge (ChatGPT-3.5) via the ConvEval pipeline, employing pairwise win rates against human references and direct 0-10 grading. A distinguishing feature is the use of ""perfect"" perception and reasoning prompts in ablative settings to isolate pure creative capability from errors in visual recognition or logic.

**Task Characteristics:**
Tasks follow a hierarchical, multi-turn structure (Perception → Reasoning → Creation). This involves constrained, open-ended generation where the creative output must be grounded in prior visual analysis. The process emphasizes progressive ideation and conversational coherence, requiring the model to maintain a stable narrative thread across multiple interactions.",fef461207f173c03231aa9a8ddf30c01bf4caca0:ConvBench,ConvBench,https://github.com/shirlyliu64/ConvBench,2.0,GPU/Local,4.0,False,True,True,huggingface,openai,multimodal
fd463896e627d5690b842b3d7d97d76908edaead,AnaScore: Understanding Semantic Parallelism in Proportional Analogies,AnaScore: Understanding Semantic Parallelism in Proportional Analogies,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper focuses on relational reasoning and semantic parallelism through proportional analogies, which are explicitly categorized under analogical mapping and structural isomorphism within the Lateral Thinking domain.","**Creative Artifacts Evaluated:**
The artifacts consist of proportional sentence-level analogies (A:B :: C:D) curated from datasets such as Semantico, MixGoogle, MulNLIs, and SATS. These are structured linguistic constructions where the fourth sentence (D) must be generated or evaluated based on its relational correspondence to a preceding triplet.

**Creative Capabilities Assessed:**
The benchmark measures relational reasoning and semantic parallelism—the ability to identify an abstract transformation between one pair of concepts and apply it to another. It assesses conceptual depth and convergent thinking by requiring models to maintain structural consistency and mapping accuracy rather than mere linguistic fluency or surface-level word association.

**Evaluation Context:**
This is a text-based evaluation of Large Language Models (LLMs) using a mix of automatic scoring methods. It introduces AnaScore to classify analogy quality into formal or semantic categories. Performance is further quantified through BLEU scores and the accuracy of generating non-overlapping components, focusing on the model's ability to handle complex, multi-sentence relational logic.

**Task Characteristics:**
The work involves both constrained generation (solving the ""missing piece"" of a proportional analogy) and discriminative judgment (classifying analogies based on formal rules). These tasks emphasize cross-domain transfer and problem-solving, requiring the model to navigate abstract semantic spaces to identify and replicate novel relationships between disparate ideas.",fd463896e627d5690b842b3d7d97d76908edaead:Curated Sentence Analogy Datasets,Curated Sentence Analogy Datasets,https://huggingface.co/datasets/liyannn/sentence_analogy,2.0,GPU/Local,2.0,False,True,True,huggingface,openai; anthropic; huggingface,text
fd72ab9fc32effaa17213a1296671a9ac62040d4,PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension,PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension,2024.0,2024.0,,Humor & Satire,Figurative Language & Rhetoric,high,"The paper focuses on benchmarking the comprehension of punchlines, humor, and irony in multimodal contexts, which directly aligns with the Humor & Satire domain's focus on wit and incongruity resolution, as well as the Figurative Language domain's focus on non-literal communication.","**Creative Artifacts Evaluated:**
The benchmark focuses on multimodal image-caption pairs specifically designed to convey humor, irony, or sarcasm. These artifacts are characterized by a synergistic relationship where the creative ""punchline"" emerges only through the interplay between the visual scene and the accompanying text.

**Creative Capabilities Assessed:**
The tasks measure conceptual depth and the ability to resolve semantic incongruity. Specifically, it assesses a model’s capacity for irony detection and nuanced interpretation of creative intent. Beyond simple recognition, it evaluates the model’s reasoning capabilities—the ability to articulate the underlying logic or ""joke"" that makes a specific multimodal combination humorous.

**Evaluation Context:**
This is a multimodal evaluation framework designed for Vision-Language Models (VLMs). The evaluation utilizes a hybrid scoring methodology: objective automatic accuracy is used for discriminative tasks (Yes/No, matching, and multiple-choice), while LLM-based evaluation (GPT-3.5-Turbo) is employed to judge the semantic correctness of generated reasoning sentences against human-authored references.

**Task Characteristics:**
The benchmark spans a spectrum from discriminative judgment to constrained generation. Tasks include perception-based classification (identifying the presence of a punchline) and reasoning-based problem-solving (matching or generating explanations for the humor). This distinguishes the work from standard VQA by focusing on the interpretation of non-literal, creative communication and cross-modal reasoning.",fd72ab9fc32effaa17213a1296671a9ac62040d4:PunchBench,PunchBench,https://github.com/OuyangKun10/PunchBench,2.0,GPU/Local,6.0,False,True,True,huggingface,openai,multimodal
ffd9c7cb773dd8e587a785bdf3aee4d89432c862,Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization,Rhetorical Text-to-Image Generation via Two-layer Diffusion Policy Optimization,2025.0,2025.0,,Figurative Language & Rhetoric,Visual Arts & Stylized Imagery,high,"The paper specifically focuses on the translation of non-literal linguistic devices like metaphors and similes into visual scenes, emphasizing cross-domain conceptual mapping and the decoding of abstract symbolic intent.","**Creative Artifacts Evaluated:**
Digital images generated from figurative linguistic prompts, specifically focusing on visual metaphors and similes that translate abstract comparisons into concrete, synthesized visual scenes.

**Creative Capabilities Assessed:**
The research assesses conceptual depth and the ability to perform cross-modal mapping of non-literal meaning. Key metrics include semantic alignment (capturing the figurative essence) and elemental alignment (representing the literal objects within the trope), measuring the model's capacity for symbolic interpretation over standard literal prompt-following.

**Evaluation Context:**
A multimodal text-to-image generation environment using a two-layer diffusion policy optimization. Evaluation is mixed, employing qualitative visual inspection alongside automated quantitative metrics based on custom CLIP-based cosine similarity to judge the fidelity of rhetorical translation.

**Task Characteristics:**
This is an open-ended generation task requiring high-level linguistic-to-visual reasoning. It differentiates itself from standard text-to-image tasks by demanding the resolution of rhetorical ambiguity. The task necessitates a sophisticated understanding of how abstract concepts (e.g., ""time is a thief"") manifest as coherent visual compositions, representing a significant challenge in cross-domain transfer from figurative language to visual art.",ffd9c7cb773dd8e587a785bdf3aee4d89432c862:FLUTE (filtered),FLUTE (filtered),https://anonymous.4open.science/r/Rhet2Pix-2D52/,2.0,GPU/Local,1.0,False,False,True,github,openai,multimodal
f75eed717c978f385dd0e5fd6d58294f4e1bbc21,How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages,How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages,2025.0,2025.0,,Figurative Language & Rhetoric,Humor & Satire; Lateral Thinking & Creative Problem Solving,high,"The paper investigates the creation of slang through morphological innovation and semantic extensions, which directly aligns with the study of non-literal linguistic devices, idioms, and cross-domain conceptual mapping.","**Creative Artifacts Evaluated**
The primary artifacts are novel linguistic units consisting of slang coinages (entirely new morphological forms), semantic reuses (existing words assigned novel meanings), formal lexicographical definitions, and contextual usage sentences that demonstrate the slang in situ.

**Creative Capabilities Assessed**
The framework evaluates morphological creativity through structural complexity and coherence, as well as semantic novelty by measuring the distance of sense extensions. It assesses divergent thinking via the generation of ""surprising"" yet functional terms and convergent thinking through the model’s ability to map specific slang terms to their intended meanings (informativeness).

**Evaluation Context**
This text-based study compares state-of-the-art LLMs (e.g., GPT-4o, Claude 3.5 Sonnet) against human-curated benchmarks from the Online Slang Dictionary. Evaluation is primarily automatic, utilizing morphological segmentation (Morfessor), embedding-based semantic similarity (SBERT), and LLM-based surprisal scores to quantify the ""creative gap"" between human and machine-generated informal language.

**Task Characteristics**
Tasks range from open-ended generation (creating datasets of novel slang) to constrained generation (producing definitions for specific terms). The framework also includes evaluative judgment tasks, such as multiple-choice cloze tests and interpretation tasks, which require the model to navigate the intersection of linguistic innovation and semantic clarity.",f75eed717c978f385dd0e5fd6d58294f4e1bbc21:Slang Generation Evaluative Framework,Slang Generation Evaluative Framework,https://github.com/siyangwu1/LLM-Slang-Dictionary,2.0,GPU/Local,7.0,False,False,True,github,openai,text
fc09ee18ab94884cb8026a7db645dd0f0fc04d38,ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models,ANALOGICAL - A Novel Benchmark for Long Text Analogy Evaluation in Large Language Models,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper focuses on analogical reasoning and identifying structural isomorphisms across texts, which are explicitly listed as core components of the Lateral Thinking domain, while also incorporating metaphorical and figurative content.","**Creative Artifacts Evaluated**
The benchmark evaluates a spectrum of linguistic artifacts, including word pairs (proportional analogies), crossword clues, dictionary definitions (WordNet), and syntactic sentence variants (masked, deleted, or reordered). Crucially, it extends to complex, long-form creative texts such as proverbs, quotes, and their corresponding explanatory narratives or elaborated meanings.

**Creative Capabilities Assessed**
The primary focus is on analogical reasoning and conceptual mapping. It assesses the model’s ability to identify structural similarities across varying levels of abstraction, from morphological and syntactic consistency to deep metaphorical understanding. It measures conceptual depth and the capacity for cross-domain transfer by requiring the model to link literal narratives with figurative proverbs.

**Evaluation Context**
This is a text-based discriminative benchmark for Large Language Models. Unlike generative tasks, it evaluates the quality of the models' internal representations (embeddings) using automatic distance metrics, including Cosine, Euclidean, and Mahalanobis distances. It spans a complexity gradient from simple word-level relations to high-level creative cognition.

**Task Characteristics**
Tasks involve evaluation and judgment rather than open-ended generation. They range from constrained, SAT-style word problems to complex relational matching of long-form text. The inclusion of a negation dataset acts as a ""negative"" control to ensure the model distinguishes true relational analogy from simple semantic similarity or contradiction.",fc09ee18ab94884cb8026a7db645dd0f0fc04d38:ANALOGICAL,ANALOGICAL,https://github.com/JHU-CLSP/AnaloBench,2.0,GPU/Local,13.0,False,True,True,huggingface,openai; anthropic; google; huggingface,text
73c6533c2988eebb71cdaa758f38546cdf01f655,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,2023.0,2023.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric,high,"The paper focuses on overcoming mental fixation and identifying non-obvious connections in a word-based puzzle task (Only Connect), which directly aligns with lateral thinking, brain teasers, and cross-domain conceptual mapping.","**Creative Artifacts Evaluated:**
The primary artifacts are **semantic clusters** of word-based clues and **conceptual labels** (natural language descriptions) that explain the underlying thematic, linguistic, or lateral-thinking links between disparate items.

**Creative Capabilities Assessed:**
The research evaluates **convergent thinking** and **pattern recognition** through the lens of **cognitive flexibility**. Specifically, it measures a model's ability to overcome the **Einstellung effect** (mental fixation) by ignoring ""red herrings""—deliberate distractors that suggest obvious but incorrect associations. It also assesses the ability to identify **heterogeneous connections** across diverse domains (e.g., pop culture, history, and linguistics) and the **conceptual depth** required to link remote associates.

**Evaluation Context:**
This is a **text-based** evaluation utilizing **automatic scoring** to measure problem-solving accuracy. Grouping performance is quantified via clustering metrics such as **Adjusted Mutual Information (AMI)**, **Adjusted Rand Index (ARI)**, and **Wasserstein Distance**. Connection naming is assessed using NLP similarity metrics, including **ROUGE-1 F1** and **BERTScore**. The context is one of high task complexity, requiring multi-step reasoning within a constrained semantic space.

**Task Characteristics:**
The tasks involve **constrained problem-solving** and **conceptual ideation**. They are modeled after the ""Remote Associates Test"" (RAT), requiring **combinatorial grouping** (sorting 16 jumbled clues) and **interpretive generation** (naming the hidden connection). The inclusion of randomized and WordNet-based control datasets allows for a specific ablation of how **distractors** and **semantic ambiguity** impact creative reasoning.",73c6533c2988eebb71cdaa758f38546cdf01f655:Only Connect Wall (OCW) Dataset,Only Connect Wall (OCW) Dataset,https://github.com/TaatiTeam/OCW,1.0,API-Only,2.0,True,True,True,huggingface,openai,text
123df211ca36cf7c85ba0b392168f048549db683,WritingBench: A Comprehensive Benchmark for Generative Writing,WritingBench: A Comprehensive Benchmark for Generative Writing,2025.0,2025.0,,Functional & Professional Writing,Narrative & Story Writing; Poetry & Verse; Scientific Discovery; Engineering & Technical Design,high,"The paper presents a comprehensive benchmark for text generation that spans technical, academic, and creative domains, with a primary focus on the model's ability to produce high-quality, constraint-compliant professional and functional documentation.","**Creative Artifacts Evaluated:**
The benchmark evaluates a diverse range of text-based outputs across 100 subdomains. These include academic manuscripts, engineering specifications, financial analyses, business proposals, and creative literary works such as stories and poems.

**Creative Capabilities Assessed:**
Assessment focuses on the model’s ability to adhere to complex, instance-specific constraints. Key capabilities include technical correctness in specialized fields, structural coherence in long-form generation, and emotional intelligence. The framework measures ""generative writing quality"" through five dynamically generated criteria per prompt, capturing nuances like conceptual depth, stylistic alignment, and functional utility.

**Evaluation Context:**
The evaluation is text-centric, utilizing a fine-tuned LLM critic to assign 1-10 scores based on prompt-specific rubrics. This automated system is rigorously validated against human pairwise comparisons (A/B/Tie) to ensure alignment. Notably, the context includes testing the efficacy of Chain-of-Thought (CoT) prompting specifically for enhancing creative and emotional output.

**Task Characteristics:**
Tasks consist of high-complexity, open-ended, and constrained generation. The benchmark is distinguished by its cross-domain breadth, requiring models to navigate 1,000 unique queries that range from rigid technical documentation to fluid creative ideation, emphasizing the model's versatility in shifting between formal and expressive writing styles.",123df211ca36cf7c85ba0b392168f048549db683:WritingBench,WritingBench,https://github.com/X-PLUG/WritingBench,1.0,API-Only,1.0,True,False,True,github,anthropic; openai,text
8f2da3a0775ec12983308c56a84cee38f02e799c,Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,2024.0,2024.0,,Lateral Thinking & Creative Problem Solving,Figurative Language & Rhetoric; Narrative & Story Writing,high,"The paper explicitly focuses on lateral thinking, riddles, and brain teasers to evaluate 'outside-the-box' reasoning, while utilizing narrative scenarios and linguistic puzzles as the primary evaluation artifacts.","**Creative Artifacts Evaluated:**
The paper evaluates LLM performance on incomplete narrative scenarios (situation puzzles), linguistic riddles, and sentence- or word-based brain teasers. These artifacts consist of fragmented stories and unconventional linguistic structures that require ""thinking outside the box"" to resolve.

**Creative Capabilities Assessed:**
Assessment focuses on lateral thinking, specifically the ability to bypass conventional logic to solve puzzles. Key capabilities include divergent thinking, hypothesis generation through iterative inquiry, commonsense reasoning, and conceptual depth in identifying hidden narrative links or non-obvious semantic associations.

**Evaluation Context:**
The context is text-based and involves both interactive and static tasks. It utilizes a ""Weak-eval-Strong"" framework where an LLM judge evaluates the player LLM’s semantic alignment with a reference solution. Evaluation metrics include semantic accuracy, efficiency (average rounds to solution), and group-based accuracy for multiple-choice formats.

**Task Characteristics:**
Tasks range from open-ended, multi-turn scenario deduction—where models must ask constrained yes/no questions to uncover a hidden story—to constrained multiple-choice problem-solving. The work emphasizes the elicitation of creative problem-solving strategies and the cross-domain transfer of lateral thinking from interactive deduction to static riddle-solving.",8f2da3a0775ec12983308c56a84cee38f02e799c:SPLAT,SPLAT,https://github.com/chenqi008/LateralThinking,1.0,API-Only,1.0,True,False,True,github,openai,text
9f06b3caa9bb8bcf35c5175ed65a5ac451d852c3,Metaphor Understanding Challenge Dataset for LLMs,Metaphor Understanding Challenge Dataset for LLMs,2024.0,2024.0,,Figurative Language & Rhetoric,Lateral Thinking & Creative Problem Solving,high,"The paper focuses explicitly on metaphor resolution and cross-domain conceptual mapping, which are the core components of the Figurative Language & Rhetoric domain.","**Creative Artifacts Evaluated:**
The artifacts consist of metaphorical sentences and their corresponding literal paraphrases. Specifically, the dataset focuses on lexical metaphors where individual words—primarily verbs, nouns, or adjectives—are used figuratively within a sentential context to convey non-literal meanings.

**Creative Capabilities Assessed:**
The benchmark measures conceptual depth and analogical reasoning through the lens of figurative language comprehension. It specifically assesses semantic flexibility: the ability to recognize and generate shifts between literal and figurative domains. The tasks evaluate whether a model can navigate the ""mapping"" between disparate concepts, a core component of creative linguistic intelligence and the capacity for non-literal interpretation.

**Evaluation Context:**
This is a text-only evaluation of Large Language Models (LLMs) using the MUNCH dataset. It employs strictly automatic scoring mechanisms, including Accuracy for discriminative tasks and Mean Reciprocal Rank (MRR) and Recall@k for generative tasks. The context is one of linguistic ""understanding"" and semantic resolution rather than open-ended aesthetic production.

**Task Characteristics:**
The tasks involve a combination of constrained generation (single-word replacement in a fill-in-the-blank format) and discriminative judgment (multiple-choice paraphrase selection). Unlike free-form creative writing, these tasks isolate the specific cognitive mechanism of metaphor resolution, requiring the model to perform cross-domain transfer and semantic disambiguation within a controlled linguistic framework.",9f06b3caa9bb8bcf35c5175ed65a5ac451d852c3:Metaphor Understanding Challenge Dataset (MUNCH),Metaphor Understanding Challenge Dataset (MUNCH),https://github.com/xiaoyuisrain/metaphor-understanding-challenge,2.0,GPU/Local,2.0,False,False,True,github,openai; huggingface,text
98b1f8601799d84885b3f7ed1f72441811517df1,D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning,D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning,2025.0,2025.0,,Humor & Satire,Visual Arts & Stylized Imagery; Figurative Language & Rhetoric,high,"The paper explicitly focuses on the interpretation of dark humor and satire within multimodal memes, requiring the decoding of ironic intent and the synergy between visual and textual elements.","**Creative Artifacts Evaluated:**
The artifacts consist of multimodal internet memes specifically employing dark humor. These are composite digital works where static images and textual captions interact to create meaning through the subversion of sensitive, taboo, or cynical themes, such as death, tragedy, or social marginalization.

**Creative Capabilities Assessed:**
The benchmark measures the ability to decode implicit social signals and ironic intent. Key capabilities include conceptual depth (recognizing underlying ""dark"" themes), social awareness (identifying specific target groups or topics of satire), and emotional calibration (assessing the intensity of the humor from mild to extreme). It evaluates the model’s capacity to interpret the nuanced, often controversial boundary between humor and offense.

**Evaluation Context:**
This is a multimodal evaluation framework utilizing automatic scoring metrics (Accuracy, Macro-F1, and Pearson correlation). The focus is on the interpretation of subjective, culturally dependent content where meaning is emergent from the synergy between visual and textual modalities, requiring models to move beyond literal description toward nuanced understanding.

**Task Characteristics:**
The tasks are discriminative and evaluative, involving binary detection, multi-class categorization (target identification), and ordinal classification (intensity prediction). These tasks require high-level reasoning to judge the presence, focus, and severity of dark humor, representing a shift from standard sentiment analysis toward complex reasoning about creative social commentary.",98b1f8601799d84885b3f7ed1f72441811517df1:D-HUMOR,D-HUMOR,https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning,2.0,GPU/Local,3.0,False,False,True,request_access,none,multimodal
aedf6d577bf36287ff30bbaa96d424231b447d25,VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning,VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning,2025.0,2025.0,,Lateral Thinking & Creative Problem Solving,Visual Arts & Stylized Imagery,high,"The paper focuses on analogical reasoning and abstract rule inference through visual problem-solving tasks, specifically using analogical mapping to generate novel images based on structural isomorphisms.","**Creative Artifacts Evaluated:**
The benchmark evaluates **synthetic photorealistic images** and **structured textual descriptions** of visual scenes. These artifacts are organized into analogical quartets (A:B :: C:D), where the final output is a **novel generated image** that must satisfy an inferred abstract transformation rule derived from the preceding images.

**Creative Capabilities Assessed:**
VOILA measures **analogical reasoning**, **abstract rule inference**, and **conceptual consistency**. It assesses the model’s ability to perform **divergent thinking** by applying learned transformations to novel subjects and **fluency** in translating abstract concepts across modalities. Evaluation focuses on **technical correctness** regarding specific properties like object count, subject identity, and action alignment.

**Evaluation Context:**
This **multimodal** framework evaluates MLLMs using a hierarchical, four-step reasoning pipeline (perception, relation extraction, application, and synthesis). Scoring is **mixed**, utilizing **GPT-4o as a judge** to evaluate semantic and visual property alignment against ground truth, alongside **human baselines** to calibrate task difficulty.

**Task Characteristics:**
The tasks represent **constrained generation** and **visual problem-solving**. They require **cross-domain transfer**, where a model extracts a latent logic from one image pair and applies it to a different context. The process moves from **open-ended description** to **zero-shot generative synthesis**, testing the limits of perceptual and relational understanding.",aedf6d577bf36287ff30bbaa96d424231b447d25:VOILA,VOILA,https://github.com/nlylmz/Voila,2.0,GPU/Local,4.0,False,True,True,huggingface,openai; huggingface,multimodal
fc7e58340e84edf85023cac2894c51921ca8c501,AAAR-1.0: Assessing AI's Potential to Assist Research,AAAR-1.0: Assessing AI's Potential to Assist Research,2024.0,2024.0,,Scientific Discovery,Mathematical Reasoning,high,"The paper explicitly evaluates AI's ability to generate scientific experiment designs and identify research weaknesses, while also assessing technical correctness in mathematical equation inferences.","**Creative Artifacts Evaluated:**
Scientific experiment designs, critical identifications of research paper weaknesses, mathematical equation inferences, and justifications for peer review critiques.

**Creative Capabilities Assessed:**
The benchmark measures divergent problem-finding and ideation through the generation of novel experimental setups. It assesses critical thinking and conceptual depth via the identification of non-obvious paper flaws, alongside technical correctness in mathematical reasoning and the perceived necessity/usefulness of generated research ideas.

**Evaluation Context:**
This multimodal framework processes text, figures, and tables using LLMs and multimodal models. Evaluation is hybrid, combining traditional NLP metrics (ROUGE, F1) with semantic similarity (SentenceBERT), LLM-as-a-judge (GPT-4o), and human expert necessity ratings to capture the nuance of scientific utility.

**Task Characteristics:**
Tasks span the research lifecycle, including open-ended generation (experimentation), critical evaluation (paper weaknesses), and discriminative judgment (equation verification). It emphasizes the ""ideation"" and ""refinement"" phases of scientific creativity, moving beyond simple text generation to complex, cross-modal reasoning and problem-solving within the academic domain.",fc7e58340e84edf85023cac2894c51921ca8c501:AAAR-1.0,AAAR-1.0,https://github.com/RenzeLou/AAAR-1.0,1.0,API-Only,4.0,True,True,True,huggingface,openai; anthropic; google; huggingface,multimodal
